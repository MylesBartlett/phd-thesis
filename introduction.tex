\epigraph{
    %
    \emph{
        %
        ``Begin at the beginning,'' the King said gravely, ``and go on till you come to the end:
        then stop.''
    %
} 
%
}
{Alice in Wonderland\\Lewis Carroll}
%
% \section{Life is a story and a story must be told}
\section{All in the golden afternoon}
%
\noindent It is common knowledge that every good story should have a beginning, a middle, and an
end; for if a story had not those things it could scarcely be called a story at all, at most it
would be a nonsense one and nonsense should only be abided when founded on good sense.
%
This is where this thesis begins, a thesis that hopefully satisfies some of the reader's
sensibilities regarding what makes a good story; at the very least I hope it bears some sense of
sense, and perhaps even an estimate of erudition.
%
When I say \emph{story} I mean not, of course, to say that the contents are in any way
fictitious, counterfeit or embellished; as a story, this is a work of chapters and
bridging those chapters is evolution, both academically and personally.
%
For a story to \emph{become} -- to tell itself or let itself be told -- it must grow, by nature, by
contrivance, by necessity; stories reflect life and life is a process of growth, of betterment --
where each step moves us further along on a journey (not on a `chequerboard of nights and days', as
Khayyam so poetically but cynically scribed
%
\footnote{
    \emph{
        %
        “Tis all a Chequerboard of nights and days //
        %
        Where Destiny with men for Pieces plays: //
        %
        Hither and thither moves, and mates,and slays, //
        %
        And one by one back in the closet lays.”
        %
    }
}
%
), one without a destination, but a journey one should
never yield on regardless of the times one stumbles.
%
I would also call this thesis a \emph{story} for the simple reason that -- having childish
tendencies -- I am fond of stories and I would naturally like would like a work I have devoted so
much to -- there are perhaps elements I think less fondly of but might recast (as in some dualistic
tale) as the shadow that makes the light -- the nobler qualities -- burn all the brighter.

%
\section{On the sea of themes}\label{sec:themes}
%, even if that theme is no theme at all: 
% , and whether it be conscious or otherwise:
Every story has a theme:
%, the warp and weft,
an unbroken thread that weaves all into one.
%
This story is no exception; its theme is not one of valour, of defiance in the face of impossible
odds, or of taking the next step when the path ahead is fogged and the path behind beset with
demons -- indeed, the theme is not quite so elevating -- having the power to rouse our best,
eudaimonic selves -- but it bears its own importance, nevertheless -- not to the human condition
but the autonomous one.

%
\marginpar{\raggedleft\textbf{A theme to connect all other themes}}
%
This is a Machine Learning (ML) thesis and the themes are appropriately related to ML; if there
is one central theme that unites all themes across all chapters, it is the \emph{disconnect}
between the \emph{statistical} nature of ML and the \emph{causal} nature of reality -- to mistake
the notion of \emph{correlation} -- born from the former -- with the notion of \emph{causation} is
a great fallacy (\emph{cum hoc ergo propter hoc}), as every new Statistics student learns before
most else.
%
That is to say, if two events, \(X\) and \(Y\), routinely co{\"o}ccur, with the former preceding
the latter, they are correlated, in that one can use the occurrence of \(X\) to predict the
occurrence of \(Y\) at an above-chance rate -- such is the indispensable utility of statistics, as
the quantification of \emph{patterns} (and ML is but sophisticated \emph{pattern recognition}) --
but one cannot reasonably extend this deduction that `\(X\) \emph{predicts} \(Y\)' to the much
deeper one, `\(X\) \emph{causes} \(Y\)' without employing interventions with the view to eliminate
confounding factors, i.e. factors causing both (and thereby correlating) \(X\) and \(Y\).
%
I should emphasise -- to stem right away any misconceptions about what this thesis is or aspires to
be -- that despite their being rooted in it, the themes of this thesis are not \emph{of} causality
itself, by which I mean that, while I may use the calculus of causality to characterise and
interconnect phenomena, I never venture to solve the formidable problem of causal discovery head
on, only those problems emanating from it, representing the aforementioned disconnect.
%
% I shall now explain precisely what I mean by all this.

The simplest explanations are usually the best ones, and by \emph{best} I mean \emph{true} (or
approximately so) when it comes to \emph{reality} -- indeed, such was the consummate genius of
Newtonian, Einsteinian and Darwinian theories to collect myriad related-but-seemingly-distinct
phenomena, within unified models of startling (relatively) simplicity.
%
However, what is simplest in a statistical sense, with respect to a finite representation of
reality -- a \emph{dataset} -- does not always -- and often does not -- align with what is simplest
in the general sense that we can presume \emph{true}.
%
Modern physics frames the universe in terms of \emph{symmetries}, in terms of what changes and what
does not change subject to a particular action or group of actions or \emph{interventions} --
\emph{variances} and \emph{invariances} -- and this same notion, one of \emph{modularity of
subsystems}, is naturally expressible under a causal (and group-theoretic) framework.
%
Through this lens, the \emph{best} model is the one that completely accounts for all observations
as a function of the fewest variables -- one that is maximally invariant or modular.

%
This thesis is not a story of learning a true, causally-complete, world model -- for that would be
too lofty a goal -- however, the notion of invariance -- to specific concepts inducing specific
(open or closed) sets of transformations -- is at the heart of all problems broached -- each on
its own quite humble.
%
`Concepts' is, of course, a rather nebulous term but generality presupposes a degree of
nebulousness; in relation to the works contained herein, I specifically and concretely mean, for
instance, some indicator of group-membership, or the site, or context, of collection (the
\emph{domain}), though the exact nature of said concepts is indeed both conceptually and
practically (as far as the methods I introduce are concerned) arbitrary.
%

The reader has assuredly heard of the many astounding feats accomplished by ML in the last decade,
borne on the winds of the deep-learning revolution stirred by \cite{krizhevsky2012imagenet}.
%
Indeed, the statistical-learning paradigm has given rise to, no less than, autonomous agents
capable of discovering new, more-efficient algorithms for age-old mathematical problems
\citep{fawzi2022discovering}; of deeply comprehending language -- in all its daedal complexity --
and in turn generating it with remarkable coherence, consistency and -- on occasion -- expert-level
insight \citep{brown2020language}; of going toe-to-toe with, or even trouncing, the most adept
players of the most strategically- and physically-demanding games conceived by humanity
\citep{silver2017mastering,berner2019dota,vinyals2019grandmaster,meta2022human}.
%
Yet, where there is light there is shadow, and for every marvel there is a misstep; the annals of
ML present no exception to that.
%
For all the mystique that enshrouds it, ML is (as I have mentioned in passing), au fond,
\emph{statistical} modelling: an ML algorithm ingests a set of data, collected via some (generally
imperfect) mechanism, and models correlations between the covariate and response variables (and
between the covariate variables themselves) in some higher-dimensional space, assuming for the sake
of simplicity (and consistency with the works in this thesis) a \emph{supervised-learning} task
(and if otherwise then the response variables are but some (dynamic) subset of the covariate
variables).
%
This is perhaps slightly reductive but it is nonetheless accurate and all that is important for
what I write of next.

%
\marginpar{\raggedleft\textbf{A misstep for every marvel: the problem with correlations}}
%
There are \emph{correlations} that are causally-supported, and these are the correlations one hopes
are learned by one's ML algorithm of choice, because they are \emph{real} and thus
\emph{generalisable}; there are also those that are \emph{not} causally-supported but are
nonetheless present due to deficiencies on the data-collection (or data-curation) side -- I will
refer to such correlations as being \emph{spurious} henceforth.
%
Of course, I am being `nebulous' again when I say `deficiencies', and again it is for generality's
sake, for this covers anything from systemic bias -- as is the remit of algorithmic fairness (AF)
-- to insufficient coverage (geographically, demographically, etc.), as a result, for instance, of
constrained resources -- as is more the remit of domain adaptation and domain generalisation (DG),
though the lines between these and AF are often blurred.
%
These deficiencies have led to a spate of (in)famous cases, within both academic and journalistic
spheres, igniting public concern and redoubling research efforts to allay said concern.
%
To illustrate: a 2018 investigation \citep{dastin2018amazon} into the automated-hiring system
deployed by Amazon revealed said system to overwhelming prefer male candidates to female ones for
software-development roles based on their resumes; the same year, \cite{buolamwini2018gender}
investigated the behaviour of three commercial gender-classification algorithms \wrt{} different
skin types (`lighter' vs. `darker') and observed marked disparities in the resulting accuracies
between lighter-skinned and darker-skinned subjects, and between male and female subjects, with a
compounding of the two trends; again the same year, \cite{zech2018variable} demonstrated that ML
classifiers can predict incidences of pneumonia with near-perfect accuracy based on only
site-specific tags and the prevalence rates associated thereof.

%
\marginpar{\raggedleft\textbf{The Knots and Nots of data and bias}}
%
The solution -- the sword to this Gordian knot -- seems a staggeringly simple one, so simple as to
perhaps invite the reader to question the whys and wherefores of this thesis and cry: `just collect
more/better data!'
%
Unfortunately, if this best-of-all-possible-worlds were so perfect as to always provide us this
knot-cutter, there would likely be no need for it in the first place; in this imperfect world, we
often have little choice but to make the best of what we have. 
%
There is little hope, for instance, of deriving diverse, bias-free data from any significant
population of people given how rife, and arguably intrinsic (from an evolutionary-psychology
perspective; \cite{kurzban2001evolutionary}), out-group biases (conscious or subconscious) are in
us humans.
%
There is also little hope of capturing, by camera trap, every species within a given region under
every condition and in every locale given that different animals have different ecologies
(nocturnal vs. diurnal, being a clear distinguisher) and there are only so many devices one can
afford to place and thereafter monitor; it may not even be possible to capture every locale alone
(but wish to later generalise to the thitherto-unseen locales) and in such case we have a pure DG
problem.
%
While we cannot correct the problem at the source, we can intervene to \emph{mitigate} the
downstream biases; given that we know what these biases are, we can seek to be \emph{invariant} to
their cause.
%
Thus, in AF, we can (depending on our definition of `fairness') couch our desideratum as learning a
predictor that is invariant to (does not take into account) the designated sensitive attribute(s)
-- race, gender, and age being the usual candidates; in DG, it is the eponymous \emph{domain} one
targets for invariance, this being the locale in the forgoing camera-trap example.

%
\marginpar{\raggedleft\textbf{In search of unannotated data}}
%
I have used the all-encompassing word `data' above to mean `annotated data', for it is the in the
context of supervised learning that I speak of these things, a context wherein the task, and thus
the function to be approximated, is defined by annotations, or at least some target attribute,
thinking of the tabular data where there may not be such a clear divide between the annotations and
the annotated (in contrast with the prototypical example of image-classification where the inputs
are composed of pixels and the annotations are human-conferred labels).
%
While `annotated data' is often indeed scarce due to reasons, inter alia, of a budgetary,
geographic, or inability-to-travel-retrograde-along-the-fourth-dimension nature, data of the
`unannotated' (or `partially annotated') variety is more readily obtained, and it is this reality
by which the embers of unsupervised learning have been stoked, into what has become a bustling
blaze of research in but the short time since this thesis was begun.
%
And so this leads us, word-weathered, to the final of port in our thematic voyage: though this
theme is the first of in the title -- `semi-supervised methods for\dots' -- it is the last I shall
write of here.
%
In this thesis, I entertain two particular motives for using supplementary, unannotated data,
unified in their premise of enabling invariance to `concepts' of interest.
%
Since the unannotated data is being used concurrently with the usual task-defining, annotated data,
we tread in semi-supervised learning's domain.
%
The aforementioned motives are as such:
%
1) while for a bias-bearing dataset the desired invariances may not be learnable
from the annotated data, due to statistical entanglement between the target and concept variables,
they may be so -- or to a greater extent, at least -- given better-covering and more diverse, but
unannotated, data;
%
2) in the context of DG, one may view the domains as defining a finite perturbation set, informing
our model of the types of invariances it need learn in order to generalise to domains yet unseen;
by augmenting this perturbation set with said unannotated data we may aspire to further robustify
said model.

And so, with that, I finish charting a crude atlas of the thematic realms for which we are headed.
%
In the next section I shall chart a definite course into those realms and following that, we shall
embark, full sail on that course, into the very heart of the thesis.

% It would be a gross exaggeration to say, however that it is a Barmecide Feast -- that has
% promised the world and delivered and delivered an atlas -- for the accomplishments are very much
% real.

\section{Charting the course ahead}\label{sec:charting}
%
This is a thesis of three parts and six chapters. 
%
As this is the first chapter, it is necessarily the beginning yet, at the same time, it is not
the end of the beginning. 
%
In the subsequent chapter -- what remains of the beginning -- I provide background on the main
topics I think relevant to the works that constitute the `middle' and marrow of this thesis.
%
That second chapter has its own introduction and so I will not expatiate here on the precise
nature of its contents.
%
The `middle' is a part of three chapters, each chapter corresponding to a distinct paper and a
distinct problem; these papers appear in an order that is chronological yet also most
thematically-contiguous.
%
In the `end', I discuss the works holistically, both in the context of one another and in the
context of more recent developments in the germane fields; taking stock of this, I also ponder
future avenues of work in similar vain.
%
To help orient the reader, I adumbrate below the three `middle' chapters of the thesis, stating
in each case their motivations, methods, and merits. 
%
I will shift to using `we' here, in self-reference, as all these works were done collaboratively,
yet I claim credit enough (as first or second author) to feel deserving of their ownership and of
their inclusion in this thesis, a text that represents work of my shaping and doing.
%
To substantiate this claim, I include at the end of each of the corresponding chapters an estimate
of the contributions made by myself and each co{\"a}uthor.

\paragraph{Chapter 3: Null-sampling for Interpretable and Fair Representations.}
%
Here, we consider a setup in which sampling bias induces a one-to-one correspondence between
subgroup and target; since learning from the training data alone is ill-posed in this case, we
propose a two-stage, interpretable framework exploiting the unique properties of invertible models,
founded on the assumption that more-diverse unlabelled data is often readily obtainable, e.g. from
censuses.

\paragraph{Chapter 4: Addressing Missing Sources with Adversarial Support-Matching.}
%
In this second paper, we relax the problem constructed in Chapter 3 such that subgroup-target
combinations are missing in an asymmetric fashion, \wrt{} the target, and propose to solve this by
matching the support of the training set with that of an unlabelled dataset representative of the
test set. 
%
This is realised through a combination of clustering, adversarial training with a set
discriminator, and a hierarchical-sampling strategy.
%

\paragraph{Chapter 5: Okapi: Generalising Better by Making Statistical Matches Match.}
%
The third and final paper contends with the broader problem of generalising to data from
\emph{domains} outside the training distribution  and how unlabelled data from extra domains can
effectively further this goal.
%
Here, changes in domain induce natural distribution shifts corresponding, for example, to
variations in lighting, perspective, and environs; to counteract them, we propose a simple,
consistency-regularised approach based on causal matching.

