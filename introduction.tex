\epigraph{
    %
    \emph{
        %
        ``It is good to have an end to journey toward; but it is the journey that matters, in the
        end.''
        %
} 
%
}
{The Left Hand of Darkness\\Ursula K. Le Guin}
%
\section*{Preamble}
\noindent It is generally thought that every good story should have a beginning, a middle, and an
end; for if a story had not those things it could scarcely be called a story at all, at most it
would be a nonsense one and nonsense is only sens\emph{ible} when founded on \emph{able} sense.
%
This is where this thesis begins, a thesis that hopefully satisfies some of the reader's
sensibilities regarding what makes a good story; at the very least I hope it bears some sliver of
sense, even perhaps an estimate of erudition.
%
\marginpar{\raggedleft\textbf{The thesis of stories and the journey without an end}}
%
When I say \emph{story} I mean not, of course, to say that the contents are in any way fictitious
or embellished; as a story, this is a work of chapters and bridging those chapters is evolution,
both academically and personally.
%
For a story to \emph{become} -- to tell itself or let itself be told -- it must grow, by nature, by
contrivance, by necessity; stories reflect life and life is a process of growth, of betterment --
where each step carries us onward on a journey, one without a destination, but a journey one should
never yield on regardless of the times one stumbles. 
%
I am not ashamed admitting that the journey paved by this thesis itself was marked by many such
stumbles, by many foibles and follies, times when I felt I could not stand again for the weight of
the past and the murk of the future; I cannot say that I conquered all, if any, of my frets and
fears and failings, but I can at least say that I forged on and became \emph{better}, not by own
mettle alone but as much by the support of those who have staunchly companioned me each step of the
way, through summer-sweetened meadow and gloom-drenched thicket alike.
%
I would also call this thesis a \emph{story} simply for my fondness of stories, for all that they
might teach us about others, and, most of all, ourselves -- by exploring fabulous \emph{other}
worlds we also come to better know the inner one; to want to be fond of something one has given so
much to is, I think, natural and should there be any lesser elements to it, I can fancy them, as in
some dualistic tale, the darkness that makes the light shine all the brighter. 
%
% For while every lit candle may cast a shadow, it is equally true that the darker, the more
% moonless, the night the more splendent the starlight.
%
\section*{On the sea of themes}\label{sec:themes}
%, even if that theme is no theme at all: 
% , and whether it be conscious or otherwise:
Every story has a theme:
%, the warp and weft,
an unbroken thread that weaves all into one.
%
This story is no exception; its theme is not one of valour, of defiance in the face of impossible
odds, or of taking the next step when the path ahead is fogged and the path behind beset with
demons -- indeed, the theme is not quite so elevating -- having the power to rouse our best selves
-- but it bears its own importance, nevertheless -- not to the human condition but rather to the
autonomous one.

%
\marginpar{\raggedleft\textbf{A theme to connect all other themes}}
%
This is a Machine Learning (ML) thesis and the themes are appropriately related to ML; if there is
one central theme that unites all themes across all chapters, it is the \emph{disconnect} between
the \emph{statistical} nature of ML and the \emph{causal} nature of reality -- to mistake the
notion of \emph{correlation} -- born from the former -- with the notion of \emph{causation} is a
great fallacy, as every new Statistics student learns before most else.
%
That is to say, if two events, \(X\) and \(Y\), routinely co{\"o}ccur, with the former preceding
the latter, they are correlated, in that one can use the occurrence of \(X\) to predict the
occurrence of \(Y\) at an above-chance rate -- such is the indispensable utility of statistics, as
the quantification of \emph{patterns} (and ML is but sophisticated \emph{pattern recognition}) --
but one cannot reasonably extend this deduction that `\(X\) \emph{predicts} \(Y\)' to the much
deeper one, `\(X\) \emph{causes} \(Y\)' without employing interventions with the view to eliminate
confounding factors, i.e. factors causing both (and thereby correlating) \(X\) and \(Y\).
%
I should emphasise -- to stem right away any misconceptions about what this thesis is or aspires to
be -- that despite their being rooted in it, the themes of this thesis are not \emph{of} causality
itself, by which I mean that, while I may use the calculus of causality to characterise and
interconnect phenomena, I never venture to solve the formidable problem of causal discovery head
on, only those problems emanating from it, representing the aforementioned disconnect.
%

The simplest explanations are usually the best ones, and by \emph{best} I mean \emph{true} (or
approximately so) when it comes to \emph{reality} -- indeed, such was the consummate genius of
Newtonian, Einsteinian and Darwinian theories to collect myriad related-but-seemingly-distinct
phenomena, within unified models of startling (relatively) simplicity.
%
However, what is simplest in a statistical sense, with respect to a finite representation of
reality -- a \emph{dataset} -- does not always -- and often does not -- align with what is simplest
in the general sense that we can presume \emph{true}.
%
Modern physics frames the universe in terms of \emph{symmetries}, in terms of what changes and what
does not change subject to a particular action or group of actions or \emph{interventions} --
\emph{variances} and \emph{invariances} -- and this same notion, one of \emph{modularity of
subsystems}, is naturally expressible under a causal (and group-theoretic) framework.
%
Through this lens, the \emph{best} model is the one that completely accounts for all observations
as a function of the fewest variables -- one that is maximally invariant or modular.

%
This thesis is not a story of learning a true, causally-complete, world model -- for that would be
too lofty a goal -- however, the notion of invariance -- to specific concepts inducing specific
(open or closed) sets of transformations -- is at the heart of all problems broached -- each on
its own quite humble.
%
`Concepts' is, of course, a rather nebulous term but generality presupposes a degree of
nebulousness; in relation to the works contained herein, I specifically and concretely mean, for
instance, some indicator of group-membership, or the site, or context, of collection (the
\emph{domain}), though the exact nature of said concepts is indeed both conceptually and
practically (as far as the methods I introduce are concerned) arbitrary.
%

The reader has assuredly heard of the many astounding feats accomplished by ML in the last decade,
borne on the winds of the deep-learning revolution stirred by \cite{krizhevsky2012imagenet}.
%
Indeed, the statistical-learning paradigm has given rise to, no less than, autonomous agents
capable of new, more-efficient algorithms for age-old mathematical problems
\citep{fawzi2022discovering}; of deeply comprehending language -- in all its daedal complexity --
and in turn generating it with remarkable coherence, consistency and -- on occasion -- expert-level
insight \citep{brown2020language}; of going toe-to-toe with, or even trouncing, the most adept
players of the most strategically- and physically-demanding games conceived by humanity
\citep{silver2017mastering,berner2019dota,vinyals2019grandmaster,meta2022human}.
%
Yet, where there is light there is shadow, and for every marvel there is a misstep; the annals of
ML present no exception to that.
%
For all the mystique that enshrouds it, ML is (as I have mentioned in passing), au fond,
\emph{statistical} modelling: an ML algorithm ingests a set of data, collected via some (generally
imperfect) mechanism, and models correlations between the covariate and response variables (and
between the covariate variables themselves) in some higher-dimensional space, assuming for the sake
of simplicity (and consistency with the works in this thesis) a \emph{supervised-learning} task
(and if otherwise then the response variables are but some (dynamic) subset of the covariate
variables).
%
% This is perhaps slightly reductive but it is nonetheless accurate and all that is important for
% what I write of next.
%
\marginpar{\raggedleft\textbf{A misstep for every marvel: the problem with correlations}}
%
There are \emph{correlations} that are causally-supported, and these are the correlations one hopes
are learned by one's ML algorithm of choice, because they are \emph{real} and thus
\emph{generalisable}; there are also those that are \emph{not} causally-supported but are
nonetheless present due to deficiencies on the data-collection (or data-curation) side -- I will
refer to such correlations as being \emph{spurious} henceforth.
%
Of course, I am being `nebulous' again when I say `deficiencies', and again it is for generality's
sake, for this covers anything from systemic bias -- as is the remit of algorithmic fairness (AF)
-- to insufficient coverage (geographically, demographically, etc.), as a result, for instance, of
constrained resources -- as is more the remit of domain adaptation and domain generalisation (DG),
though the lines between these and AF are often blurred.
%
These deficiencies have led to a spate of (in)famous cases, within both academic and journalistic
spheres, igniting public concern and redoubling research efforts to allay said concern.
%
To illustrate: a 2018 investigation \citep{dastin2018amazon} into the automated-hiring system
deployed by Amazon revealed said system to overwhelming prefer male candidates to female ones for
software-development roles based on their resumes; the same year, \cite{buolamwini2018gender}
investigated the behaviour of three commercial gender-classification algorithms \wrt{} different
skin types (`lighter' vs. `darker') and observed marked disparities in the resulting accuracies
between lighter-skinned and darker-skinned subjects, and between male and female subjects, with a
compounding of the two trends; again the same year, \cite{zech2018variable} demonstrated that ML
classifiers can predict incidences of pneumonia with near-perfect accuracy based on only
site-specific tags and the prevalence rates associated thereof.
%
Such systems can be said to \emph{not} be \emph{distributionally robust} as they either fail to
generalise beyond the training distribution or to exhibit approximate performance-parity between
sub-distributions even within the training data, corresponding to different concepts (gender and
race in the foregoing examples).

%
\marginpar{\raggedleft\textbf{The knots and nots of data and bias}}
%
The solution -- the sword to this Gordian knot -- seems a staggeringly simple one, so simple as to
perhaps invite the reader to question the whys and wherefores of this thesis and cry: `just collect
more/better data!'
%
Unfortunately, if this best-of-all-possible-worlds were so perfect as to always provide us this
knot-cutter, there would likely be no need for it in the first place; in this imperfect world, we
often have little choice but to make the best of what we have. 
%
There is little hope, for instance, of deriving diverse, bias-free data from any significant
population of people given how rife, and arguably intrinsic (from an evolutionary-psychology
perspective; \cite{kurzban2001evolutionary}), out-group biases (conscious or subconscious) are in
us humans.
%
There is also little hope of capturing, by camera trap, every species within a given region under
every condition and in every locale given that different animals have different ecologies
(nocturnal vs. diurnal, being a clear distinguisher) and there are only so many devices one can
afford to place and thereafter monitor; it may not even be possible to capture every locale alone
(but wish to later generalise to the thitherto-unseen locales) and in such case we have a pure DG
problem.
%
While we cannot correct the problem at the source, we can intervene to \emph{mitigate} the
downstream biases; given that we know what these biases are, we can seek to be \emph{invariant} to
their cause.
%
Thus, in AF, we can (depending on our definition of `fairness') couch our desideratum as learning a
predictor that is invariant to (does not take into account) the designated sensitive attribute(s)
-- race, gender, and age being the usual candidates; in DG, it is the eponymous \emph{domain} one
targets for invariance, this being the locale in the forgoing camera-trap example.

%
\marginpar{\raggedleft\textbf{In search of cost(less) data}}
%
I have used the all-encompassing word `data' above to mean `annotated data', for it is the in the
context of supervised learning that I speak of these things, a context wherein the task, and thus
the function to be approximated, is defined by annotations, or at least some target attribute,
thinking of the tabular data where there may not be such a clear divide between the annotations and
the annotated (in contrast with the prototypical example of image-classification where the inputs
are composed of pixels and the annotations are human-conferred labels).
%
While `annotated data' is often indeed scarce due to reasons, inter alia, of a budgetary,
geographic, or inability-to-travel-retrograde-along-the-fourth-dimension nature, data of the
`unannotated' (or `partially annotated') variety is more readily obtained, and it is this reality
by which the embers of unsupervised learning have been stoked, into what has become a bustling
blaze of research in but the short time since this thesis was begun.
%
And so this leads us, hopefully not-too-word-weathered, to the final of port in our thematic
voyage: though this theme is the first written of in the title -- `semi-supervised methods
for\dots' -- it is the last I shall write of here.
%
In this thesis, I entertain two particular motives for using supplementary, unannotated data,
unified in their premise of enabling invariance to `concepts' of interest.
%
Since the unannotated data is being used concurrently with the usual task-defining, annotated data,
we tread in semi-supervised learning's (SemiSL's) domain.
%
The aforementioned motives are as such:
%
First, while for a bias-bearing dataset the desired invariances may not be learnable from the
annotated data, due to statistical entanglement between the target and concept variables, they may
be so -- or to a greater extent, at least -- given better-covering and more diverse, but
unannotated, data;
%
Second, in the context of DG, one may view the domains as defining a finite perturbation set,
informing our model of the types of invariances it need learn in order to generalise to domains yet
unseen; by augmenting this perturbation set with said unannotated data we may aspire to further
robustify said model.

With this, I finish charting a crude atlas of the thematic realms for which we are headed.
%
In the next section I shall chart a definite course into those realms and following that, we shall
embark, full sail, into the heart of the thesis.
%
\section*{Charting the course ahead}\label{sec:charting}
%
This is a thesis of three parts and six chapters. 
%
As this is the first chapter, it is necessarily the beginning yet, at the same time, it is not
the end of the beginning. 
%
In the subsequent chapter -- what remains of the beginning -- I provide background on the main
topics I think relevant to the works that constitute the `middle' and marrow of this thesis.
%
That second chapter has its own introduction and so I will not dwell here on the precise nature of
its contents.
%
The `middle' is a part of three chapters, each chapter corresponding to a distinct paper and a
distinct problem; these papers appear in an order that is chronological yet also most
thematically-contiguous.
%
In the `end', I discuss the works holistically, both in the context of one another and in the
context of more recent developments in the germane fields; taking stock of this, I also ponder
future avenues of work in similar vain.
%
To help orient the reader, I adumbrate below the three `middle' chapters of the thesis, stating
in each case their motivations, methods, and merits. 
%
I will shift to using `we' here, in self-reference, as all these works were done collaboratively,
yet I claim credit enough (as first or second author) to feel deserving of their ownership and of
their inclusion in this thesis, a text that represents work of my shaping and doing.
%
To substantiate this claim, I include at the end of each of the corresponding chapters an estimate
of the contributions made by myself and each co{\"a}uthor.
%
\subsection*{Chapter 3}%: Null-sampling for Interpretable and Fair Representations}
%
In this first of the middle chapters, we tackle what we term an \emph{aggravated fairness problem},
characterised strong spurious spurious correlations between the sensitive and target attributes, to
the extent of one-to-one correspondence which is not maintained at test time.
%
This is, consequently, a shortcut-learning (SCL) problem in which the former attribute serves as a proxy
for the latter attribute, by virtue of its lower-complexity; to ensure generalisability across the
spectrum of intersectional groups, we require a model that learns the correct, causal mapping for
the features to the target, which is to say that is invariant to the sensitive attribute, though we
are harried by the entailed problem of identifiability.
%
Similar characterisations had been considered in the domain generalisation literature
\cite{arjovsky2019invariant, jacobsen2019excessive}, though we approached and, argued for the
validity of, the problem from an AF perspective, partly motivated by the cases of systemic
censoring adduced in \citep{kallus2018residual}.
%
Given the intractability of disentangling the two attributes, we assume the existence of a
supplementary, dataset that contains all intersectional groups but is annotated only partially, in
the sense that the target annotations are absent -- we assume such data is more easily obtained
than fully-annotated data, with census data being one potential source, for example.
%
This gives rise to a kind of transfer-learning setup in which the quality to be transferred is
invariance to the sensitive attribute for which we propose to interpretable framework exploiting
the unique properties of invertible models, in particular their losslessness connoted by their
bijectivity along with their namesake exact invertibility (whereas more traditional
autoencoder-based approaches furnish only approximate invertibility).
%

We demonstrate that these unique properties are practically meritable, giving rise to models that
perform more robustly over a range of datasets and degrees of correlations, compared with baseline
models, and especially so in full transfer-learning scenarios where the partially-annotated and
fully-annotated datasets are drawn from disparate distributions sharing a sensitive attribute.
%
Moreover, the exact invertibility allows us insight into what the model has learned and diagnose
potential failures, such as unforeseen entanglements between the sensitive attribute and certain
dimensions of the inputs.
%
\subsection*{Chapter 4}%: Addressing Missing Sources with Adversarial Support-Matching}
%
The aforementioned paper only require partially-annotated unbiased data but it required annotations
nonetheless and there are undoubtedly cases where this is prohibitive.
%
In the second chapter, we accordingly consider a relaxed version of the problem -- this time,
couched in non-AF-centric manner -- which is soluble using supplementary data that is unannotated
in the truest sense, in that neither the sensitive -- here, `subgroup' -- nor target annotations
are provided.
%
The problem is still one of SCL, however, the biasing is imposed in a hierarchical (with the
targets -- specifically classes -- constituting the top level the implied tree, the subgroup the
bottom level) and asymmetric fashion such that the identifiability is possible; its general
formulation -- which admits the problem from the previous chapter as a special case -- is as much a
contribution of the chapter as the solution we ultimately propose to it, a solution predicated on
the idea of aligning support rather than distributions as historically practised in domain
adaptation \citep{ben2006analysis}.
%
By `asymmetric' I mean that for we observe all subgroups and classes expected at deployment time
and for at least one class we observe more than one subgroup. 
%
In this context, we refer to the intersectional groups -- target-subgroup combinations -- as
`sources' and the aforementioned problem, characterised by their missingness, as one of `missing
sources'.
%
The problem is strongly redolent of the classic unsupervised domain adaptation (UDA) one, in that
one has access to unlabelled data from a \emph{target} domain, distributionally-shifted relative to
the training data, and seeks to maximise positive transfer, or adaptation, to that test data from
that same domain.
%
However, the distinction lies in the missing-sources problem being of a
hierarchical/class-conditional nature, whereas in UDA there would be entire subgroups missing from
the training data yet at the same time no shortcuts between subgroup and targets induced.

%
In order to accomplish the alluded-to support-alignment, or \emph{support-matching}, we employ
semi-supervised clustering to estimate the sources in the unannotated dataset, or \emph{deployment
set}, which we assume to be source-complete \wrt{} the test set, and may even be the test set
itself in a transductive setting.
%
With the estimates in hand, we proceed to use a hierarchical-sampling procedure to construct
batches from the training and deployment sets representative of their respective support over the
sources, training an encoder to generate representations of them that are \emph{dataset}-invariant
by means of an adversarial set-discriminator.
%
We find this approach can generate, and with surprisingly swift and stable convergence, invariant
representations in a way that is robust to the approximation-error incurred by clustering,
significantly more so than instance-wise and supervised (using clusters for balancing and as direct
targets) baselines.
%
\subsection*{Chapter 5}%: Okapi: Generalising Better by Making Statistical Matches Match}
%
In the forgoing chapters, we considered setups in which the concepts (sensitive subgroup attributes
for Chapters 3 and 4 respectively) constituted a closed set, which is to plainly say that their
possible test-time values were known and represented -- in some form, annotated,
partially-annotated, or entirely unannotated -- at training time.
%
We also assumed that the target played a role in the distributional shift that consequently
corresponded to one of target and subpopulation/subgroup/domain (all these terms being synonymous
here and throughout the literature, differing only by context) shift combined yet could not be
treated with conventional methods, such as reweighting, due to the emergence of spurious
correlations.
%
The focus of this third chapter, in contrast, is not on any spurious component but on the
domain-shift one, such that is the well-established problem of DG that we tackle, albeit with an
SemiSL slant introduced by then-recent benchmarks \citep{SagWeiLeeGaoetal22}. 
%
We may distil this focus into the motivating question ``Given annotated training data drawn from a
finite set of domains, disjoint from those of the test set, might we use additional unannotated
data from again disjoint (\wrt{} both training and test sets) domains to improve generalisation (to
the aforesaid unseen domains)?''
%

The springboard for our proposed method was statistical-matching algorithm developed by my
co{\"a}uthors, as appearing in \cite{RomInsShaQua22}, that pairs samples from different (in this
case) domains based on certain (robust) statistical criteria. 
%
In the context of \cite{RomInsShaQua22}, this algorithm is applied in a pseudo-post-hoc fashion to
construct a `patched' dataset, with which the initial model is trained anew in a
distributionally-robust manner.
%
In the context of the chapter-being-discussed, this algorithm is generalised and embedded in a
online-learning framework --  motivated by improved-efficiency and the finding that the original
two-stage (offline) framework proved insufficient for problems of the kind in question -- with the
generated matches used to define matches for a consistency-regularised SemiSL objective that is
task- and modality-agnostic.
%
We find the resulting algorithm consistently outperforms baseline methods, including
full-supervised ones which \cite{SagWeiLeeGaoetal22} showed many existing SemiSl methods failed to
do, and, as in Chapter 3, it comes with a valuable interpretable aspect, conferred by the
trained-with matching algorithm.


% The third and final paper contends with the broader problem of generalising to data from
% \emph{domains} outside the training distribution  and how unlabelled data from extra domains can
% effectively further this goal.
% %
% Here, changes in domain induce natural distribution shifts corresponding, for example, to
% variations in lighting, perspective, and environs; to counteract them, we propose a simple,
% consistency-regularised approach based on causal matching.

