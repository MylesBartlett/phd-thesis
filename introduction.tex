\epigraph{
    %
    \emph{
        %
        ``Begin at the beginning,'' the King said gravely, ``and go on till you come to the end:
        then stop.''
    %
} 
%
}
{Alice in Wonderland\\Lewis Carroll}
%
\noindent It is common knowledge that every good story should have a beginning, a middle, and an
end; for if a story had not those things it could scarcely be called a story at all, at most it
would be a nonsense one and nonsense should only be abided when founded on good sense.
%
This is where this thesis begins, a thesis that hopefully satisfies some of the reader's
sensibilities regarding what makes a good story; at the very least I hope it bears a scintilla of
sense, or perhaps even some estimate of erudition.
%
When I say \emph{story} I mean not, of course, to say that the contents are in any way
fictitious, counterfeit or embellished; as a story, this is a work of chapters and
bridging those chapters is evolution, both academically and personally.
%
For a story to \emph{become} -- to tell itself or let itself be told -- it must grow, by nature, by
contrivance, by necessity; stories reflect life and life is a process of growth, of betterment --
where each step moves us further along on a journey (not on a `chequerboard of nights and days', as
Khayyam so poetically but cynically scribed
%
\footnote{
    \emph{
        %
        “Tis all a Chequerboard of nights and days //
        %
        Where Destiny with men for Pieces plays: //
        %
        Hither and thither moves, and mates,and slays, //
        %
        And one by one back in the closet lays.”
        %
    }
}
%
), one without a destination, but a journey one should
never yield on regardless of the times one stumbles.
%
I would also call this thesis a \emph{story} for the simple reason that -- having childish
tendencies -- I am fond of stories and I should naturally like to like a work born from my hand and
\emph{essentia} -- there are perhaps elements I should think less fondly of but might imagine (in
the style of some dualistic tale) as the shadow that makes the light -- the nobler qualities --
burn all the brighter.

%
\section{Humble themes}\label{sec:themes}
%
Every story has a theme, even if that theme is no theme at all, and whether it be conscious or
otherwise: an unbroken thread, the warp and weft, that weaves all into one.
%
This story is no exception; its theme is not one of valour, of defiance in the face of impossible
odds, or of taking the next step when the path ahead is fogged and the path behind beset with
demons -- indeed, the theme is nothing so inspiriting or ennobling -- that has the power to rouse
our best, eudaimonic selves -- but it bears its own importance -- not to the human condition but
the autonomous one -- nevertheless.

%
This is a Machine Learning (ML) thesis and the themes are suitably related to ML; if there
is one central theme that unites all themes across all chapters, it is the dialectical tension
between the \emph{statistical} nature of ML and the \emph{causal} nature of reality -- to mistake
the notion of \emph{correlation} -- born from the former -- with the notion of \emph{causation} is
a great fallacy (\emph{cum hoc ergo propter hoc}), as every new Statistics student learns before
most else.
%
That is to say, if two events, \(X\) and \(Y\), routinely co{\"o}ccur, with the former preceding
the latter, they are correlated, in that one can use the occurrence of \(X\) to predict the
occurrence of \(Y\) at an above-chance rate -- such is the indispensable utility of statistics, as
the quantification of \emph{patterns} (and ML is but sophisticated \emph{pattern recognition}) --
but one cannot reasonably extend this deduction that `\(X\) \emph{predicts} \(Y\)' to the much
deeper one, `\(X\) \emph{causes} \(Y\)' without employing interventions with the view to eliminate
confounding factors, i.e. factors causing both (and thereby correlating) \(X\) and \(Y\).
%
I should emphasise -- to stem right away any misconceptions about what this thesis is or aspires to
be -- that despite their being rooted in it, the themes of this thesis are not of causality itself,
by which I mean that, while I may use the calculus of causality to characterise and interconnect
phenomena, I never venture to solve the formidable problem of causal discovery head on, only those
problems emanating from it, representing the aforementioned tension.
%
% I shall now explain precisely what I mean by all this.

The simplest explanations are usually the best ones, and by \emph{best} I mean \emph{true} (or
approximately so) when it comes to \emph{reality} -- indeed, such was the consummate genius of
Newtonian, Einsteinian and Darwinian theories to collect myriad related-but-seemingly-distinct
phenomena, within unified models of startling (relatively) simplicity.
%
However, what is simplest in a statistical sense, with respect to a finite representation of
reality -- a \emph{dataset} -- does not always -- and often does not -- align with what is simplest
in the general sense that we can presume \emph{true}.
%
Modern physics frames the universe in terms of \emph{symmetries}, in terms of what changes and what
does not change subject to a particular action or group of actions or \emph{interventions} --
\emph{variances} and \emph{invariances} -- and this same notion, one of \emph{modularity of
subsystems}, is naturally expressible under a causal (and group-theoretic) framework.
%
Through this lens, the \emph{best} model is the one that completely accounts for all observations
as a function of the fewest variables -- one that is maximally invariant or modular.

%
This thesis is not a story of learning a true, causally-complete, world model -- for that would be
too lofty a goal -- however, the notion of invariance -- to specific concepts inducing specific
(open or closed) sets of transformations -- is at the heart of all problems broached -- each on
its own quite humble.
%
`Concepts' is, of course, a rather nebulous term but generality presupposes a degree of
nebulousness; in relation to the works contained herein, I specifically and concretely mean, for
instance, some indicator of group-membership, or the site, or context, of collection (the
\emph{domain}), though the exact nature of said concepts is indeed both conceptually and
practically (as far as the methods I introduce are concerned) arbitrary.
%

The reader has assuredly heard of the many astounding feats accomplished by ML in the last decade,
borne on the winds of the deep-learning revolution stirred by \cite{krizhevsky2012imagenet}.
%
Indeed, the statistical-learning paradigm has given rise to, no less than, autonomous agents
capable of discovering new, more-efficient algorithms for age-old mathematical problems
\citep{fawzi2022discovering}; of deeply comprehending language -- in all its daedal complexity --
and in turn generating it with staggering coherence and -- on occasion -- expert-level insight
\citep{brown2020language}; of going toe-to-toe with the most adept players of the most
strategically- and physically-demanding games conceived by the human mind
\citep{silver2017mastering,vinyals2019grandmaster,meta2022human}.

% \cite{zech2018variable}, for instance, famously demonstrated that one can train a model for
% pneumonia diagnosis achieving near-perfect accuracy solely based on the site-specific tags and
% the associated prevalence rates -- such a model is demonstrably useless (which is to say,
% dangerous) in practice despite its well-satisfying the training objective.


%
\section{Charting the course}\label{sec:charting}
%
This is a thesis of three parts and six chapters. 
%
As this is the first chapter, it is necessarily the beginning yet, at the same time, it is not the
end of the beginning. 
%
In the subsequent chapter, and what remains of the beginning, I provide background on the main
topics I think relevant to the works that constitute the `middle' and marrow of this thesis.
%
That second chapter has its own introduction and so I will not dwell more here on the precise
nature of its contents.
%
The `middle' is a part of three chapters, each chapter corresponding to a distinct paper and a
distinct problem; these papers appear in an order that is chronological yet also thematically
contiguous in some sense.
%
In the `end', I discuss the works holistically, both in the context of one another and in the
context of more recent developments in the germane fields; taking stock of this, I also ponder
future avenues of work in similar vain.
%
To help orient the reader, I adumbrate below the three `middle' chapters of the thesis, detailing
in each case their motivations, methods, and merits.

\paragraph{Chapter 3: Null-sampling for Interpretable and Fair Representations.}
%
Here, we consider a setup in which sampling bias induces a one-to-one correspondence between
subgroup and target; since learning from the training data alone is ill-posed in this case, we
propose a two-stage, interpretable framework exploiting the unique properties of invertible models,
founded on the assumption that more-diverse unlabelled data is often readily obtainable, e.g. from
censuses.

\paragraph{Chapter 4: Addressing Missing Sources with Adversarial Support-Matching.}
%
In this second paper, we relax the problem constructed in Chapter 3 such that subgroup-target
combinations are missing in an asymmetric fashion, \wrt{} the target, and propose to solve this by
matching the support of the training set with that of an unlabelled dataset representative of the
test set. 
%
This is realised through a combination of clustering, adversarial training with a set
discriminator, and a hierarchical-sampling strategy.
%

\paragraph{Chapter 5: Okapi: Generalising Better by Making Statistical Matches Match.}
%
The third and final paper contends with the broader problem of generalising to data from
\emph{domains} outside the training distribution  and how unlabelled data from extra domains can
effectively further this goal.
%
Here, changes in domain induce natural distribution shifts corresponding, for example, to
variations in lighting, perspective, and environs; to counteract them, we propose a simple,
consistency-regularised approach based on causal matching.



    





%


% \emph{
%     %
%     ``You might just as well say,'' added the March Hare, “that `I like what I get` is the same
%     thing as `I get what I like`!''
%     %
%     \\
%     %
%     ``You might just as well say,'' added the Dormouse, who seemed to be talking in his sleep,
%     ``that `I breathe when I sleep' is the same thing as `I sleep when I breathe'!'' 
%     %
% }

% \emph{
%     ``Would you tell me, please, which way I ought to go from here?''
%     \\
%     ``That depends a good deal on where you want to get to,'' said the Cat.
%     \\
%     ''I don’t much care where—'' said Alice.
%     \\
%     ``Then it doesn’t matter which way you go,'' said the Cat.
%     \\
%     ``-so long as I get somewhere,'' Alice added as an explanation.
%     \\
%     ``Oh, you’re sure to do that,'' said the Cat, ``if you only walk long enough.''
% }

% \emph{
%     %
%     The King’s argument was, that anything that had a head could be beheaded, and that you
% weren’t to talk nonsense.
%     %
% }

% \emph{
%     %
% ``Well! I’ve often seen a cat without a grin,'' thought Alice; ``but a grin without a cat! It's the
% most curious thing I ever saw in all my life!'' 
%     %
% }

% \emph{
%     %
%     I don’t see how he can ever finish, if he doesn't begin.
%     %
% }

% \emph{
% %
%     Alice didn't think that proved it at all; however, she went on ``And how do you know that
%     you’re mad?''
% %
%     ``To begin with,” said the Cat, ``a dog’s not mad. You grant that?''
% %
%     ``I suppose so,'' said Alice.
% %
%     ``Well, then,'' the Cat went on, ``you see, a dog growls when it's angry, and wags its tail
%     when it's pleased. Now I growl when I'm pleased, and wag my tail when I’m angry. Therefore I’m
%     mad.''
% %
% }


% \emph{
%     %
%     ``If there’s no meaning in it,'' said the King, ``that saves a world of trouble, you know, as
%     we needn't try to find any.''
%     %
% }
