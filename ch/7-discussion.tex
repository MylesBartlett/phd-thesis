\chapter{Discussion and future work}\label{ch:conclusion}
In this thesis, I have presented three approaches for dealing with dataset bias.
The first one deals with a form of label bias and the other two with forms of sampling bias;
in both cases the bias is linked to a special attribute \(s\) of the data.
%
% mention again the main strengths of the presented methods
%
With the first approach, the user specifies target rates that can be used to tune the method to the desired outcome.
The method is flexible and easy to integrate with existing algorithms.
The second approach uses a partially-labelled representative set to learn an interpretable and invariant representation.
The user can directly observe in what way the data was changed to make it invariant
to the spurious correlation that is present in the training data.
The third approach in many ways builds upon the second one;
a set similar to the representative set is needed but there is no requirement for labels nor for balance of subgroups.
However, in the training set, the relationship between the class label \(y\) and subgroups \(s\) may not be
as close to a one-to-one mapping of \(y\) and \(s\) as in the second approach.
From these simple starting points, an invariant representation is learned
that can be used to train unbiased classifiers.

The remainder of this chapter is split into three parts:
In \secref{sec:ch7-limitations},
I discuss some of the limitations of the presented methods to keep in mind,
and what the goal of the applications of the methods should be.
\Secref{sec:potential-extentions} is about ways in which the work could be extended,
and \secref{sec:broader-perspective} discusses the broader perspective of this topic,
including a wider discussion of the future of the field.

\section{Limitations and intended use}\label{sec:ch7-limitations}
%
% ========== acknowledge that this doesn't cover all biases =====
The presented work by no means covers \emph{all} possible biases,
but it contributes to a growing literature that tries to tackle this problem.
One could ask if there is one method that is able to cover all possible dataset biases,
but I think there is a strong argument to be made that no general method can exist,
because it is, \eg, not possible to describe in general what is spurious information and what is relevant information.
Nevertheless, finding methods that are more generally applicable is a worthy goal.
One immediate avenue for future work is
the combination of label bias correction and sampling bias correction.
For example, it might be possible to leverage an unlabelled context set for correcting label bias as well.
% ==============

% ============ price of fairness ==========
It also should be mentioned here that compared to training straightforward, bias-unaware classifiers,
the sampling-bias-targeting methods (\ie, the second and the third method)
incur additional costs in terms of computing resources, training time and human labour.
This label-bias-targeting method (\ie, the first one) only impacts the computation of the loss
and does not require additional training data,
so it has a negligible impact training cost.
The second method is the most costly
as the training of \acp{INN} requires a large amount of memory and compute
because \acp{INN} only \emph{transform} the (very high-dimensional) inputs
and never perform anything like the lossy compression that other neural network do.
Furthermore, the method requires additional data with \(s\) labels.
Still, if the additional training cost enables the use of data
that was previously unusable due to strong spurious correlations,
then it can still be worthwhile,
and it is also important to keep in mind that a model can be trained once
and then used millions of times.
This has to be decided on a case-by-case basis,
and unfortunately means that these de-biasing methods will likely not be use prophylactically,
but only if there is an explicit expectation of harm otherwise.
% ==============

% ========= argument that dataset bias is an important topic =======
In any case, correcting dataset bias remains a challenging topic
and one that is increasingly relevant to today's machine learning applications.
Any cutting-edge \ac{ML} system will have to deal with imperfect data,
especially if the collected data is human-related.
The possible effects of these imperfections in the data are certainly highly undesirable:
a photo-tagging service might only work for a certain kind of person;
a speech recognition system might only work for a certain kind of dialect.
If, in these situations, sufficient representative (but unlabelled) data is available,
then the methods presented here can be used to try and correct the problem.
% ==============

% ====== why even work with bad data? =====
Now, one possible objection here is:
if those datasets are of such poor quality,
then maybe we just should not train any \ac{ML} model on these and should not use them to make automated decisions.
While this question is mostly beyond the scope of this document, let me offer some thoughts on this:
It is true that even after the application of de-biasing techniques,
the resulting models still should not be fully trusted,
but they can still ease the burden of tedious manual labour;
similar to an email spam filter which is not perfect, but still very useful.
Or, put another way, it is always important to check what the realistic alternative is;
we should not compare a model to a non-existing perfect ideal, but to the actual solution that would be used instead.
One could imagine a hybrid approach where an automated system makes preliminary decisions,
but random samples are reviewed by humans and decisions can always be challenged.
Ideally, the model itself would tell us about decisions it is uncertain about.

Moreover, two of the three methods presented in this thesis require access to (unlabelled) \emph{unbiased} data,
which is used during the training process.
The remaining method requires summary statistics of the unbiased target dataset.
So, the criticism that we are only learning from biased data does not apply here.
This should allow us to be more confident in the predictions produced by those methods,
because the methods learn from additional, unbiased information.
% ==============

% ============ does your method help with finding clusters unsupervisedly ==========
% I think by "cluster" he means demographic group
% the answer is 'no' :(
The last limitation to mention is that throughout the thesis,
the assumption is made that all relevant `demographic groups' (or `environments' or `subgroups')
are known, or at least that it is known how \emph{many} groups there are.
This goes back to the argument above that no \emph{completely general} method can exist to solve dataset bias;
there has to be \emph{some} inductive bias in order to be able to learn anything.
There have been attempts in the literature to address this,
like \citet{HasSriNamLia18} (see \secref{sec:groundtruth-centric-view-of-bias}),
which only requires knowledge of a lower bound on the size of groups in the given data.
Such an assumption could potentially replace the assumption of knowing the \emph{number} of groups
(as we assume for the third method presented).
The other direction to go in is to give up on groups altogether and try to enforce \emph{individual fairness}.
However, this has its own set of problems;
the foremost of which is requiring a sensible distance function.
Given these issues, it is likely not possible to remove all bias everywhere
(though that should not discourage us from trying to).
% ==============

% ======== data augmentation ========
A topic that was left out of the thesis is \emph{data augmentation},
which refers to the practice of modifying copies of the samples in the training set
with pre-defined transformations, which do not change the ``semantic content'',
such that more samples are available for training.
For image data, typical transformations are rotations, crops, mirroring, Gaussian noise, and slight colour modifications.
If chosen right, data augmentation alone can solve the problems presented in this thesis:
grey-scaling is a transformation that is sometimes used for image data,
and works quite well on the Coloured MNIST problem
(though not perfectly well, because even as shades of grey, the colours are distinguishable).
However, this only works because the spurious feature is so simply here
(after all, Coloured MNIST is mostly intended to be a simple toy dataset).
Grey-scaling would not help with real-world datasets like CelebA.
Furthermore, even if a simple transformation would be able to remove the spurious feature,
knowing \emph{which} transformation will accomplish this is not always as obvious as with Coloured MNIST,
and may require special domain knowledge (like signal processing for audio data).
Thus, the goal for the presented methods here was to make them applicable do any kind of data,
without requiring knowledge of the specific details of how the bias manifests itself in the data,
and without requiring knowledge of which data augmentations can be applied.
However, this is not a recommendation \emph{against} data augmentations;
if you have reason to believe that augmentations will help,
there is no reason not to use them.
% ==============

\section{Potential extensions}\label{sec:potential-extentions}
%
% ======= potential technical improvements ======
% There are also certain technical improvements that could be made.
In general, it would be desirable to have more theoretical bounds on performance.
This likely involves specifying the requirements for the algorithms more precisely.
% which is also useful information for potential users.
The method based on target labels would furthermore benefit from better-calibrated probability outputs.
\acf{GP} classifiers show generally good calibration,
but cannot be easily applied in domains where deep neural networks are used.
On the other hand, neural networks are known to be overconfident
\citep[especially when using ReLU activations;][]{hein2019relu},
so to apply the proposed method to images directly is not straightforward.
The other two proposed methods would benefit from improved adversarial training procedures.
Training these models is not straightforward as the losses have to be balanced
and in some cases, the update schedule needs to be changed.
Both the calibration of neural networks and the stability of adversarial training
are issues that are widely recognised in the \ac{ML} community,
so there is hope that progress will be made on these.
% ==============

% ======== other modalities ======
Another potential extension of this work is to extend it to other modalities.
The experiments were all performed on either tabular or image data
--- the reason predominantly being ease of visualisation ---
so working with audio (especially speech) or text could present new, interesting challenges.
% ==============

% ======== algorithm for identifying which bias is present ======
Furthermore, as it currently stands,
the \ac{ML} practitioner has to know the bias in the data very well in order to choose a method to correct it.
It would be desirable to have a simple algorithm for deciding which method to use.
A related limitation is that the dataset bias considered in this thesis
is assumed to be linked to a special attribute \(s\).
While \(s\) can be very high-dimensional and is not limited to, say, binary attributes,
this nevertheless represents a restriction
that excludes large areas of dataset biases.
% ==============

% ======== what can we do if we don't know there is a bias? ========
A potential -- very ambitious -- extension would be
to try to learn to automatically to detect biases:
\ie an algorithm that can recognise biases without prompting a human operator.
The question is then, where can an \ac{ML} algorithm learn about all possible biases?
This could potentially be done with models like GPT-3 \citep{brown2020language},
which are trained on enormous amounts of text data,
such that they conceivably have learned a lot about what human writers consider to be biases.
There is then perhaps a way to extract what such a model knows about biases
(it could be as easy as a text prompt which asks ``is it okay to discriminate based on gender''),
which would allow us to check whether those biases exist in our data.
Of course, this assumes that humans agree what biases are --
otherwise the model might be very unreliable --
which seems like quite a risky assumption to make.
There are also other potential problems with unsupervised models like GPT-3;
some of which are discussed below.
% ==============

\section{Broader perspective}\label{sec:broader-perspective}
%
% ====== human-machine interface aspect =======
An important issue is the communication between human and machine.
The methods presented in this thesis have all strived to make it easy for human operators to understand what is going on:
the method in \chapref{ch:paper1} is configured with simple statistics;
the method in \chapref{ch:paper2} produces invariant images that can be inspected;
and the method in \chapref{ch:paper3} has the same capability
(though it is not part of the core functionality).
However, this is only a beginning.
It is still not easy to notice that a given dataset has problems,
and machines on their own cannot notice the problem,
because they do not (yet) have a complete understanding about what it is that humans care about;
human preferences are incredibly complex \citep{yudkowsky2011complex} and hard to predict for today's algorithms.
Thus, it is important that machines get feedback early and often,
to keep them aligned with human goals.
% (The aforementioned \citet{stiennon2020learning} is arguably an example of this.)
Applied to the problem of dataset bias,
this could mean visualising correlations in the dataset,
or routinely producing invariant representations to show what the network thinks it is meant to learn.
% ==============

% ========= unsupervised learning =======
One area of machine learning that has recently seen increased interest is unsupervised learning,
and the latter two chapters make use of it to some degree.
The exciting promise of unsupervised learning in general is
that labour-intensive labelling is not needed and so vast amounts of existing, unlabelled data can be put to good use.
One could ask the question whether bias-correcting methods are still needed, with access to so much data.
It could be that, while the data is certainly not perfect,
there is so much of it that the biased parts ``cancel each other out''.
However, recent investigations of the GPT models \citep{radford2018improving,radford2019language,brown2020language} do not seem to support this \citep{khalifa2021distributional}.
One reason for this might be the way these models are trained at the moment:
they maximise the probability assigned to the next token.
Thus, such a model has to account for the wide array of human opinions and assign a non-zero probability to all of them.
So, when asked to summarise a text \citep{stiennon2020learning}, GPT does not give the \emph{best} summary;
rather, it gives a summary that an average person might have written.
However, with the help of a very high-quality labelled dataset (that was expensive to create),
GPT could be finetuned to actually produce very good summaries.
I suspect this pattern of learning the basics in an unsupervised fashion,
and then finetuning with high-quality labels, will continue in the future.
With these massive models,
it is, more than ever, crucial to build tools to make the biases within the models transparent.
Given the black-box nature of neural networks, this represents a major challenge.
% When examining these models trained on massive datasets through the lens of the dataset biases discussed in \chapref{ch:related-work},
% we encounter a novel problem:
% somewhere in the model,
% (an approximation of) the fundamental structures of reality are likely represented,
% but we cannot easily access this.
% ==============

% ======== is it fair that AI has to be fair, if humans tend to be unfair anyhow? ========
%   - yes, because there is no excuse to make something artificially biased, and also, an AI might affect many more people than a single person
%   - it's like asking: isn't it unfair that our president should have a better character than the average person?
It was mentioned before that an \ac{ML} model should be judged by how useful it is,
how it stacks up to realistic alternatives, and not if it is perfect.
However, one has to be careful not to take this philosophy too far.
Namely, one should avoid giving up and saying,
``why should AI need to be fair if humans tend to be bias anyhow?''
The faults of humans has little to do with the question of what we expect of machines.
% and we should certainly not go out of our way to encode the same biases in our creations.
If one of our machines will affect many of our fellow humans,
we would not want it to be harmful, right?
Should we make the machine artificially biased
so that humans do not need to feel too guilty about their own biases?
That way lies folly.
We should always strive to do the best we can.
% ==============

% ======== giving my social perspective viewpoint ========
As AI models (as opposed to mere \ac{ML} models) become more powerful,
the biases as presented in this thesis will likely become less relevant.
However, we can argue that more abstract forms of these problems will still be present.
Spurious correlations in the training data might become less relevant --
because a powerful AI can do its own physics experiments
and thus will be able to figure out the causal structure of our world on its own --
but what is still relevant is the existence of robust, analysable world models,
which it is possible to reason on and do planning with.
The potential trap here is not that the world model is \emph{wrong} per se,
but that it fails to have a useful structure for other tasks.
Furthermore, learning about the physical world will not in and of itself be enough to
become aligned with \emph{human values},
which is here a catch-all term for
what morality is or what good is and what wrong is.
% the things that we want more of, and what bad things are.
Such questions may appear very different from the issue of label bias
and objectives like demographic parity,
but if we take the problem of classical algorithmic fairness to be
that we want the model to treat our fellow humans right,
then this is just the generalisation of that.
% ==============