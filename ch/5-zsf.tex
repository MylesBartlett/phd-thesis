\chapter{Zero-shot Fairness with Invisible Demographics}\label{ch:zsf}
A shortcoming of the approach from the previous chapter
is its reliance on a context/pre-training set which has \(s\) labels.
As discussed, it is necessary to make use of \emph{some} kind of side information,
but perhaps we can relax some requirements.
In particular, while it is already easier to collect data without \(y\) labels (but with \(s\) labels),
it is even easier to collect data without \emph{any} label.
So, requiring only an unlabeled context set would improve the applicability of the method.
This chapter presents an approach based on that idea.

The setting is very similar to the previous chapter:
the training set suffers from severe sampling bias, but we have access to a (mostly) unbiased context set.
The change is that the context set may be completely unlabeled,
but in exchange, we have some stronger requirements for the training set.
Namely, the holes left by the sampling bias may not be so numerous
as to make \(s\) and \(y\) completely indistinguishable.
In the previous chapter, the example of colored MNIST had a training set
where there was a strict one-to-one mapping of color and digit;
but this kind of blending of \(s\) and \(y\) into one is not the focus of this chapter.
The focus is instead on a setting where the training set lacks certain combinations of \(s\) and \(y\),
which results in poor predictions for these combinations on the test set (or deployment setting)
where those combinations do occur.
We refer to these missing combinations as \emph{invisible demographics}.
The value of \(s\) acts as a spurious attribute and should be ignored in order to make accurate predictions.

As in the previous chapter, the first step is to train a neural network to produce an invariant representation.
The second step is then to train a classifier on that.
The invariant representation is trained by doing distribution matching between the training set and the context set.
Finally, the training set is transformed into the invariant representation,
to serve as training set for the classifier.
