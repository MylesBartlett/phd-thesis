\chapter{Summary of contributions}\label{ch:content}
The main contributions of this thesis are as follows.
All presented approaches rely on some form of side information with allows us to overcome the dataset bias.
This side information is always significantly easier to obtain than unbiased data.

\section{Using target labels to mitigate label bias}\label{sec:target-labels}
\citet{kehrenberg2020tuning} (\chapref{ch:paper1}) is predominantly concerned with label bias.
The main idea is that we make use of pseudo labels (or \emph{target labels})
to implicitly learn from a \emph{balanced} dataset,
in which \(y\perp s\) holds, and which thus satisfies \acf{DP}.
This falls under the area of \emph{fair classifiers} discussed in \secref{fair-classifiers}

We can interpret the contributions of this publication in two ways.
The first corresponds to the definition-centric view of dataset bias,
and the second to the ground-truth-centric view.
\begin{enumerate}
  \item
    We can say that the classifier should satisfy demographic parity in its predictions,
    and learning from a balanced training set is just one particular way to achieve this.
    In this view, the pseudo labels have no deeper meaning and are just a computational trick.
  \item
    We can see the training set as a corrupted version of a true dataset, which is balanced (\(y\perp s\)),
    and so, by learning from these pseudo labels, we are simply approximating the true dataset.
    However, we do not actually have access to the true dataset; we only know that it is balanced.
    In order to evaluate the trained model, we compute fairness metrics with respect to \ac{DP}.
\end{enumerate}
Within the paper, we sometimes jump between these two views.

While the main focus is on label bias,
the experiments are performed on real-world fairness datasets,
which also display a significant amount of sample bias (disadvantaged groups are underrepresented).
Furthermore, in addition to the result for \ac{DP}, we also show that the proposed scheme improves \acf{EOpp}.

In order to construct the target labels,
we use side information about summary statistics for an balanced training set.
This allows us to target a specific balanced set, instead of just any balanced set.
In other words, rather than just enforcing \ac{DP}, the method gives control over the target rates \(P(\hat{y}=1|s)\).
These target rates are the only parameters of the model.
The target labels represent an uncertain estimate of labels corresponding to a balanced dataset.
Via the sum rule of probabilities, it is possible to express the model likelihood in terms of these target labels,
such that maximizing the likelihood corresponds to improving the prediction of the target labels
instead of the given (biased) training labels.

\section{Using a representative set to overcome severe sampling bias}\label{sec:nifr}
In the setting from the previous paper \citep{kehrenberg2020tuning},
labels were untrustworthy because they had been flipped;
a phenomenon we referred to as \emph{label bias}.
However, flipped labels is not the only way that labels can become untrustworthy.
Another way is \emph{sampling bias}, which is the subject of \citet{kehrenberg2020nullsampling} and \citet{kehrenberg2020zeroshot} (\twochaprefs{ch:paper2}{ch:paper3}).

As an example, consider a dataset where the task is to distinguish smiling from non-smiling faces.
Say, we initially have a very diverse and balanced training set,
but then, from the set of smiling faces, we remove nearly all samples where the person does not have red hair,
and from the set of non-smiling faces, we remove nearly all samples with non-black hair.
The result is that even though the labels are still correct, they do not point reliably to the right target anymore.
As hair colour is easier to recognise with a \ac{CNN}, the labels have effectively been turned into hair colour labels.
In other words, hair colour has become a \emph{spurious attribute}.
In the following, we denote the spurious attribute with \(s\),
as it takes a role that is very similar to that of the sensitive attribute that was also denoted by \(s\).

The method, proposed in the previous paper (\chapref{ch:paper1}),
is not able to deal with such a dataset bias as we can easily see:
Say, \emph{smiling} corresponds to \(y=1\) and \emph{not smiling} to \(y=0\);
furthermore, let red hair correspond to \(s=1\), black hair to \(s=0\), and all other hair colours to \(s=2\).
Then, the problem with the described dataset is, that it mostly consists of samples with \(y=0\wedge s=0\)
and those with \(y=1\wedge s=1\).
If we call \(P(y=1|s=s')\) the acceptance rate,
then the problem can be described as one of very different acceptance rates in the hair colour groups given by \(s\).
This is the problem tackled in the previous paper,
and yet, if we were to equalize the acceptance rates with the method there,
the result would be very incorrect.
The issue is that we would treat the labels as incorrect, when in truth, they are correct.

To deal with sampling bias, a different approach is needed.
Indeed, the problem, as posed, is not solvable in the general case.
To make headway with this problem, we introduce the concept of a \emph{representative set}.
This set is not subject to the sampling bias,
but is unlabelled (with respect to $y$) and so does not by itself suffice for training.
However, this set does have labels for the spurious attribute \(s\).
This allows us to learn an \emph{invariant representation},
\ie, a representation of the input features \(x\) which is invariant to the spurious attribute.
This kind of representation is equivalent to a fair representation -- as described in \secref{sec:fair-representation} --
which is invariant to a sensitive attribute.
With the invariant representation of the training set,
a classifier can then be trained to accurately predict the class label \(y\).
The invariant representation cannot be learned from the training set
because there, due to the sampling bias, \(s\) and \(y\) are not distinguishable.

A parallel to the previous paper is that the method makes use of side information
(in this case the representative set)
in order to overcome the bias in the training set.

The method presented in \citet{kehrenberg2020nullsampling} (\chapref{ch:paper2}) is based on the idea of \emph{null-sampling},
which refers to zeroing out part of an encoding,
and then reconstructing the modified encoding as if it were a normal encoding.
Before we apply null-sampling, an encoding of the input \(x\) is learned that is split into two parts:
\(z_u\), which has no information about the spurious attribute \(s\),
and \(z_b\), which has all information needed to reconstruct \(x\) that is not contained in \(z_u\).
\(z_u\) is ensured to be not predictive of \(s\) via adversarial training.
During null-sampling, \(z_b\) is zeroed out, and after decoding it,
we obtain an invariant representation \emph{in the data domain}.
The fact that it is in the data domain makes it interpretable.

The described method works particularly well with \acp{INN},
as they ensure that no information is lost that is unrelated to \(s\).

We perform experiments on the Coloured MNIST dataset (as described in \secref{sec:groundtruth-centric-view-of-bias}),
which has a one-to-one mapping between the class label (\ie, digit) and the spurious attribute (colour)
in the training set.
As colour is ``easier'' to learn, a neural network will learn to predict \(s\) instead of \(y\).
For additional experiments on the CelebA dataset and the UCI Adult Income dataset,
we deliberately apply sampling bias to the training set and then apply our method.

\section{Using an unlabelled deployment set to overcome sam\-p\-ling bias}\label{sec:zsf}
A shortcoming of the approach from the previous publication \citep{kehrenberg2020nullsampling}
is its reliance on a representative set which has \(s\) labels.
As discussed, it is necessary to make use of \emph{some} kind of side information,
but perhaps we can relax some requirements.
In particular, while it is already easier to collect data without \(y\) labels (but with \(s\) labels),
it is \emph{even} easier to collect data without \emph{any} label.
Thus, requiring only an unlabelled context set would improve the applicability of the method.
\citet{kehrenberg2020zeroshot} (\chapref{ch:paper3}) presents an approach based on that idea.

The setting is very similar to the previous publication \citep{kehrenberg2020nullsampling}:
the training set suffers from severe sampling bias, but we have access to a (mostly) unbiased \emph{deployment set}.
The idea is that the deployment set corresponds to the setting in which the model is meant to be deployed.
The change is that this additional set may be completely unlabelled,
but in exchange, we have some stronger requirements for the training set.
Namely, the holes left by the sampling bias may not be so numerous
as to make \(s\) and \(y\) completely indistinguishable.
In the previous paper, the example of Coloured MNIST had a training set
where there was a strict one-to-one mapping of colour and digit;
but this kind of blending of \(s\) and \(y\) into one is not the focus of this paper.
The focus is instead on a setting where the training set lacks certain combinations of \(s\) and \(y\),
which results in poor predictions for these combinations on the test set (or deployment setting)
where those combinations do occur.
We refer to these missing combinations as \emph{invisible demographics}.
The value of \(s\) acts as a spurious attribute and should be ignored in order to make accurate predictions.

As in the previous paper, the first step is to train a neural network to produce an invariant representation.
The second step is then to train a classifier on that.
The invariant representation is trained by doing distribution matching between the training set and the deployment set.
Finally, the training set is transformed into the invariant representation,
to serve as training set for the classifier.

The distribution matching is realized with adversarial networks which compare batches of samples,
in order to try to distinguish data drawn from the training set and the deployment set.
This process requires balanced batches as an inductive bias,
becaus the network will only learn the intended difference between training and deployment set,
if the drawn batches exhibit this difference.
For example, if batches drawn from Coloured MNIST differ not in colour but in digit class,
then distribution matching will learn to change digit shapes.
Balancing batches from the training set is easily possible with the available labels,
but those are not available for the deployment set,
so we use clustering techniques to identify the different groups in the deployment set,
and then draw samples for the batches at equal rate from all clusters.
It is important to note here that imperfections in the clustering are not a problem
as long as the show on average the intended difference between training and deployment set.
