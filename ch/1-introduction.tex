\chapter{Introduction}\label{ch:introduction}
\section{Problem statement}
In order for \ac{ML} systems to be used more widely,
they have to become more trustworthy.
The susceptibility of \acp{ANN} to adversarial attacks has been well documented, but there are other problems as well.
This includes their opaqueness and their general tendency to take shortcuts;
leading to situations where \acp{ANN} do not do `what we meant', but just what they were explicitly told to do.

This problem becomes especially severe when the training data is biased in some way,
which, by default, makes the \ac{ML} system copy the bias, or even exacerbate it.
When applied in the deployment setting, the system will then not behave in the desired way.
The topic of this thesis is how to deal with biased data;
what types of bias exist and how to correct it if possible.

\section{Motivation and aims}
Many datasets with person-related features display biases when examined by common fairness criteria.
This can range from relatively harmless biases,
like men being, on average, older than women in the CelebA dataset \citep{liu2015faceattributes},
to more serious ones,
like black men being several times less likely to receive bail than white men in the COMPAS dataset \citep{angwin2016machine}.
In the absence of truly `fair' datasets, our methods have to be able to deal with these biases.

Now, one possible objection here is:
if those datasets are so bad,
then maybe we just should not train any \ac{ML} model on these and should not use them to make automated decisions.
While this question is mostly beyond the scope of this document, let me offer some thoughts on this:
It's true that even after the application of de-biasing techniques,
the resulting models still should not be fully trusted,
but they can still ease the burden of checking everything manually;
similar to an email spam filter which is not perfect, but still very useful.
Or, put another way, it is always important to check what the realistic alternative is;
we should not compare a model to a non-existing perfect ideal, but to the actual thing that would be used instead.
One could imagine a hybrid approach where an automated system makes preliminary decisions,
but random samples are reviewed by humans and decisions can always be challenged.
Ideally, the model itself would tell us about decisions it is uncertain about.

Moreover, two of the three methods presented in this thesis require access to (unlabeled) \emph{unbiased} data,
which is used during the training process.
So, the criticism that we are only learning from biased data does not apply here.
This should allow us to be more confident in the predictions of those methods.

Throughout this thesis, the aim is to learn a model from biased training data,
which gives fair predictions on an unbiased training set.
It is thus not enough to simply optimize the cross-entropy on the training set;
we have to change the optimization target to achieve our stated goal.
While it is possible to extend notions of fairness beyond classification tasks,
the vast majority of work in this area concerns classification only and that will be the case here as well.

This thesis will discuss two kinds of dataset bias: \emph{label bias} and \emph{sampling bias}.
In both cases,
we consider a classification problem in which the class labels $y$ needs to be predicted from input features $\vx$.

In the setup for \emph{label bias},
there is additionally a \emph{sensitive attribute} $s$ associated with each input.
This sensitive attribute usually encodes membership in a demographic group, such as gender,
but more generally is a feature that should not be used to make predictions for legal or ethical reasons.
We will also refer to the set of all samples with a specific sensitive attribute as one \emph{demographic group}.
In our examples, $s$ and $y$ are binary variables, but this does not have to be the case.
The label bias now manifests itself in a specific way:
depending on $s$, labels are either flipped from $y=0$ to $y=1$ or vice versa,
\ie, there is an error in the labels which is correlated with the sensitive attribute.

For \emph{sampling bias},
there is also a special attribute $s$, but it does not necessarily have to denote a \emph{sensitive} attribute;
it can more generally be a \emph{spurious} variable which is correlated with the class label $y$ in the training set,
but is not truly predictive of $y$ in the general case.
Sampling bias then means that the training set is not uniformly sampled from the underlying distribution,
instead, sampling depends on $s$ and $y$;
\eg, there might be almost no samples of $s=0$ and $y=1$ in the training set.
The effect is that \ac{ML} models are tempted to take shortcuts and use $s$ as a shorthand for $y$.

As the training set is biased in these settings,
evaluation metrics like accuracy should be computed with respect to an \emph{unbiased} test set.
However, such a set is often not available or has to be constructed, which leaves open the question how to construct it.
Furthermore, the overall accuracy often does not capture all that we care about.
For these and other reasons,
it is usually necessary to go beyond metrics like accuracy, when evaluating models in these settings.
Concretely, we use \emph{fairness metrics} which are based on certain definitions of fairness.
These fairness metrics can be computed for predictions on a \emph{biased} test set
and still give us an idea of the bias in the model.
Even with an \emph{unbiased} test set, these metrics are useful,
because they tell us in what specific way, if any, a model has failed to correct for the training set bias.

The simplest fairness definition is \ac{DP}, also called statistical parity or independence.


\section{Claims and contributions}%
\label{sec:claims-contributions}
This thesis is based on 3 publications, corresponding to \rangechapref{ch:target-labels}{ch:zsf}.
The first one is concerned with \emph{label bias} and the other two with \emph{sampling bias}.
\begin{enumerate}
  \item Thomas Kehrenberg, Zexun Chen, and Novi Quadrianto (2020). ``Tuning Fairness by Balancing Target Labels.''
    In: \emph{Frontiers in Artificial Intelligence} 3, p.\ 33.
    \citep{kehrenberg2020tuning}
  \item Thomas Kehrenberg, Myles Bartlett, Oliver Thomas, and Novi Quadrianto (2020).
    ``Null-sampling for Interpretable and Fair Representations.''
    In: \emph{Computer Vision - ECCV 2020}. Glasgow, UK: Springer International Publishing.
    \citep{kehrenberg2020nullsampling}
  \item Thomas Kehrenberg, Viktoriia Sharmanska, Myles Bartlett, and Novi Quadrianto (2021).
    ``Zero-shot Fairness with Invisible Demographics.''
    Under review.
    \citep{kehrenberg2020zeroshot}
\end{enumerate}
A shorter version of \citet{kehrenberg2020tuning} was published as a workshop paper:
\begin{itemize}
  \item Thomas Kehrenberg, Zexun Chen, and Novi Quadrianto (2018).
    ``Interpretable Fairness via Target Labels in Gaussian Process Models.''
    In: \emph{Workshop on Ethical, Social and Governance Issues in AI at NeurIPS}. Montreal, Canada.
\end{itemize}

The contributions of these publications are as follows.
All presented approaches rely on some form of side information which helps us overcome the dataset bias.
This side information is always significantly easier to obtain than unbiased training data.

To tackle the problem of label bias, we introduce the concept of \emph{target labels} $\bar{y}$.
These represent an uncertain estimate of the true labels.
The side information needed here is true acceptance rate for each of the demographic groups: $P(\bar{y}=1|s=s')$
(assuming a binary label $y$).
Via the sum rule, it is then possible to express the likelihood in terms of these target labels,
such that maximizing the likelihood corresponds to improving the prediction of the target labels
instead of the given (biased) training labels.

\thomas{to follow: summary of other papers}
