\chapter{Introduction}\label{ch:introduction}
\section{Problem statement}
In order for \ac{ML} systems to be used more widely,
they have to become more trustworthy \citep{hleg2019ethics}.
The susceptibility of deep \acp{NN} to adversarial attacks has been well documented, but there are other problems as well.
This includes their opaqueness and their general tendency to take shortcuts;
leading to situations where neural networks do not do `what we meant', but just what they were explicitly told to do.

This problem becomes especially severe when the training data is biased in some way,
which, by default, makes the \ac{ML} system internalize the bias, or, in some cases, even exacerbate it.
When applied in the deployment setting, the system will then not behave in the desired way.
The topic of this thesis is how to deal with biased data,
where the bias is inextricably linked to a special attribute $s$.
% and what types of biases exist and how to correct them if possible.

\section{Motivation and aims}
Many datasets with person-related features display biases when examined by common fairness criteria.
This can range from relatively harmless biases,
like men being, on average, older than women in the CelebA dataset \citep{liu2015faceattributes},
to more serious ones,
like black men being several times less likely to receive bail than white men in the COMPAS dataset \citep{angwin2016machine}.
In the absence of truly `fair' datasets, our methods have to be able to deal with these biases.

Now, one possible objection here is:
if those datasets are so bad,
then maybe we just should not train any \ac{ML} model on these and should not use them to make automated decisions.
While this question is mostly beyond the scope of this document, let me offer some thoughts on this:
It's true that even after the application of de-biasing techniques,
the resulting models still should not be fully trusted,
but they can still ease the burden of checking everything manually;
similar to an email spam filter which is not perfect, but still very useful.
Or, put another way, it is always important to check what the realistic alternative is;
we should not compare a model to a non-existing perfect ideal, but to the actual thing that would be used instead.
One could imagine a hybrid approach where an automated system makes preliminary decisions,
but random samples are reviewed by humans and decisions can always be challenged.
Ideally, the model itself would tell us about decisions it is uncertain about.

Moreover, two of the three methods presented in this thesis require access to (unlabelled) \emph{unbiased} data,
which is used during the training process.
So, the criticism that we are only learning from biased data does not apply here.
This should allow us to be more confident in the predictions of those methods.

Throughout this thesis, the aim is to learn a model from biased training data,
which gives fair predictions on an unbiased test set.
It is thus not enough to simply optimize the cross-entropy on the training set;
we have to change the optimization target to achieve our stated goal.
While it is possible to extend notions of fairness beyond classification tasks,
the vast majority of work in this area concerns classification only and that will be the case here as well.

This thesis will discuss two kinds of dataset bias: \emph{label bias} and \emph{sampling bias}.
In both cases,
we consider a classification problem in which the class labels $y$ needs to be predicted from input features $\vx$.

In all cases, there is a special attribute $s$ associated with each input.
This attribute can have different meanings:
In the setting of \emph{label bias}, $s$ usually encodes membership in a demographic group, such as gender,
but more generally is a feature that should not be used to make predictions for legal or ethical reasons.
(See below for other possible meanings of $s$.)
In this setting, we call $s$ the \emph{sensitive attribute} (because it carries sensitive information).
We will often refer to the set of all samples with a specific sensitive attribute as one \emph{demographic group}.
In most of the examples, $s$ and $y$ are binary variables, but this does not have to be the case.
The label bias is related to $s$ in a very specific way:
depending on the value of $s$, labels are either flipped from $y=0$ to $y=1$ or vice versa,
\ie, there is an error in the labels which is correlated with the sensitive attribute.

For \emph{sampling bias},
there is also a special attribute $s$, but it does not necessarily have to denote a \emph{sensitive} attribute;
it can more generally be a \emph{spurious} variable which is correlated with the class label $y$ in the training set,
but is not truly predictive of $y$ in the general case.
Or, it can refer to natural \emph{subgroups} of the $y$ classes.
Sampling bias then means that the training set is not uniformly sampled from the underlying distribution:
Instead, sampling depends on $s$ and $y$;
\eg, there might be almost no samples of $s=0$ and $y=1$ in the training set.
The effect is that \ac{ML} models are tempted to take shortcuts and use $s$ as a shorthand for $y$.

The overall goal in all cases is to make the classifier invariant to $s$.
We can measure the invariance to $s$ directly by using \emph{fairness metrics}
on the predictions of the classifier.
In contrast to the accuracy metric,
these fairness metrics give a more complete picture of how invariant the classifier is.
This is especially true if evaluation is done on an imbalanced set
-- for example because a truly unbiased test set is not available.
In this case, accuracy can be highly misleading,
because a good performance in the majority class can hide a poor performance in a minority class.
Fairness metrics do not suffer from this problem,
because they specifically look at the results in the different subgroups.
However, we typically try to evaluate our models on a test set that is as unbiased as possible,
in order to have a meaningful accuracy.

The fairness definition with the clearest interpretation in the described setup of label bias and sampling bias is \acf{DP},
also called statistical parity or independence.
It demands that the predictions $\hat{y}$ be independent of the sensitive attribute \(s\).
So, for binary $s$ and $y$:
\begin{align}
  P(\hat{y}=1|s=0) &= P(\hat{y}=1|s=1)~.
  \label{eq:dp-def}
\end{align}
There are multiple \ac{DP} \emph{metrics} which track how close the predictions are to satisfying the equality,
among them the difference and the ratio of the terms on the two sides of the equation.

When we previously talked about unbiased datasets,
we did not specify exactly what this meant,
and that is because this can differ from task to task,
but one way to define it is as a \emph{balanced} dataset where all combinations of $s$ and $y$ occur at the same rate:
\begin{align}
  \label{eq:balanced-dataset}
  P(y=0,s=0)=P(y=0,s=1)=P(y=1,s=0)=\dots
\end{align}
In such a dataset, we have $y \perp s$, and thus,
perfect predictions ($\hat{y}=y$) on this dataset will satisfy $\hat{y} \perp s$ and hence \ac{DP}.
We can conclude that perfect accuracy on a balanced test set implies \acl{DP} (the opposite does not hold).
However, if a model's predictions satisfy \ac{DP} on a \emph{biased} dataset,
then they cannot be perfectly accurate with respect to that dataset's biased labels anymore,
which makes sense because the goal is to be accurate to the \emph{unbiased} dataset.
This leads to a fairness-accuracy trade-off on biased test sets.

The other two fairness definitions which are commonly used are \ac{EOpp} and \ac{EOdds}.
These do not require the prediction $\hat{y}$ to be independent of $s$,
but they do require that the model makes equally high-quality predictions for all values of $s$.
In particular, for \ac{EOpp}, the \acp{TPR} need to be the same for all demographic groups
(again for binary $s$ and $y$):
\begin{align}
  \label{eq:eopp-def}
  P(\hat{y}=1|y=1,s=0) = P(\hat{y}=1|y=1,s=1)~.
\end{align}
\ac{EOdds} has the same requirement, but also extends it to the \acp{TNR}:
\begin{align}
  P(\hat{y}=y'|y=y',s=0) = P(\hat{y}=y'|y=y',s=1)\quad\forall y'~.
  \label{eq:eodds-def}
\end{align}
Just like \ac{DP},
\ac{EOpp} and \ac{EOdds} can help us evaluate how much a classifier is affected by the bias in the training set.

% \section{Claims and contributions}%
\section{List of publications and author contributions}%
\label{sec:claims-contributions}
This thesis is based on 3 publications, corresponding to \rangechapref{ch:paper1}{ch:paper3}.
The first one is concerned with \emph{label bias} and the other two with \emph{sampling bias}.

\subsection{Publication 1}
\begin{refsection}[allreferences]
    \nocite{kehrenberg2020tuning}
    \printbibliography[heading=none]
\end{refsection}
\noindent A shorter version was published as a workshop paper:
\begin{refsection}[allreferences]
    \nocite{kehrenberg2018interpretable}
    \printbibliography[heading=none]
\end{refsection}
\noindent\textsc{Contributions:}
\begin{itemize}
  \item I conceived the idea of using Target Labels to target a balanced set. I developed the proof from a starting point that my supervisor pointed me to. I wrote all of the code dealing with the modified loss function and nearly all of the remaining code as well. I ran most of the experiments and wrote all of the methods section and most of the remaining text as well.
  \item Z. Chen was a discussion partner and helped run the experiments, and wrote some parts of the code.
  \item N. Quadrianto suggested the initial direction of the work, was a discussion partner, and helped write the introduction and related work.
\end{itemize}

\subsection{Publication 2}
\begin{refsection}[allreferences]
    \nocite{kehrenberg2020nullsampling}
    \printbibliography[heading=none]
\end{refsection}%
\noindent\textsc{Contributions:}
\begin{itemize}
  \item I conceived the idea of using an \acf{INN} and a representative set to learn an invariant representation. I wrote a large part of the code, and ran about half the experiments. I wrote a significant part of the text.
  \item M. Bartlett developed a large part of the tricks needed for training the \ac{INN} successfully. He wrote a significant part of the code, and of the text.
  \item O. Thomas helped with writing the code and with running the experiments.
  \item N. Quadrianto gave feedback on the progress and suggested directions to explore.
\end{itemize}

\subsection{Publication 3}
\begin{refsection}[allreferences]
    \nocite{kehrenberg2020zeroshot}
    \printbibliography[heading=none]
\end{refsection}%
\noindent\textsc{Contributions:}
\begin{itemize}
  \item I developed the idea of distribution matching as an extension of the previous paper. In addition, I tried to achieve similar goals by applying clustering to an unlabelled auxiliary set, with supervision from the labelled training set. I wrote all of the initial implementation of the method, and a large part of the later refined implementation. I ran the majority of the experiments.
  \item V. Sharmanska developed the original link to a fairness problem and wrote parts of the introduction and related work and contributed to other sections.
  \item M. Bartlett introduced the idea of batches-of-bags. He also improved the \ac{NN} architectures of the encoder and the discriminator, ran experiments, wrote the sections on architecture and contributed to other parts of the paper.
  \item N. Quadrianto suggested how to combine the two research directions that I had into a coherent whole. He also gave general feedback and suggested directions to explore.
\end{itemize}

\section{Structure of document}%
\label{sec:thesis-structure}
In \chapref{ch:related-work}, I present related work,
concentrating on the area of algorithmic fairness and dataset bias.
\Rangechapref{ch:paper1}{ch:paper3} constitute the main contribution of this thesis.
Each of these chapters starts with a brief section which positions the chapter in the context of the this document.
This is followed by the contents of publications.
In the final chapter, \chapref{ch:conclusion}, I present conclusions from the main work and directions for future work.
