\chapter{Introduction}\label{ch:introduction}
\section{Problem statement}
In order for \ac{ML} systems to be used more widely,
they have to become more trustworthy.
The susceptibility of \acp{ANN} to adversarial attacks has been well documented, but there are other problems as well.
This includes their opaqueness and their general tendency to take shortcuts;
leading to situations where \acp{ANN} do not do `what we meant', but just what they were explicitly told to do.

This problem becomes especially severe when the training data is biased in some way,
which, by default, makes the \ac{ML} system internalize the bias, or in some cases even exacerbate it.
When applied in the deployment setting, the system will then not behave in the desired way.
The topic of this thesis is how to deal with biased data;
what types of biases exist and how to correct them if possible.

\section{Motivation and aims}
Many datasets with person-related features display biases when examined by common fairness criteria.
This can range from relatively harmless biases,
like men being, on average, older than women in the CelebA dataset \citep{liu2015faceattributes},
to more serious ones,
like black men being several times less likely to receive bail than white men in the COMPAS dataset \citep{angwin2016machine}.
In the absence of truly `fair' datasets, our methods have to be able to deal with these biases.

Now, one possible objection here is:
if those datasets are so bad,
then maybe we just should not train any \ac{ML} model on these and should not use them to make automated decisions.
While this question is mostly beyond the scope of this document, let me offer some thoughts on this:
It's true that even after the application of de-biasing techniques,
the resulting models still should not be fully trusted,
but they can still ease the burden of checking everything manually;
similar to an email spam filter which is not perfect, but still very useful.
Or, put another way, it is always important to check what the realistic alternative is;
we should not compare a model to a non-existing perfect ideal, but to the actual thing that would be used instead.
One could imagine a hybrid approach where an automated system makes preliminary decisions,
but random samples are reviewed by humans and decisions can always be challenged.
Ideally, the model itself would tell us about decisions it is uncertain about.

Moreover, two of the three methods presented in this thesis require access to (unlabeled) \emph{unbiased} data,
which is used during the training process.
So, the criticism that we are only learning from biased data does not apply here.
This should allow us to be more confident in the predictions of those methods.

Throughout this thesis, the aim is to learn a model from biased training data,
which gives fair predictions on an unbiased training set.
It is thus not enough to simply optimize the cross-entropy on the training set;
we have to change the optimization target to achieve our stated goal.
While it is possible to extend notions of fairness beyond classification tasks,
the vast majority of work in this area concerns classification only and that will be the case here as well.

This thesis will discuss two kinds of dataset bias: \emph{label bias} and \emph{sampling bias}.
In both cases,
we consider a classification problem in which the class labels $y$ needs to be predicted from input features $\vx$.

In the setup for \emph{label bias},
there is additionally a \emph{sensitive attribute} $s$ associated with each input.
This sensitive attribute usually encodes membership in a demographic group, such as gender,
but more generally is a feature that should not be used to make predictions for legal or ethical reasons.
We will also refer to the set of all samples with a specific sensitive attribute as one \emph{demographic group}.
In our examples, $s$ and $y$ are binary variables, but this does not have to be the case.
The label bias now manifests itself in a specific way:
depending on $s$, labels are either flipped from $y=0$ to $y=1$ or vice versa,
\ie, there is an error in the labels which is correlated with the sensitive attribute.

For \emph{sampling bias},
there is also a special attribute $s$, but it does not necessarily have to denote a \emph{sensitive} attribute;
it can more generally be a \emph{spurious} variable which is correlated with the class label $y$ in the training set,
but is not truly predictive of $y$ in the general case.
Sampling bias then means that the training set is not uniformly sampled from the underlying distribution,
instead, sampling depends on $s$ and $y$;
\eg, there might be almost no samples of $s=0$ and $y=1$ in the training set.
The effect is that \ac{ML} models are tempted to take shortcuts and use $s$ as a shorthand for $y$.

When evaluating models in these setup,
the problem is often that we do not have access to the underlying true unbiased dataset.
In this case, we cannot just rely on metrics like accuracy to compare models:
the model is not being evaluated on the right dataset, so accuracy is not that informative.
For these cases, where we do not have access to a truly unbiased test set,
we use \emph{fairness metrics}, which do not rely on access to an unbiased set.
These fairness metrics, which are based on different fairness definitions,
are then used alongside metrics like accuracy to evaluate how well a model has corrected for the training set bias.
Note however, that evaluation on a truly unbiased dataset is still preferred:
fairness metrics make assumptions about what it means to be unbiased that might not fit the actual use case.

While fairness metrics are primarily useful when no truly unbiased test sets are provided,
even when they are provided,
these metrics can be useful in identifying where models retained some of the bias from the training set.

The fairness definition that most fits the described setup of label bias and sampling bias is \acf{DP},
also called statistical parity or independence.
It demands that the predictions $\hat{y}$ be independent of the sensitive attribute \(s\).
So, for binary $s$ and $y$:
\begin{align}
  P(\hat{y}=1|s=0) &= P(\hat{y}=1|s=1)~.
  \label{eq:dp-def}
\end{align}
There are multiple \ac{DP} \emph{metrics} which track how close the predictions are to satisfying the equality,
among them the difference and the ratio of the terms on the two sides of the equation.

When we previously talked about unbiased datasets,
we did not specify exactly what this meant,
and that is because this can differ from task to task,
but one way to define it is as a \emph{balanced} dataset where all combinations of $s$ and $y$ occur at the same rate:
\begin{align}
  \label{eq:balanced-dataset}
  P(y=0,s=0)=P(y=0,s=1)=P(y=1,s=0)=\dots
\end{align}
In such a dataset, we have $y \perp s$, and thus,
perfect predictions ($\hat{y}=y$) on this dataset will satisfy $\hat{y} \perp s$ and hence \ac{DP}.
We can conclude that perfect accuracy on a balanced test set implies \acl{DP} (the opposite does not hold).
However, if a model's predictions satisfy \ac{DP} on a \emph{biased} dataset,
then they cannot be perfectly accurate with respect to that dataset's biased labels anymore,
which makes sense because the goal is to be accurate to the \emph{unbiased} dataset.
This leads to a fairness-accuracy trade-off on biased test sets.

The other two fairness definitions which are commonly used are \ac{EOpp} and \ac{EOdds}.
These do not require the prediction $\hat{y}$ to be independent of $s$,
but they do require that the model makes equally high-quality predictions for all values of $s$.
In particular, for \ac{EOpp}, the \acp{TPR} need to be the same for all demographic groups
(again for binary $s$ and $y$):
\begin{align}
  \label{eq:eopp-def}
  P(\hat{y}=1|y=1,s=0) = P(\hat{y}=1|y=1,s=1)~.
\end{align}
\ac{EOdds} has the same requirement, but also extends it to the \acp{TNR}:
\begin{align}
  P(\hat{y}=y'|y=y',s=0) = P(\hat{y}=y'|y=y',s=1)\quad\forall y'~.
  \label{eq:eodds-def}
\end{align}
Just like \ac{DP},
\ac{EOpp} and \ac{EOdds} can help us evaluate how much a classifier is affected by the bias in the training set.

\section{Claims and contributions}%
\label{sec:claims-contributions}
This thesis is based on 3 publications, corresponding to \rangechapref{ch:target-labels}{ch:zsf}.
The first one is concerned with \emph{label bias} and the other two with \emph{sampling bias}.
\begin{enumerate}
  \item Thomas Kehrenberg, Zexun Chen, and Novi Quadrianto (2020). ``Tuning Fairness by Balancing Target Labels.''
    In: \emph{Frontiers in Artificial Intelligence} 3, p.\ 33.
    \citep{kehrenberg2020tuning}
  \item Thomas Kehrenberg, Myles Bartlett, Oliver Thomas, and Novi Quadrianto (2020).
    ``Null-sampling for Interpretable and Fair Representations.''
    In: \emph{Computer Vision - ECCV 2020}. Glasgow, UK: Springer International Publishing.
    \citep{kehrenberg2020nullsampling}
  \item Thomas Kehrenberg, Viktoriia Sharmanska, Myles Bartlett, and Novi Quadrianto (2021).
    ``Zero-shot Fairness with Invisible Demographics.''
    Under review.
    \citep{kehrenberg2020zeroshot}
\end{enumerate}
A shorter version of \citet{kehrenberg2020tuning} was published as a workshop paper:
\begin{itemize}
  \item Thomas Kehrenberg, Zexun Chen, and Novi Quadrianto (2018).
    ``Interpretable Fairness via Target Labels in Gaussian Process Models.''
    In: \emph{Workshop on Ethical, Social and Governance Issues in AI at NeurIPS}. Montreal, Canada.
\end{itemize}

The contributions of these publications are as follows.
All presented approaches rely on some form of side information which helps us overcome the dataset bias.
This side information is always significantly easier to obtain than unbiased training data.

To tackle the problem of label bias, we introduce the concept of \emph{target labels} $\bar{y}$.
These represent an uncertain estimate of the true labels.
The side information needed here is true acceptance rate for each of the demographic groups: $P(\bar{y}=1|s=s')$
(assuming a binary label $y$).
Via the sum rule, it is then possible to express the likelihood in terms of these target labels,
such that maximizing the likelihood corresponds to improving the prediction of the target labels
instead of the given (biased) training labels.

The next approach is concerned with sampling bias.
We consider a very extreme sampling bias from which it is not possible to recover without additional information.
Concretely, there is a one-to-one mapping between the class label $y$ and a spurious attribute $s$ in the training set,
such that the model learns the (easier-to-predict) $s$ instead of $y$.
In order to nudge the model towards learning the right target,
we make use of a \emph{context set} which is approximately balanced ($y\perp s$),
but where we only have access to the $s$ labels and \emph{not} the $y$ labels.
Thus, the context set cannot be used directly for training, but it can be used to learn an \emph{invariant representation}.
Such a representation captures all information in $\vx$ that is relevant to the task of predicting $y$,
but is invariant to the spurious attribute.
I present a particularly suitable method for this based on Invertible Neural Networks,
which ensures that no information is lost that is unrelated to $s$.

The final approach is in the setting of sampling bias as well,
but compared to the previous method, some assumptions are relaxed and different trade-offs exist.
One perhaps strong assumption in the previous method was
that the context set needs to include labels for $s$.
Conversely, in this new setting, we have a completely unlabeled context set
and a training set which is missing certain combinations of $s$ and $y$.
In this setting, it is possible to learn an invariant representation
by enforcing the representation for the training set to be indistinguishable from the representation on the context set,
when conditioning on $y$.
This enforcement is realized with adversarial networks which compare batches of samples
in order to compare data drawn from the training set and the context set.
This process requires balanced batches which is achieved via clustering.
The end result is an invariant representation which allows training a classifier
that learns the correct relationship between $x$ and $y$.

\section{Thesis structure}%
\label{sec:thesis-structure}
In \chapref{ch:related-work}, I present related work,
concentrated on the area of algorithmic fairness and dataset bias.
\Rangechapref{ch:target-labels}{ch:zsf} comprise the main contribution of this thesis.
Each of these chapters starts with a brief section which positions the chapter in the context of the this document.
This is followed by the contents of publications.
In the final chapter, \chapref{ch:conclusion}, I present conclusions from the main work and directions for future work.
