\chapter{Discussion and future work}\label{ch:conclusion}
In this thesis, I have presented three approaches for dealing with dataset bias.
The first one deals with a form of label bias and the other two with forms of sampling bias;
in both cases the bias is linked to a special attribute \(s\) of the data.
%
% mention again the main strengths of the presented methods
%
With the first approach, the user specifies target rates that can be used to tune the method to the desired outcome.
The method is flexible and easy to integrate with existing algorithms.
The second approach uses a partially-labelled representative set to learn an interpretable invariant representation.
The user can directly observe in what way the data was changed to make it invariant
to the spurious correlation that is present in the training data.
The third approach in many ways builds upon the second one;
a set similar to the representative set is needed but there is no requirement for labels nor for balance of subgroups.
However, in the training set, the relationship between class label \(y\) and subgroups \(s\) may not be
as close to a one-to-one mapping of \(y\) and \(s\) as in the second approach.
From these simple starting points, an invariant representation is learned
that can be used to train unbiased classifiers.

While this by no means covers \emph{all} possible biases,
it contributes to a growing literature that tries to tackle this problem.
One could ask if there is one method that is able to cover all possible dataset biases,
but I think there is a strong argument to be made that no general method can exist,
because it is, \eg, not possible to describe in general what is spurious information and what is relevant information.
Nevertheless, finding methods that are more generally applicable is a worthy goal.

Another potential extension of this work is to work with other modalities.
The experiments were all performed on either tabular or image data
--- the reason predominatly being ease of visualization ---
so working with audio (especially speech) or text could present new interesting challenges.

Furthermore, as it currently stands,
the \ac{ML} practitioner has to know the bias in the data very well in order to choose a method to correct it.
It would be desirable to have a simple algorithm for deciding which method to use.

Thus, correcting dataset bias remains a challenging topic
and one that is relevant to today's machine learning applications.
Any cutting-edge \ac{ML} system will have to deal with imperfect data,
especially if the collected data is human-related.
The possible effects of these data imperfections are certainly highly undesirable:
a photo tagging service might only work for a certain kind of person;
a speech recognition system might only work for a certain kind of dialect.
If, in these situations, enough representative (but unlabelled) data is available,
then the methods presented here can be used to try and correct the problem.

One area of machine learning that has recently seen increased interest is unsupervised learning.
The latter two chapters also make use of it to some degree.
The exciting promise of unsupervised learning in general is
that labour-intensive labelling is not needed and so vast amounts of existing, unlabelled data can be put to good use.
One could ask the question whether bias-correcting methods are still needed, with access to so much data.
It could be that, while the data is certainly not perfect,
there is so much of it that the biased parts ``cancel each other out''.
However, recent investigations of the GPT models \citep{radford2018improving,radford2019language,brown2020language} do not seem to bear this out \citep{khalifa2021distributional}.
One reason for this might be the way these models are trained at the moment:
they maximise the probability assigned to the next token.
Thus, such a model has to account for the wide array of human opinions and assign a non-zero probability to all of them.
So, when asked to summarise a text \citep{stiennon2020learning}, GPT does not give the \emph{best} summary;
it gives a summary that an average person might have written.
However, with the help of a very high quality labelled dataset (that was expensive to create),
GPT could be finetuned to actually produce very good summaries.
I suspect this pattern of learning the basics in an unsupervised fashion
and then finetuning with high-quality labels will continue in the future.
