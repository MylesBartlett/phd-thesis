\chapter{Discussion and future work}\label{ch:conclusion}
In this thesis, I have presented three approaches for dealing with dataset bias.
The first one deals with a form of label bias and the other two with forms of sampling bias;
in both cases the bias is linked to a special attribute \(s\) of the data.
%
% mention again the main strengths of the presented methods
%
The first approach used knowledge of general statistics of the ``fair'' data distribution to offer a quite general method for correcting label bias,
that is intuitive to tune.

While this by no means covers \emph{all} possible biases,
it contributes to a growing literature that tries to tackle this problem.
One could ask if there is one method that is able to cover all possible dataset biases,
but I think there is a strong argument to be made that no general method can exist,
because it is, \eg, not possible to describe in general what is spurious information and what is relevant information.
Nevertheless, finding methods that are more generally applicable is a worthy goal.

Furthermore, as it currently stands,
the \ac{ML} practitioner has to know the bias in the data very well in order to choose a method to correct it.
It would be desirable to have a simple algorithm for deciding which method to use.

Thus, correcting dataset bias remains a challenging topic
and one that is relevant to today's machine learning applications.
Any cutting-edge \ac{ML} system will have to deal with imperfect data,
especially if the collected data is human-related.
The possible effects of these data imperfections are certainly highly undesirable:
a photo tagging service might only work for a certain kind of person;
a speech recognition system might only work for a certain kind of dialect.
If, in these situations, enough representative (but unlabelled) data is available,
then the methods presented here can be used to try and correct the problem.

One area of machine learning that has recently seen increased interest is unsupervised learning.
The latter two chapters also make use of it to some degree.
The exciting promise of unsupervised learning in general is
that labour-intensive labelling is not needed and so vast amounts of existing, unlabelled data can be put to good use.
One could ask the question whether bias-correcting methods are still needed, with access to so much data.
It could be that, while the data is certainly not perfect,
there is so much of it that the biased parts ``cancel each other out''.
However, recent investigations of the GPT models do not seem to bear this out \citep{khalifa2021distributional}.
One reason for this might be the way these models are trained at the moment:
they maximise the probability assigned to the next token.
Thus, such a model has to account for the wide array of human opinions and assign a non-zero probability to all of them.
So, when asked to summarise a text \citep{stiennon2020learning}, GPT does not give the \emph{best} summary;
it gives a summary that an average person might have written.
However, with the help of a very high quality labelled dataset (that was expensive to create),
GPT could be finetuned to actually produce very good summaries.
I suspect this pattern of learning the basics in an unsupervised fashion
and then finetuning with high-quality labels will continue in the future.
