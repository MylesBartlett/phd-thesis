\chapter{Related work}\label{ch:related-work}
\section{Introduction}\label{introduction}
Fairness in Machine Learning is a topic that has recently received increasing attention.
Machine Learning has become very widespread in our society
which brings some dangers with it.
Decisions such as credit eligibility and hiring are increasingly done automatically.
However, the algorithms that are used in these decisions can have strong biases
that stem from the training data.
Often, the algorithms are even more biased than the training data.
\citet{kamishima2011fairness} give a great illustration for this
(adapted from \citet{calders2010three}).
The illustration is based on the Adult/Census Income dataset \citep{kohavi1996scaling}
which contains records of individuals with thirteen attributes, among them age and gender.
The task is to predict whether or not the individual earns more than \$50K per year.
In the dataset, 32\% of the men have a positive label (\ie, they earn more than \$50K),
but of the women only 11\% have a positive label.
With this ratio of about 3:1, the dataset is already skewed.
However, when training a \href{https://en.wikipedia.org/wiki/Naive_Bayes_classifier}{Naive Bayes classifier} on the data,
the ratio in the predictions is even more unfair:
the amount of women who get a positive prediction is even lower than 11\%.
If the Naive Bayes classifier does not get access to the gender information,
the predictions will be a bit fairer,
but they are still more unfair than the original dataset labels.

This phenomenon of predictions being more unfair than an already skewed dataset
is due to the generalization that classifiers have to make.
It is easier to classify all but the most outstanding women as low income
than to learn a complicated decision boundary.
Furthermore, removing the gender information does not solve the problem
because the gender can be inferred from other data.

The following is a review of the literature on Fairness in Machine Learning.
Several different approaches to and definition of Fairness are presented.
The focus is on identifying the major trends in the field from the beginning.

\subsection{Precursor}\label{precursor}
The first published work to address the problem of biases in machine learning
was arguably the 2008 paper by \citet{pedreshi2008discrimination}
with the main focus being Data Mining.
The first fundamental conclusion of the authors
is that it is not enough to take out the discriminatory features
(referred to as \emph{sensitive attributes} in other works) from the data.
That is, if the information about, for example, race
is removed from the data,
there is usually enough background information to reconstruct that attribute.
The experiments were done on the ``German credit dataset''
where the task is to classify people as a good or a bad credit risk.
As their focus is Data Mining,
the authors' goal is to identify discriminatory rules.
They distinguish between direct and indirect discrimination.
According to their definition, direct discrimination explicitly uses
discriminatory features in the premise of the rule, while indirect discrimination uses
features that are closely associated with discriminatory features. The latter case can be
detected by checking the other rules in the dataset.

\section{Modification of labels for Fairness enforcement}\label{modification-of-labels-for-fairness-enforcement}
Independently of \citet{pedreshi2008discrimination}, but with a similar intention,
\citet{kamiran2009classifying} considered discrimination in classification.
As opposed to \citet{pedreshi2008discrimination}, their goal was not to identify discriminatory rules in the dataset,
but to modify the dataset so that any classifier trained on it does not become biased.
The same dataset (German credit dataset) is used in both papers.

First, Kamiran and Calders present the problem formulation
that was later used in other works on Fairness as well:
there is a set of attributes, a binary class label
and a special \emph{sensitive attribute} which determines the demographic group.
The sensitive attribute \(s\) is assumed to be binary in the derivation,
but does not have to be.
\(s=0\) refers to the group vulnerable to discrimination and \(s=1\) to all other individuals.
It is further assumed that one of the class labels is generally desirable;
for example, it might correspond to being accepted for a credit or being given bail.
This class label is being referred to as the positive label: \(y = 1\).

After the general setup, the authors define a concrete measure of discrimination:
\begin{align}
  \label{eq:disc}
  Disc := P(y=1|s =1) - P(y=1|s = 0)
\end{align}
(The paper uses a different notation.
The notation here reflects the notation in later papers.)

This measure of fairness is today called \emph{Demographic Parity}.
It is zero when the individuals in both groups have the same chance to get a positive prediction.

Their algorithm for de-biasing the dataset works as follows:
first, a well-calibrated classifier (Naive Bayes in the paper) is trained on the data
\emph{without} the sensitive attribute.
This classifier gives a ranking for how likely a positive label is for a given individual.
The highest ranked individuals that have label \(y=0\)
and are potentially discriminated against \(s=0\) get ``promoted'' to \(y=1\)
and the lowest ranked individuals with label \(y=1\) and \(s=1\) get ``demoted'' to \(y=0\)
until the dataset is fair according to the \(Disc\) measure.
According to the authors, the ranking ensures that the changes in the dataset
happen to the most appropriate candidates (close to the decision boundary),
the main goal being a prevention of a big drop in accuracy of the final classifier
with respect to the original dataset.

The paper concludes with results on the German credit data,
using Naive Bayes for ranking and also for the final classifier.
Using \emph{Age} as the sensitive attribute,
the dataset itself shows a \(Disc\) of about 15\% (depending on the train-test split).
As a baseline, a Naive Bayes classifier was trained with the sensitive attribute
resulting in \(Disc = 25\%\).
Without the sensitive attribute it leads to \(Disc = 10\%\).
The scheme that the authors propose reaches \(Disc = 5\%\).
The exact numbers depend on the split and the choice of sensitive attribute.
Generally, higher discrimination in the dataset itself
leads to bigger differences in discrimination between the fair classifier and the baseline.

This work was extended by \citet{calders2009building}.
They use the same discrimination criterion as before,
but call it \emph{dependency} between \(s\) and \(y\).
They again present the same dataset changing technique
and a different technique based on giving training examples different weights
instead of re-labeling them.
Examples with \(y=1\) and \(s=0\) get higher weight than those with \(y=0\) and \(s=0\).
For \(s=1\), those with \(y=0\) get higher weight than those with \(y=1\).
Weighting means that when the training examples are sampled from the dataset,
those with higher weight are used more often.
The two approaches are compared on the Adult/Census Income dataset \citep{kohavi1996scaling}
where the task is to estimate if someone earns more than \$50K per year.
The sensitive attribute is \emph{sex}.
In addition to Naive Bayes,
two classifiers based on nearest neighbors and a decision tree classifier are used.
The comparison shows that changing the labels leads to lower dependency (discrimination)
than reweighting the examples.
However, reweighting the examples leads to lower dependency than the baseline.
The difference in accuracy is minor.

\section{Fair classifiers}\label{fair-classifiers}
While the two previous papers were based on manipulating the dataset,
Calders and Verwer proposed three completely different approaches
to make classifiers fair in 2010 \citep{calders2010three}.
They all center around putting \emph{constraints} on the classifier make fair predictions.
This potentially gives more control over the prediction bias than manipulating datasets.
All three approaches are based on Naive Bayes
and again aim to enforce Demographic Parity as defined before.
The first approach shifts the predicted probabilities in a post-processing step
after training the classifier normally.
To this end, the sensitive attribute is treated differently
than the other features in the Naive Bayes model.
Instead of the class being the cause of the sensitive attribute,
the sensitive attribute is regarded as one cause for the class.
This is a significant change in perspective that enables a better intuition about the problem.
In this modified Naive Bayes,
the conditional probability \(P(y|s)\) is then modified in the already trained model
until the discrimination score is below a given threshold.
Care is taken to not distort the predicted distribution too much with respect to the unfair model.
The end result is a model that is (ideally) unbiased
but still relies on the sensitive attribute for predictions.
This is unworkable if the sensitive attributes are not always available when making predictions.
Nevertheless, the algorithm can be considered an improvement over manipulating the dataset,
as the resulting bias can be controlled to a much finer degree.

The second method that Calders and Verwer present in their 2010 paper
is based on training two separate models for each sensitive attributes.
That is, the dataset is split in two
where one half contains all examples with \(s=0\), we call this dataset \(D_0\),
and the other half contains all examples with \(s=1\), called \(D_1\).
For prediction, we choose either the model that was trained on \(D_0\) or the one for \(D_1\),
depending on the sensitive attribute of the example.
This method relies on the availability of the sensitive attribute for prediction,
like the previous method.
The two models are separately tweaked to produce overall fair results.
In order to minimize the effect on the accuracy, the models are tweaked the same amount each,
but in opposite directions.
Conceptually, having two different Naive Bayes model depending on the sensitive attribute
is equivalent to using one Naive Bayes model
in which the sensitive attribute is connected to all other features.
This means that the bias is in all of the features and not just in the class label.
The latter was the assumption of the previously discussed model.
The difference between these assumptions is not explored in much detail in the paper.

The third methods introduces a new latent variable, called \(L\) in the paper.
\(L\) is intended to be an unbiased target class label
that replaces the biased class label \(y\) from the training data.
\(L\) can only be estimated and is unbiased
in the sense that it is statistically independent from sensitive attribute: \(P(L|s=0) = P(L|s=1)\).
The observed class label \(y\) then only depends on the latent label and the sensitive attribute.
\(L\) is found by using expectation maximization
to iteratively search for a value that maximizes the likelihood of the dataset.
The search is restricted by enforcing that \(L\) must be equal to \(y\) except
when \(s=0\) and \(y=0\) or when \(s=1\) and \(y=1\)
because these are the two cases where we expect discrimination.
There is other prior knowledge that can be incorporated into the search.
This method seems to have been the most sophisticated at that point in time.

\citet{calders2010three} present experimental results for the three methods on an artificial dataset
and the Adult/Census Income data that \citet{calders2009building} used before.
The artificial data has biased and also unbiased class labels
which are generally not available for real-world datasets
and conforms to the assumptions made for the third model.
Artificial datasets generally have the advantage that the bias can be controlled very precisely,
but there is always the danger of tailoring the dataset too closely to the proposed algorithm.
To the authors surprise, on both datasets the two simpler methods outperformed the third method
that was based on fair latent class labels.
The two first methods perform approximately equally well.
The authors mention the possibility of a better fairness measure.

\citet{kamishima2011fairness} take a closer look
at the underlying causes for unfair predictors and condense it to three main causes:
\emph{prejudice}, \emph{underestimation} and \emph{negative legacy}.
(Let \(x\) be the non-sensitive features.)

\begin{enumerate}
\item
  Prejudice:
  a statistical dependency between the sensitive attribute
  and the class label or the (non-sensitive) input features.
  If the dependency is with the class label, then the paper calls it \emph{indirect prejudice}.
  If it is with the non-sensitive features, they call it \emph{latent prejudice}.
  In line with \citet{pedreshi2008discrimination},
  \emph{direct prejudice} refers to classifiers that make direct use of the sensitive attribute.
  Indirect prejudice and latent prejudice can be quantified by
  the mutual information between \(y\) and \(s\) or \(x\) and \(s\) respectively.
  \citet{kamishima2011fairness} do not consider latent prejudice to be immediately harmful,
  but it can be a problem with respect to the protection of personal data and compliance with laws.
\item
  Underestimation:
  non-convergence of machine learning algorithm due to limited amount of data.
  This magnitude of this problem can be estimated
  by considering the difference between the actual training sample distribution
  and the distribution that the machine learning model has internalized.
  This is the underlying cause of models that make predictions
  that are even more unfair than the dataset.
\item
  Negative Legacy:
  sampling bias and wrong labels in the dataset.
  In contrast to the problem of \emph{prejudice},
  \emph{negative legacy} might not be detectable by analyzing the dataset.
  \emph{Sampling bias}, meaning that certain data points simply are missing,
  and \emph{wrong labels} can only be corrected if other sources of information are available.
\end{enumerate}

We see that previous works nearly exclusively dealt with the first of these issues, \emph{prejudice},
and more precisely \emph{indirect prejudice}.
\citet{kamishima2011fairness} also focus on \emph{indirect prejudice};
essentially because it is the easiest to deal with, apart from \emph{direct prejudice}.
They use \href{https://en.wikipedia.org/wiki/Logistic_regression}{logistic regression} as the basis for their fair classifier.
Their approach to enforcing fairness is qualitatively different from the previous proposals.
Instead of manipulating the dataset or using explicit algorithm to make a classifier fair,
\citet{kamishima2011fairness} treat the fairness constraint like a regularizer.
A term is added to the objective function
that estimates the mutual information between \(y\) and \(s\)
and gets minimized together with the other parts of the objective function.
This strategy leaves most of the work to the optimization that would need to be run anyway.
A factor in front of the regularization term determines how much to value fairness over accuracy.

In a comparison between \citet{kamishima2011fairness}'s algorithm
and the Naive Bayes method with two different models for the sensitive attributes
from \citet{calders2010three},
the Calders algorithm performed better on the Adult/Census Income dataset
(higher accuracy and lower discrimination score).
However, the Kamishima algorithm has the advantage
that the sensitive attribute does not necessarily have to be known for making predictions
and the trade-off between accuracy and fairness can be easily controlled
with the factor for the regularization term.

\subsection{Fairness based on similarity}%
\label{fairness-based-on-similarity}
Nearly all previously discussed papers express some dissatisfaction
with the \emph{Demographic Parity} metric for measuring discrimination.
\citet{luong2011k} use a very different fairness metric
based on the idea that similar people should be treated similarly.
They first define a distance metric which defines a neighborhood
and is basically the Manhattan distance on the z-scores of the attributes.
The distance metric is supposed to be only make use of legally admissible attributes.
An individual is then considered to be unfairly treated
if it is treated differently than its neighbors.
More concretely, for any data point we can check
how many of the \(k\) nearest neighbors have the same class label as that data point.
If the percentage is under a certain threshold
then there was discrimination against the individual corresponding to that data point.
In order to create a fair classifier, for this definition of fairness,
the authors propose to pre-process the dataset,
similar to \citet{kamiran2009classifying} before,
by flipping the class labels of those data points where the class label is considered unfair.
The experiments in the paper show that this method is successful in reducing the discrimination
(according to the given criterion)
for a range of classifiers on the Adult/Census Income dataset.

\citet{dwork2012fairness} give a more in-depth analysis
of the idea of distance-based fairness and Demographic Parity.
They present explicit detailed criticism of the Demographic Parity metric,
mainly in the form of three scenarios in which Demographic Parity is maintained,
but the system treats a lot of individuals very unfairly.
In the first scenario, the sensitive attribute carries important information
and removing it, makes everyone worse off.
In the second scenario, a malicious actor achieves Demographic Parity
by promoting the weakest members of the disadvantaged group to receive a positive label
and then argues that fairness is not a worthwhile goal
because these individuals will not be able to succeed.
The goal of the actor here is to undermine the whole goal of avoiding discrimination.
Scenario three considers the case of subset targeting,
where there is discrimination against subgroups
which are not covered by the Demographic Parity metric
that only considers the major demographic groups.
Defining fairness via group membership will always leave open the possibility
to discriminate against ever smaller subgroups down to individuals.

For these reasons, \citet{dwork2012fairness} try to do better with a metric
that is centered around a measure of similarity between \emph{individuals}.
The goal with this is to achieve \emph{individual fairness} instead of just \emph{group fairness}.
Just like \citet{luong2011k}, they want to treat similar people similarly.
However, the authors do not give a general recipe to construct the necessary similarity measure.
This is left to be decided on a case-by-case basis.
In order for the results to be meaningful,
the similarity measure has to be as close to the ground truth as possible.

The proposed idea is then as follows: a classifier is fair
if and only if the predictive distributions for any two data points
are at least as similar as the two points themselves,
according to a given similarity measure for distributions
and a given similarity measure for data points.
They call this condition the \emph{Lipschitz condition}.
The authors propose two practical similarity measures for distributions
that are well-known in the respective literature.
In order to train a fair classifier,
the Lipschitz condition is then used as a constraint for the optimization.
This is similar to how Kamishima et al.~used the fairness criterion as a regularizer.
However, Dwork et al.~intend to realize it as a strict constraint.
Furthermore, they show that their criterion is compatible with Demographic Parity
but does not guarantee it in general.
When the two demographic groups are similar according to the individual similarity measure,
then the Lipschitz condition will lead to a good approximation of Demographic Parity.
If the demographic groups are very different,
then the Lipschitz criterion is essentially incompatible with Demographic Parity.

As an alternative framework,
the authors also discuss a relaxation of the Lipschitz condition
with an added constraint for Demographic Parity.
This is intended for cases in which the groups are very different
but we nevertheless want to enforce Demographic Parity, i.e.~affirmative action.
In this case, the (relaxed) Lipschitz condition ensures
that there is still some individual fairness despite the affirmative action.
However, the paper does not provide an implementation or experiments for the idea,
so it is difficult to know if the Lipschitz condition is practical.

Finally, Dwork et al.~discuss the problem of removing sensitive information
from data that someone wants to sell.
The hope would be that the sold data has no information in it
that would allow a malicious actor to discriminate.
The authors argue that enforcing Demographic Parity would not be enough
to make the data unsuitable for discrimination.
They speculate that the Lipschitz condition would work better,
but ultimately do not provide a real solution to the problem.

\section{Fair representation}\label{fair-representation}
\citet{zemel2013learning} pick up on this idea of creating a \emph{fair representation}.
That is, the features of the data points are transformed such that they contain no bias anymore,
but still carry the information necessary for predicting the class label.
This is different from \citet{kamiran2009classifying} who also manipulated the dataset,
but only changed the \emph{labels}.
In this work, the \emph{inputs} are transformed.
This is the first of many papers with such a goal.
More specifically, the stated goal of \citet{zemel2013learning} is
to achieve both \emph{group fairness} (in the sense of Demographic Parity)
and \emph{individual fairness} (in the sense of treating similar people similarly),
as Dwork et al.~did before.
The learned fair representation is denoted by \(z\).
The condition for fairness is then
\begin{align}
  \label{eq:fair-representation}
  P(s=0|z=z') = P(s=1|z=z') ~.
\end{align}
Zemel et al.~propose to map the biased inputs \(x\) to \emph{prototypes} in the same space as \(x\).
The probability for being assigned to a particular prototype
must be the same for inputs with \(s=0\) and for \(s=1\).
Based on a given distance function (or similarity measure),
inputs are more likely to be assigned to prototypes that are close-by.
The classification task is then based entirely on the prototypes (the fair representation \(z\)).
In the paper, a linear model is used to map the prototypes to the outcomes \(y\).
Aside from the distance function, the whole model has therefore only two kinds of parameters:
the locations of the prototypes and the weights in the linear model.
These parameters are all optimized together.
The objective function consists of three terms:
the first enforcing Demographic Parity via the prototype locations,
the second ensuring that the prototypes are close to the inputs
and the third targeting a good accuracy by adapting the weights of the classifier.

The experiments are done on the German credit dataset, the Adult / Census Income dataset
and a dataset based on the Heritage Health Prize milestone 1 challenge
where the goal is to predict how many days a given person will spend in the hospital in a year.
The proposed methods performs better in these experiments
than the methods from Kamiran and Calders (2009) and the method from Kamishima.
However, they do not compare against Calders and Verwer (2010),
presumably because those methods rely on the sensitive attribute for making predictions,
which can be considered an unfair advantage.

A major flaw in Zemel et al.'s fair representation learning is
that the representation only enforces fairness
with respect to the one classifier that it was trained with.
This is shown in the paper when they try to predict \(s\) from \(z\) with a newly trained classifier
and get results that are significantly better than chance.
This shows that the proposed approach does not produce a representation
that could be sold to potentially malicious actors.
The other question is
whether the fair representation \(z\) still contains enough information about \(x\),
so that other classifiers can achieve high accuracy.
Here, the proposed system performs well:
a different classifier only shows a small reduction in accuracy
compared to the one that was trained together with the representation.

\citet{feldman2015certifying} try to improve upon the work by \citet{zemel2013learning},
focusing on fair representation as well.
The authors try to ground their definition of fairness in U.S. law.
They define \emph{Disparate Impact} (DI) as
\begin{align}
  \label{eq:disparate-impace}
  DI = \frac{P(y=1|s=0)}{P(y=1|s=1)} ~.
\end{align}
With a target of \(DI = 1\), this would simply enforce Demographic Parity.
However, based on some rulings and recommendations of the U.S. legal system,
they advocate the 80\% Rule, which states that \(DI\) should not be below 80\% (or above 125\%).
This means that the acceptance rate of a disadvantaged demographic group
should not be less than 80\% of the acceptance rate of the other group.
With this definition, there is an explicit allowed range for small unfairness.
Previously, people just tried to get as close as possible to \(DI = 1\),
without stating how close is close enough.

For measuring the fairness of the (non-sensitive) input features
(disregarding class labels for the moment), the authors define \(\varepsilon\)-fairness.
The features \(x\) are \(\varepsilon\)-fair,
if any predictor that tries to predict \(s\) from \(x\) can only achieve a balanced error rate
that is higher than \(\varepsilon\).
Feldman et al.~prove that that \(\varepsilon\)-fairness with a suitable \(\varepsilon\)
is incompatible with violating the 80\% rule.
That is, if the sensitive attribute cannot be predicted from the features,
then a classifier trained on that data will automatically be fair (to a certain degree).
For datasets that are extremely unbalanced,
the required \(\varepsilon\) approaches 1/2
which corresponds to absolutely no information in \(x\) about \(s\).
Note that the definitions require that the performance of the best possible classifiers is known,
which is rarely the case.
In the paper, an SVM classifier is used to measure the \(\varepsilon\) for \(\varepsilon\)-fairness.

In order to create a fair dataset,
the authors present an algorithm that considers every feature individually
and shifts the values such that the distributions \(P(z=z'|s=0)\) and \(P(z=z'|s=1)\) are identical
(\(z\) refers to the shifted values, \(x\) refers to the original values).
This shift retains the ordering of the data points with regard to that feature.
This is to ensure that \(z\) can still be used to predict \(y\).
The method only works on numerical features.
The paper contains two other algorithms for removing unfairness which are not as drastic,
meaning some amount of unfairness remains, but the ability to predict is improved.
They are referred to as \emph{Partial Repair} algorithms, as opposed to full repair.
This is the fairness-accuracy trade-off that basically all works in this area consider.
Both of the other methods try to preserve the ranking of the data points
with respect to the individual features
while minimizing the distance of the distributions for the different groups.

In the experiments, the authors first investigate the theoretically predicted relationship
between \(\varepsilon\)-fairness and the \(DI\) measure.
All experiments were done with either of the two Partial Repair algorithms
which give very similar results.
Three datasets are used:
the German Credit dataset, the Adult/Census Income dataset
and the Ricci dataset from the \emph{Ricci vs DeStefano} court case.
The goal for the Ricci dataset is to predict which firefighters to promote based on test scores.
The sensitive attribute is \emph{race}.
The experimental results support the theoretical predictions to a reasonable extent.
To compare the method with previous work,
several classifiers are trained on the fair representation:
logistic regression, SVM and Naive Bayes.
Of these, the Naive Bayes classifier seems to outperform the algorithms from previous works
\citep{kamiran2009classifying,kamishima2011fairness,zemel2013learning}
in both achieved fairness and accuracy.

This work by \citet{feldman2015certifying} is a major improvement over the work by \citet{zemel2013learning}
because the representation (ideally) is fair with regards to any machine learning algorithm, not just one particular.
Furthermore, theoretical bounds were proved for the expected bias of a classifier.
However, Feldman et al.~do not take into account individual fairness in any way.

While the algorithms by \citet{feldman2015certifying} are manually constructed and explicit,
\citet{louizos2015variational} use an approach
that falls more in the area of end-to-end learning where fewer hand-crafted algorithms are used.
The method is based on deep variational autoencoders (VAE).
There is an encoder and a decoder that are both modeled as deep neural networks.
The encoder produces the distribution of the latent (fair) representation \(z\)
from the original features \(x\) and the sensitive attribute \(s\).
The decoder recovers the distribution of \(x\) from \(z\) and \(s\).
By choosing a factorized prior \(P(s)P(z)\), a separation between \(s\) and \(z\) is encouraged.

This method can be improved further by taking into account the labels
when constructing the fair representation.
If this is not done, \(z\) loses the ranking information from \(x\) (see Feldman et al.~above).
To this end, a second latent variable is introduced, \(\tilde{z}\),
which encodes the variation in \(z\) that is not explained by the class labels \(y\).
\(z\) is then determined by \(y\) and \(\tilde{z}\),
and \(x\) is determined by \(z\) and \(s\) as before.
\(\tilde{z}\) and \(s\) have independent priors.
This structure ensures that \(y\) can be predicted from \(z\).
However, this introduces a new problem:
if \(y\) is correlated with \(s\), then \(z\) will be as well.
To overcome this, an additional penalty term is introduced that forces \(P(z|s=0)\)
and \(P(z|s=1)\) to be as close as possible.
This is realized with a measure of distance
between distributions called Maximum Mean Discrepancy (MMD).

The experiments were done on the Adult/Census Income dataset,
the German Credit dataset and the Heritage Health dataset.
To test whether \(s\) can be recovered from \(z\),
a Random Forest model and a Logistic Regression model were trained to predict \(s\) from \(z\).
This is compared to Zemel et al. (2013).
The proposed model can outperform Zemel et al.~on some tasks, but many results seem inconclusive.
Using MMD for an additional unfairness penalty, seems to improve fairness.
When training a classifier on \(z\) to predict \(y\), there is a small drop in accuracy.
In general, it is not clear whether this method gives better performance than Feldman et
al.~However, the algorithm by Louizos seems more general and more widely applicable
as it simply re-uses a widely used technique, variational autoencoders.

\section{Improved definitions of Fairness}%
\label{improved-definitions-of-fairness}
The early literature on Fairness in Machine Learning used only Demographic Parity
and similarity-based fairness as fairness definitions.
\citet{kleinberg2016inherent} on the other hand, formalized three fairness conditions.
The authors consider a scenario where the data points \(x\) are sorted into bins \(b\)
and each bin is associated with a prediction score \(f = P(\hat{y}=1|b)\)
where \(\hat{y}\) is the predicted class label and \(x\) the (non-sensitive) features.
\begin{enumerate}
\item
  Calibration within groups:
  If the prediction score for a given bin is \(f\),
  then when considering all the data points with group \(s\) in the bin,
  a fraction of \(f\) of those should have the class label \(y=1\).
  In other words, the prediction score \(f\) is well calibrated with respect to group \(s\).
  This should be the case for all groups.
\item
  Balance for the negative class:
  The true negative rate should be the same for both groups: \(P(\hat{y}=0|y=0,s=0) = P(\hat{y}=0|y=0,s=1)\).
\item
  Balance for the positive class:
  The true positive rate should be the same for both groups: \(P(\hat{y}=1|y=1,s=0) = P(\hat{y}=1|y=1,s=1)\).
\end{enumerate}
These are formalizations of things that have been proposed as definitions of fairness.
The first criterion essentially ensures that the predictor works correctly for both groups.
This prevents a classifier from predicting one group correctly
but always returning a negative answer for the other group.
The second and third put emphasis on negative and positive class labels respectively.
In criterion 2, we allow the classifier to mis-classify those data points with \(y=0\),
but we want to make sure that the misclassification rate is the same for both groups.
In other words, members of the different groups have the same chance
to get a correct classification if they should receive a negative classification.
The same holds for criterion 3 and positive classifications (\(y=1\)).

The next question is whether we can achieve all of these criteria simultaneously. The
authors show that a perfect predictor, that gives a score of \(P(\hat{y}=1|x) = 1\) to data
points with \(y=1\) and \(P(\hat{y}=0|x) = 1\) to those with \(y=0\), automatically fulfills all
three criteria. As we saw before, this is not true of Demographic Parity. Furthermore, the
authors show that if the dataset is fair in the sense that it satisfies \(P(y=1|s=0) = 
P(y=1|s=1)\), \ie, the base acceptance rate is the same for both classes, then a ``random''
classifier which assigns that base rate as the prediction score to all data points
indiscriminately also fulfills all three criteria. For this case, even Demographic Parity
is fulfilled. The paper contains a proof that those two cases are the only ones that
achieve the three presented guarantees simultaneously. Demographic Parity can only be
achieved simultaneously with these in the second scenario. The general case sits between
those extremes: the predictor is not perfect and the dataset is not fair, and therefore,
these definitions of fairness are not compatible. Note, however, that criterion 2 and 3
\emph{are} compatible with one another.

The paper's main contribution is a theorem showing that the three criteria are in general
incompatible even if we only consider approximations of them. The conclusion to draw is
that it is remains a difficult choice to choose the appropriate definition of fairness.

\citet{hardt2016equality} expand on criterion 3 by Kleinberg et al.~and develop a
method to enforce it via post-processing. Furthermore, an alternative criterion is
introduced that is the combination of criterion 2 and 3. This is also the work that gave
criterion 3 the name \emph{Equality of Opportunity} and that popularized the name \emph{Demographic
Parity}. The treatment of fairness by the authors is explicitly ``oblivious'', which here
means that only the general statistics are known about the features \(x\), the sensitive
attributes and the class labels \(y\). In particular, there is not enough information
available to develop a similarity measure which could be used to target \emph{individual
fairness}.

\citet{hardt2016equality} define the criteria via statistical independence: \emph{Equalized Odds} refers to
the case where \(\hat{y}\) and \(s\) are independent conditional on \(y\). This is equivalent to
the true positive rate and the false positive rate being the same for all groups.
\emph{Equality of Opportunity} is, as mentioned above, the case where just the true positive
rate is the same for all groups. This means that \(\hat{y}\) and \(s\) are independent
conditional on \(y=1\). One main advantage that these definitions have over Demographic
Parity is that a perfect predictor fulfills them. Demographic Parity is at odds with
achieving perfect accuracy. Furthermore, for Demographic Parity a classifier might be
forced to give a positive prediction to a lot of individuals where it is really not
appropriate.

In order to construct a fair classifier out of a \emph{binary predictor} (giving only binary
predictions) via post-processing, the output is randomized in such a way as to remove the
bias. If the overall probability for a positive predictions of the unfair classifier is
given by \(P(\hat{y}=1|s)\), then the randomized probability for the fair positive
predictions \(\tilde{y} = 1\) is given by:
\begin{align}
  \label{eq:hardt}
  P(\tilde{y}=1| x, s) = \sum\limits_{y' \in \{0, 1\}} P(\tilde{y} = 1| \hat{y}=y', s) 
  P(\hat{y}=y'| x, s)
\end{align}
There are 4 free parameters \(P(\tilde{y} = 1| \hat{y}=y', s=s')\) with \(y' \in \{0, 1\}\)
and \(s' \in \{0, 1\}\). As an example, if the unfair predictor predicts \(\hat{y} =0\) for an
input with \(s=0\), then the fair predictor predicts \(\tilde{y} =1\) with probability
\(P(\tilde{y} = 1| \hat{y}=0, s=0)\) which might be non-zero. In addition to the fairness
condition, we also want to enforce accuracy. To that end, we introduce a loss function
\(\ell (\tilde{y}, y)\) that quantifies the cost of predicting the wrong class label. The
final optimization problem for the 4 free parameters is then to minimize \(\ell\) under the
constraint of Equality of Opportunity or Equalized Odds and the constraint that the
parameters must be valid probabilities.

For a predictor that outputs a \emph{score function}, the post-processing step consists of
choosing differing thresholds for \(s=0\) and \(s=1\), such that the predictions become fair.
If \(f\) is the score, then for a given threshold \(t\) we predict \(\hat{y} = 1\) if \(f > t\).
The two thresholds (one for each group) are found by minimizing \(\ell\) with the fairness
constraints, as before. To satisfy the constraint, it might be necessary to randomize the
result. This is done by using two constraints per group: if \(f\) is above both, the result
is \(\hat{y} = 1\), if it is below both, \(\hat{y} =0\) and if \(f\) is between the thresholds,
then the result is chosen at random. This method -- as well as the one before -- requires
the sensitive attribute for all predictions as it has to choose the right threshold.

The authors prove that with this post-processing, the Bayes optimal (but unfair)
classifier becomes the Bayes optimal fair classifier. Additionally, they investigate the
limitations of the ``oblivious'' approach where the only available information about the
data is the joint distribution over \((y, s, \hat{y})\). With just this information, it is
impossible to uncover the underlying dependency structure that produced the bias. The
authors present two very different scenarios that are indistinguishable with just the
joint distribution. Demographic Parity is also affected by this as it only uses the
information from the distribution over \((s, \hat{y})\). Another shortcoming of Equality of
Opportunity and Equalized Odds is that they carry the assumption that the labels are
correct. Demographic Parity does not rely on this assumption so much. All-in-all, the
fairness definition still has to be chosen by the user based on the situation and the
goals.

\section{Recent developments in Fairness}\label{recent-developments-in-fairness}
At this point, fairness in machine learning has come a long way.
There are several working implementations for various fairness criteria
that give reasonable performance.
Nevertheless, there are still many unexplored areas.

There is, for example, a lot of other recent work on fair representation
with adversarial neural networks.
These approaches vary in the structure of the network and the objective function.
These works are, however, all right now only in pre-print.
There is \citet{edwards2015censoring}, \citet{beutel2017data}
and \citet{zhang2018mitigating} for learning fair representations with adversaries.
\citet{calmon2017optimized} improve on the work by \citet{zemel2013learning}.

\citet{kilbertus2018blind} recently proposed a mechanism
where the sensitive attributes are encrypted to avoid being used for discrimination.
\citet{bechavod2017penalizing} use (as some others before) a regularization approach to train fair classifiers.

Finally, \citet{everitt2017reinforcement} investigate
a problem in reinforcement learning that is similar to bias in classification.
In the previously discussed papers,
the bias originates from the training inputs or the training labels.
In the work by Everitt et al.,
they consider the case of reinforcement learning (RL) with a corrupt reward signal.
In this case, the RL algorithm might know that the reward is not correct,
but does not know in what specific way it is not correct.
The paper discusses some strategies to deal with this,
but there are very many open questions.
