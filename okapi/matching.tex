%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Statistical matching is a sampling strategy which aims to balance the distribution of the observed
covariates in the \emph{treated} and \emph{control} groups. In general terms, observed covariates
$x$ are measured characteristics of the samples; in our work we refer to the encodings generated by
a deep neural network as covariates instead of the original characteristics. 
%
The treated and control groups are two partitions of the data; specifically, the treated group is
the set of samples having a specific value of a variable of interest (here, the domain indicator,
$s$) and the control group is its complement.

In this work we utilise Nearest Neighbour (NN) matching, a distance-based matching method that pairs
sample $i$ of the treated group with the closest sample $j$ belonging to the control group.
%
A distance measure is used to define how close two samples, $i$ and $j$, are, with \emph{propensity
score distance} (PSD) and \emph{Euclidean distance} being two widely-used distances that we employ
here
-- indirectly (as a means of filtering) and directly, respectively.

The propensity score distance is defined as the difference between propensity scores, $e_i$
and $e_j$, of samples $i$ and $j$, i.e. \( \mathrm{PSD}(e_i, e_j) \triangleq \vert e_i - e_j \vert \). 
%
In causal inference, the propensity score refers to the probability of sample $i$ belonging to the
treated group, given its covariates $x_i$ \citep{rosenbaum1983central}; in practice, this
conditional probability is rarely known a priori and thus requires estimation, typically via
logistic regression \citep{stuart2010matching}. 
%
We generalise the notion of a propensity score to categorical domains simply by modelling the
conditional probability for each domain, with  $e_i$ instead a \( |\gS| \)-dimensional probability
vector.
%
The Euclidean-distance approach, in contrast, computes the distance between the covariates,
$x_i$ and $x_j$, of a given pair of samples.
%
Despite PSD being the more prevalent of the two distances, it is ill-suited to cases in where pairs
are close in value \wrt{} all covariates and in such cases Euclidean distance should be preferred
\citep{king2019propensity}. 
%
Nevertheless, propensity scores remain a relevant component of NN-based matching for defining
\emph{calipers} that can reduce the likelihood of false-positive matches.
%

We make use of two types of caliper, \emph{fixed} and \emph{standard deviation}. The fixed caliper
\citep{crump2009dealing}, $t_f$, defines a region of common support between the estimated propensity
score distribution of the two groups; only those samples within the feasible region are admissible
for matching. For binary problems, the feasible region is symmetric such that we have \( \left\{i
\, | \, e_i \in (1 - t_{f}, t_{f}) \right\} \) whereas in the more general, categorical case the
constraint is one-sided, i.e. \( \left\{i \, | \, \lVert e_i \rVert_{\infty} < t_{f}) \right\} \).
%
This selection rule helps by removing samples with extreme propensity scores.
%
\citet{rosenbaum1985constructing} defines the maximum discrepancy permitted between paired two samples.
%
The discrepancy is usually expressed in terms of estimated PSD as \( \vert e_i - e_j \vert < \sigma
\cdot t_\sigma \), where \(\sigma\) denotes the mean of the group-wise standard deviations of the
propensity scores and \(t_\sigma\) controls the percentage bias-reduction of the
covariates. 
%
In the categorical case, we can simply substitute the absolute value for the infinity norm: \(
\lVert e_i - e_j \rVert_\infty < \sigma \cdot t_\sigma \).
%
In the following section, we describe how one can leverage this matching framework to define a
consistency loss encouraging inter-domain robustness.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
