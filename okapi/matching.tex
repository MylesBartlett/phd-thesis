Statistical matching is a sampling strategy which aims to balance the distribution of the observed
covariates in the \emph{treated} and \emph{control} groups. In general terms, observed covariates
$x$ are measured characteristics of the samples; in our work we refer to the encodings generated by
a deep neural network as covariates instead of the original characteristics.
The treated and control groups are two partitions of the data; specifically, the treated group is
the set of samples having a specific value of a variable of interest (here, the domain indicator, $s$) and
the control group is its complement.

In this work we utilise Nearest Neighbour (NN) matching, a distance-based matching method that pairs
sample $i$ of the treated group with the closest sample $j$ belonging to the control group.
%
A distance measure is used to define how close two samples, $i$ and $j$, are, with \emph{propensity
score distance} (PSD) and \emph{Euclidean distance} being two widely-used distances that we rely on
-- indirectly (as a means of filtering) and directly, respectively -- in this work.

The propensity score distance is defined as the difference between propensity scores, $e_i$
and $e_j$, of samples $i$ and $j$, i.e. $\mr{PSD}(e_i, e_j) \triangleq \vert e_i - e_j \vert$. In
causal inference, the propensity score refers to the probability of sample $i$ belonging to the
treated group, given its covariates $x_i$ \cite{rosenbaum1983central}; in practice, this
conditional probability is rarely known a priori and thus requires estimation, typically via
logistic regression \cite{stuart2010matching}. We generalise the notion of a propensity score to
categorical domains simply by modelling the conditional probability for each domain and
taking the $s$th value, i.e. the probability of the ground-truth domain.
%
In contrast to PSD, the Euclidean-distance approach computes the distance between the covariates,
$x_i$ and $x_j$, of a given pair of samples.
% i.e. $d_{ij} = (x_i - x_j)^{T} (x_i - x_j)$.
Despite PSD being the more prevalent of the two distances, it is ill-suited to cases
in where pairs are close in value w.r.t. all covariates and in such cases Euclidean distance should
be preferred
\cite{king2019propensity}. 
%
Nevertheless, propensity scores remain a relevant component of NN-based matching for defining
\emph{calipers} that can reduce the likelihood of false-positive matches.
%

In this work we make use of two types of caliper, \emph{fixed} and \emph{standard deviation}. The fixed
caliper \cite{crump2009dealing}, $t_f$, defines a region of common support between the estimated
propensity score distribution of the two groups; only those samples within the feasible region are
admissible for matching. For binary problems, the feasible region is symmetric such that we have
$\left\{i | e_i \in (1 - t_{f}, t_{f}) \right\}$) whereas in the more general, multiclass case
$e_i$ the constraint is one-sided, i.e. $\left\{i | e_i < t_{f}) \right\}$.
%
This selection rule helps by removing samples with extreme propensity scores and therefore highly
characteristic of the domain to which they belong.
% previous studies \cite{crump2009dealing} consider the common region $[0.1, 0.9]$. , i.e.
% $\left\{i | e_i \in [0.1,0.9] \right\}$.
% a rule-of-thumb used in previous studies \cite{crump2009dealing} considers the common region [0.1, 0.9]. 
The standard deviation-based caliper (std-caliper), on the other hand, \cite{rosenbaum1985constructing} determines the
maximum discrepancy that might exist between two samples while still being admissible for pairing.
%
The discrepancy is usually expressed in terms of estimated PSD, such that we have $|e_i - e_j| <
\sigma \cdot t_\sigma$ for any pair of samples, $i$ and $j$. 
%
Here, $\sigma$ denotes the mean of the group-wise standard deviations of the propensity
scores and $t_\sigma$ is a parameter controlling the percentage of bias reduction of the covariates. 
% Cochran and Rubin \cite{rubin2000combining} show the smaller the $\alpha$ the more bias reduction
% is obtained and the actual percentage of reduction depends on $\sigma$. Commonly used $\alpha$
% values are $\{0.2, 0.4, 0.6\}$ \cite{rubin2000combining}.
In the following section, we describe how one can leverage this matching framework to define a
consistency loss encouraging inter-domain robustness.
