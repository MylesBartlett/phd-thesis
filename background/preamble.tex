% ****************************************************************************** 
\section{Preamble}
% ****************************************************************************** 
\noindent
%
In this chapter we aim to provide background on the topics encompassed by the works in this thesis,
both individually, and holistically.
%
While we will on occasion point to exemplar methods, generally we will eschew delving deep into
specific methodologies -- of which there are many and many more being proposed by the day -- in
favour of keeping broader perspectives regarding the motivations of, assumptions made by, and
interconnections between, the considered learning paradigms. 
%
This is to say, this chapter does not aspire to be a comprehensive survey of Domain Adaptation
(DA), Domain Generalisation (DG), Fair Machine Learning (FairML), and the other germane subfields
touched on herein; producing such for any one of these subfields is in itself a considerable
undertaking given the breadth and depth the Machine Learning literature, the field having grown
precipitously over the last decade since the onset of the deep-learning revolution heralded by
\cite{Krizhevsky2012imagenet}.
%
The aspirations of this chapter, on the contrary, are much more humble, simply being to provide the
requisite (high-level) background for, and unified and alternative perspectives of, the problems
and methodologies featured in Chapters 3, 4 and 5.
%
Indeed, each of said chapters contain their own background sections drawing direct comparisons to
related work and we wish to avoid repetition in this respect.

The main themes of this thesis, as indicated by the title, are (SemiSL) and distributional
robustness, the latter in the context of FairML (Chapters 3 and 4) and DG (Chapter 5),
specifically. 
%
We will cover these topics directly, but to properly contextualise and motivate them requires
visiting both foundational and adjacent areas of ML.

%
With the above in mind, we begin our discussion of the classical supervised learning setup and how
standard empirical-risk minimisation (ERM) is ill-suited long-tailedness and distribution shifts,
both pervasive phenomena in real-world applications. \emph{Distributions shift} as a term is highly
polysemous, meaning very different things, and demanding commensurately different solutions,
depending on the underlying mechanisms and the direction of causality. We will give a brief
taxonomy of the different kinds of distribution shifts in terms of how the marginal and conditional
distributions are affected, and what may cause them. 

%
Spurious correlations (SCs), or (statistical) shortcuts (we will use the terms interchangeably
throughout), give rise to a particularly aggressive form of distribution shift as a result of
features in the training data being highly (conceivably to the degree of a one-to-one
correspondence) correlated with the target but not in a way that is causally consistent, and thus
in a way that should not be expected to hold consistently at test time. 
%
The idea of SCs is central to both Chapters 3 and 4 (manifested in different ways) and the idea of
Shortcut Learning (SCL) has close ties to DG \citep{arjovsky2019invariant}, the focus of Chapter 5;
in light of this, we afford dedicated discussion of the SCL problem and under the conditions needed
to engender it.

%
Before branching off into discussion of the specific learning paradigms, we first reframe the
problem of supervised learning and distribution shift through the lens of causality, yielding a
unified perspective of the latter and when when SemiSL (and by extension \emph{Self-Supervised
Learning} (SelfSL)) should be expected to work.
%
While this thesis does not directly tangle with questions of causal inference, the field of
causality \citep{pearl2009causality} provides, through Causal Bayesian graphs (CBGs) and
interventions thereof, the means of expressing different distribution shifts and the
desired/undesired variances/invariances using a single, formalised calculus.
%
Equipped with this calculus, we conclude this section -- as alluded to above -- with a discussion
of the specific subfields and learning paradigms of relevance to the three papers constituting this
thesis, namely:  DA, DG, FairML, SemiSL, SelfSL, Adversarial Learning (AL), and Normalising
Flows/Invertible Neural Networks (NFs/INNs).

