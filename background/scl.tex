% ********************************************************************************
\section{Shortcut learning}\label{sec:shortcut-learning}
% ********************************************************************************
% \epigraph{
%     %
%     \emph{
%       ``Short cuts make for long delays.''
%     %
% } 
% %
% }
% {The Fellowship of the Ring\\J.R.R. Tolkien}
%
%
While the notoriety of \acf{SCL} in \ac{ML} is relatively recent, the phenomenon
underpinning it is a fundamental one in statistics, one that may be summed up with the age-old
fallacy \emph{cum hoc ergo propter hoc}, or \emph{correlation does not imply causation}.
%
\sidepar{Simplicity bias}
%
\Acp{DNN} define highly expressive function classes, yet the solutions encoded by their parameter
space need not be commensurately complex; in fact, it is well established that these models -- in
the absence of an countervailing (inductive) bias -- exhibit a \emph{simplicity bias} (SB;
\citealp{valle2018deep}), that is, the tendency to favour simpler solutions, should those solutions
serve sufficiently well for the task (as defined by the training set and loss function) at hand.
%
While the spate of failures following the deep-learning revolution were surprising -- and at the
very least mildly-disenchanting to those with lofty hopes for \ac{ML} -- it is, given thought,
\emph{not} surprising that SB should exist and beget \ac{SCL}, for while SB alone is not alone a
precondition for \ac{SCL}, the second precondition  of \ac{SCL} is a problem (on the data side)
that has long challenged statistical modelling: \emph{sampling bias}.
%
It is the combination of simplicity bias and acausal or spurious, correlations generated by
sampling bias that give rise to \ac{SCL}, but sampling bias is not something trivially redressed,
even if the seemingly-straightforward recourse of `collect more data' does exist, which it often
does not due to physical constraints (e.g.\ the data may only have been available within a given
period of time) or limited (human or monetary) resources.
%
Although the problem may stem from the data-collection side, one is not without recourse on the
modelling side, so long as certain assumptions or criteria can be met;
%
indeed, both \ac{DG} and \ac{AF} are active -- more so than ever -- subfields of \ac{ML} contending with
different flavours of the problem and have successfully developed mitigation strategies for them.

%
The now-canonical example of shortcut-learning in the \ac{DG} literature -- which I will also
invoke here for its simplicity -- is due to \citet{beery2018recognition}, wherein the task is one of
distinguishing between cows and camels (binary classification). 
%
\sidepar{Corsican cows}
%
Since camels preponderate on sandy backgrounds, while, by contrast, cows preponderate on grassy
backgrounds -- owing to their natural habitats -- the background is a viable shortcut solution
based on which examples from the training set can be reliably predicted while taking the path of
least resistance, something the model can hardly be blamed for in absence of the requisite
inductive bias to disentangle the true and spurious features.
%
While the brittleness of the shortcut solution will not be exposed if the test set consistently
suffers the same sampling bias as the training set, it is perfectly conceivable that a cow could
appear on a beach -- a common sight on the island of Corsica, for example -- and our model would
mispredict in such a case because it does not grasp what the concept of a cow truly is -- to it,
`grassy' and `cow' are synonymous.
%
This is a relatively benign example, but there are many real-world cases where such behaviour could
induce life-endangering failures, perhaps most obviously in the medical data domain where one could
have a pneumonia classifier that has learned to predict pneumonia from X-ray images with
near-perfect accuracy based solely on a hospital-specific token and the hospitals'
pneumonia-prevalence rate, as elucidated by \citet{zech2018variable}.
%
There are also obvious ethical concerns that arise when the spurious features in question
correspond to sensitive attributes such as `race' and `gender' \citep{buolamwini2018gender,
wang2019balanced}, regardless of aggregate downstream performance. 
%
The landmark study by \citet{buolamwini2018gender}, for example, revealed significant disparities
in the performance of face analysis algorithms on individuals from marginalised (dark skin, female)
vs.\ non-marginalised (light skin, male).

There are two facets of \ac{SCL} that impair generalisation.
%
The obvious one, which we have already belaboured, is \emph{variance} to spurious features --
features that are statistically but not causally related to the target; the second one, however, is
more subtle and a consequence of the first one, that being \emph{feature suppression}, in that the
model is not simply variant to the `wrong' features but invariant to the `right' ones -- it is not
simply a matter of a difference in importance but, in reality, a more pernicious matter of
inclusion/exclusion.
%
\sidepar{Feature suppression and the multi-view hypothesis}
%
This is to say, if a shortcut solution is robust enough to achieve near-zero loss on the training
set, then there is little incentive -- owing to gradient starvation \citep{pezeshki2021gradient}
and the provably-flatter minima of shortcut solutions \citep{scimeca2021shortcut} -- for the model
to learn alternative `views' (collections of features; \citealp{allen2020towards}).
%
For instance, if texture is a reliable classification cue given the training data
\citep{geirhos2018imagenet}, a model can latch onto that cue and ignore (be invariant to) other
higher-level semantics, like shape and global structure, that human judgements are much more
strongly ascribed to.
%
High-frequency cues, such as colour and texture, are readily modulated by (unstable under) changes
in lighting, for instance, making them less reliable cues for object classification in a dynamic
environment; we are not wont, for example, to classify an object in the shape of a cat as an
elephant simply because the texture of the latter has been transplanted, \emph{ceteris paribus}, to
the former, a failure mode (in)famously shown by \citet{geirhos2018imagenet} to apply to
ImageNet-trained \acp{DNN}.

With the above in mind, it is obvious why more traditional approaches to improving group- and
adversarial-robustness fail. 
%
The power of ensembles, for instance, resides in their combining of different views of the data --
engendered by stochasticity in the weights and optimisation procedure -- yet shortcut solutions
create such a strong (easy-to-learn and potent) and stable attractor that all ensemble
members simply converge onto that one corresponding view.
%
Domain adversarial-learning -- popularised by \citet{ganin2016domain} and since a mainstay throughout the
\ac{DA}, \ac{DG}, and \ac{AF} literature alike -- on the other hand suffers from the problem that
for the features of the model to be statistically independent of the spurious feature, so must they
be statistically independent of the target since the target and spurious feature are themselves
strongly correlated, as defined by the \ac{SCL} problem.
%
\corr{
  %
I shall bestow full formalism to this particular interaction in Chapter~\ref{ch:supmatch}, but will
draw an example from the group-robustness literature here to attempt to give some immediate
clarification.
%
The CelebA dataset constitutes a standard benchmark in said literature, with the problem setup
popularised by \citet{sagawa2019distributionally} one of hair colour (\( \{ \text{not Blond},
\text{Blond} \} \)) prediction, with gender (\( \{ \text{Female}, \text{Male} \} \)) acting
as a spurious feature by virtue of blond females preponderating over blond males, such that strong
aggregate performance can be achieved with a constant, gender-conditioned predictor (a binary
decision-rule designating all females to be blonde, all males not blond).
%
A representation that is statistically independent of (invariant to) gender, but not hair colour,
satisfies our desideratum but is not a fixed point of the minimax game implied by the adversarial
learning problem (as I shall elaborate on in \S\ref{sec:advl}) as gender can be predicted with
above-random accuracy by the adversary based only on hair colour and the skewed statistics of the
intersectional groups.
%
}
% CORRECTED: could you add an example here to make it clear what you mean as done in the viva
% %
% % % --------------------------------------------------------------------------------
% % \paragraph{Simplicity Bias \citep{valle2018deep, shah2020pitfalls}}
% % % --------------------------------------------------------------------------------
% % \begin{itemize}
% %   \item In stationary settings where there is no mismatch between the training and test 
% %     distributions, generalisation is usually maximised by selecting models according to the 
% %     statistical equivalent of Occam's Razor, of using the simplest model that explains the data 
% %     well. However, when the training and test distributions are not aligned in some sense, this 
% %     fails to hold up where the simplest models are those that latch onto spurious statistical 
% %     correlations in the training data and thus are not stable under non-causal interventions in 
% %     the marginal or conditional distributions of $P(Y|X)$ (resulting in \emph{covariate} shift 
% %     and \emph{concept} drift respectively).
% %     %
% %   \item On real-world datasets there are several distinct ways to discriminate between labels (e.g. 
% %     based on shape, colour, texture, etc.) that are (a) predictive of the label to varying extents, 
% %     and (b) define decision boundaries of varying complexity. 
% %     %
% %   \item For example, in the image-classification task of swans vs. bears, a linear-like simple 
% %     classifier that only looks at colour could predict correctly on most instances except white 
% %     polar bears, while a non-linear complex classifier that infers shape could have almost perfect 
% %     predictive power.
% % \end{itemize}


% % Key findings from \cite{scimeca2021shortcut}:
% % \\begin{enumerate}
% %          \item Certain cues (conducive to the visual recognition problem at hand) are preferred to
% %            others
% %          \item Solutions biased to the easy-to-learn cues tend to converge to relatively flat minima
% %            on the loss surface.
% %          \item The solutions focusing on those preferred cues are far more abundant in parameter
% %            space.
% %          \item Solutions corresponding to Kolmogorov-simple cues are abundant in parameter space
% %            and thus preferred by DNNs.
% %           \item Devise a setup where multiple cues are equally valid for solving the task at hand;
% %             find that DNNs attend to choose cues in a certain order (e.g. colour is preferred to
% %             rotation).Simple cues, based on Kolmogorov complexity, are far more frequently
% %             represented in the parameter space and thus are far more likely to be adopted by DNNs.
          
% %  \end{enumerate}
% % Extended notes on \cite{scimeca2021shortcut}:
% % \begin{itemize}
% %         \item In some cases, the shortcut bias arises in models that suppress certain streams of
% %           inputs: visual question answering (VQA) models often neglect whole-image cues, for one
% %           does not require images to answer questions like ``what question is the banana in the
% %           image?'' \citep{cadene2019rubi}.
% %         \item Shortcut biases become problematic when it comes to generalisation to more
% %           challenging test-time conditions, where the shortcuts are no longer valid. These biases
% %           also cause ethical concerns when the shortcut features correspond to sensitive attributes
% %           like gender or race \citep{wang2019balanced}.
% % \end{itemize}

% % Notes on and from \cite{shah2020pitfalls}:
% % \begin{itemize}
% %         \item Well-studied approaches such as for improving group- and adversarial-robustness --
% %           such as ensembling and adversarial training -- do not mitigate the problem of simplicity
% %           bias on the proposed datasets.
% %           %
% %           The failure of ensembling is to be expected, as there if the simplicity bias is
% %           sufficiently strong, there is little-to-no incentive for the ensemble members to learn
% %           diverse views of the data \citep{allen2020towards} due to stochasticity in the weights
% %           and optimisation procedure, this diversity being where the presumed origin of the
% %           improved-robustness demonstrated by (deep) ensembles.
% %           %
% %           The failure of adversarial training is also to be expected given that the perturbations
% %           do nothing to intervene on the spurious feature directly.
% %           %
% % \end{itemize}

% % Notes on and from \cite{geirhos2020shortcut}:
% % \begin{itemize}
% %   \item Despite DNNs being touted as achieving super-human-level performance on complex tasks
% %     across myriad domains, we are currently observing a large number of failure cases stemming from
% %     small -- imperceptible even -- changes in the inputs or or changes in different background
% %     contexts that should be incidental to the task.
% %     %
% %   \item DNNs can generate plausible captions for images and yet do so without looking at the image
% %     in sooth \citep{cadene2019rubi}. 
    
% %     %
% %   \item DNNs can accurately recognise faces but achieve significantly higher error rates for faces
% %     from marginalised groups \citep{buolamwini2018gender}.
% %   %
% %   \item DNNs can predict hiring decisions based on resumes but the predictions are biased towards
% %     selecting men \citep{dastin2018amazon}.
% %   \item One central observation is that many failure cases are not independent phenomena but
% %     instead are interconnected in the respect that they are the result of DNNs adopting unintended
% %     `shortcut' strategies.
% %     %
% %     While superficially successful, these strategies typically fail under slightly different
% %     circumstances; for instance, a DNN may appear to caption an image perfectly well, but describes
% %     a typical grass landscape as 'a herd of grazing sheep'

% %   \item At a principal level, shortcut learning is not a novel phenomenon. 
% %     The field of \ac{ML} has long aspired to develop a formal understanding of shortcut learning, which
% %     has led to an increasing body of research under different names, such as `learning under
% %     covariate shift', `anticausal learning', `dataset bias' and the `Clever Hans effect'.

% %   \item Shortcuts are decision rules that perform well on \iid{} test data but fail on o.o.d. test
% %     data, revealing a mismatch between the intended and learned solutions.
% % \end{itemize}

