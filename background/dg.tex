% ********************************************************************************
\section{Domain Generalisation}\label{sec:domain_generalisation}
% ********************************************************************************
While closely-related to DA, Domain Generalisation (DG) is distinct in the respect that the task is
fundamentally, as the name suggests, one of \ood{} generalisation rather than one of adaptation.
%
By this we mean that while in (U)DA one is given a labelled dataset, \( \gD^{src} \triangleq
\{x_i, y_i \}_{i=1}^{N^{src}} \), belonging to the source domain, along with an unlabelled
dataset \( \gD^{tgt} \triangleq \{ x_i \}_{i=1}^{N^{tgt}} \) belonging to the target domain,
and the goal is to train a classifier to generalise from the former to the latter (which entails a
degree of invariance), DG is more general, in that one is instead given datasets from multiple
domains and seeks to train a classifier that can generalise to previously unseen ones. 
%
That is, given the meta distribution \( \mathfrak{D} \triangleq \{\gD_e \}_{e \in \gE} \) consisting of
\( |\gE| \) distributions drawn from different domains, or \emph{environments}, denoted by the
index set \( \gE \subset \sN \), the goal is to train a classifier that will perform optimally, or
with minimal degradation, when presented with distribution \( \gD_{e^{te}} \) from a novel
domain \( e^{te} \notin  \gE \).

%
While we would ideally have a model that could generalise to any arbitrary environment (assuming
the task remains consistent), this is sadly impossible given finite data
\cite{david2010impossibility}, and so our expectations must be tempered to being able to generalise
within some region the training distribution.
%
The justification of DG can then be viewed from two perspectives: 
\begin{enumerate}
%
    \item 
%
        It stands to reason that we should be able to exploit information about known the known set
        of variances -- due to the domain -- in order to learn a predictor that can generalise
        within the convex set (affine combinations of those variances) they define as well of those
        variances that are close by.
%
        The principle here is similar to that of vicinal risk minimisation
        \citep{chapelle2000vicinal} as in \cite{zhang2017mixup}, wherein data augmentation fulfils
        the role of a perturbation set that the environments fulfil in DG.
%
    \item
    %
        Given a set of interventions on the underlying causal graph defined by the set of
        environments, recover the causal relationship between the input features and the target
        such that the predictive mechanism is unaffected by causally-independent changes (by
        interventions on variables not among the target variable's causal parents).
%
        This idea of treating environments as interventions and using them to perform explicit or
        implicit causal inference has notably been exploited in \cite{peters2016causal} and in the
        foundational (to DG) work of \cite{arjovsky2019invariant}.
%
        Indeed, in the wake of \cite{arjovsky2019invariant}, it has become common \citep{
        gulrajani2020search, krueger2021out, mahajan2021domain, lin2022zin} to express the problem
        setup of DG and its desiderata in causal terms, and we will do so ourselves in
        \S~\ref{sec:lens-of-causality} in order to provide a more unified perspective of the
        distribution shift problems discussed thus far.
%
\end{enumerate}

% --------------------------------------------------------------------------------
\subsection{Connection between Fairness and DG}\label{ssec:fairml-dg-cxn}
% --------------------------------------------------------------------------------
\citet{creager2021environment} cast the problem fair machine learning as one of DG,
where the protected groups takes the role of the different domains/environments. 
%
In fairness literature, the learning objectives represent context-specific fairness notions, while
in OOD literature, the learning objectives should be designed according to invariance assumptions.
%
A number of fair representation learning methods \citep{edwards2015censoring, madras2018learning}
are derived from domain adaptation (DA) methods. When protected attributes are unknown, DRO and
adversarially learning can be applied as in \citet{hashimoto2018fairness} and
\citet{lahoti2020fairness}, respectively, to obtain a distributionally robust predictor and
minimizing the worst-subgroup performance; the former can also be adapted to cases in which such
information is known as in \citet{sagawa2019distributionally} 

% (\citep{krueger2021out} provide a good (albeit brief) discussion of the parallels between fairness and OOD
% generalization)

% Recent trend in the fairness literature is to consider how fairness behaves under distribution
% shift.
% %
% \begin{itemize}
%   \item \cite{schrouff2022diagnosing}
%   \item \cite{schrouff2022maintaining}
%   \item \cite{singh2021fairness}
%   \item \cite{slack2020fairness}

% \end{itemize}

% % --------------------------------------------------------------------------------
% \subsection{A (brief) taxonomy of domain-generalisation methods}
% % --------------------------------------------------------------------------------
%
% \subsection{In Search of Lost Domain Generalization \citep{gulrajani2020search}}
% %
% Machine learning systems often fail to generalise out-of-distribution (OOD), crashing in 
% spectacular way when tested outside the domain of training examples.

% \itemi\begin{itemize}
%   \item Self-driving car systems struggle to perform under conditions different to those of 
%     training, including variations in lighting \citep{dai2018dark}, weather \citep{volk2019towards}, 
%     and object poses \citep{alcorn2019strike}
%   \item Systems trained on medical data collected in one hospital do not generalise to other health
%     centres \citep{castro2020causality, albadawy2018deep}
%   \item failing to generalise is failing to capture the causal factors of variation in data, 
%     clinging instead to easier-to-fit spurious correlations, which are prone to change from 
%     training to testing domains (unstable in the face of interventions)
%   \item  Examples of spurious correlations (SC) commonly encountered in machine learning include 
%     racial biases, texture biases \citep{geirhos2018imagenet}, and object backgrounds
%     \citep{beery2018recognition} .
%   \item Alas, the capricious behaviour of machine learning systems to distributional shifts is a 
%     roadblock to their deployment in critical applications.
% \end{itemize}

% %
% The goal of DG is OOD generalisation: learning a predictor able to perform well on some unseen test
% domain when no data from the test domain is available during training -- we must assume the 
% existence of some statistical invariances across training and testing domains.
% %
% - DG differs from Domain Adaptation (DA) in that the latter assumes that the unlabelled data derived 
% from the test domain is available during training.


