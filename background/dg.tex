% ********************************************************************************
\section{Domain generalisation}\label{sec:dg}
% ********************************************************************************
I will discuss in \S\ref{sec:lens-of-causality} how the underlying problem of \ac{DG} may be
couched in terms of exogenous variables, over which a bilevel optimisation problem is defined, and
this section shall be, accordingly, reasonably brief in eschewing redundancy; I here focus on
providing a preliminary introduction to the problem, on distinguishing \ac{DG} from its sister
field of \ac{DA} and in providing intuition for why \ac{DG} can work despite some impossibilities.
%
Indeed, I say `sister' for both \ac{DA} and \ac{DG} aim to maximally transfer knowledge between
domains while achieving invariance to domain-specific (nuisance) factors. 
%
However, the paradigms diverge in the respect that the task entailed by \ac{DA} is fundamentally,
as per its name, one of \ac{OOD} generalisation -- in the strict sense of the transfer in question
being to novel domains -- rather than adaptation to a closed set of given, and thus known and seen,
domains.
%
\sidepar{The design of domain generalisation}
%
To endow some formalism, while in \ac{UDA} one is given a labelled dataset, \( \gD^{src} \triangleq
\{x_i, y_i \}_{i=1}^{N^{src}} \), belonging to the source domain, along with an unlabelled dataset
\( \gD^{tgt} \triangleq \{ x_i \}_{i=1}^{N^{tgt}} \) belonging to the target domain, and the goal
is to train a classifier to generalise from the former to the latter (which entails a degree of
invariance), \ac{DG} is more general, in that one is instead given datasets from multiple domains
and seeks to train a classifier that can generalise to previously unseen ones. 
%
That is, given the (empirical) meta distribution \( \mathfrak{D} \triangleq \{\gD_e \}_{e \in \gE}
\) consisting of \( |\gE| \) distributions drawn from different domains, or \emph{environments},
denoted by the index set \( \gE \subset \sN \), the goal is to train a classifier that will perform
optimally, or with minimal degradation, when presented with distribution \( \gD_{e^\dagger} \) from
a novel domain \( e^\dagger \notin  \gE \).
%
This can be done in the \ac{DRO} fashion described by Eq.~\ref{eq:dro-obj}, with the meta
distribution induced by \(\gE\), as in Eq.~\ref{eq:rob-erm-obj}.

%
We would, of course, hope our model to generalise (well) to any arbitrary environment (assuming the
task remains consistent), yet this is sadly impossible given finite data
\citep{david2010impossibility}, and so our expectations must be tempered to being able to
generalise within some region the training distribution.
%
\sidepar{Justifying domain generalisation}
%
Justification for \ac{DG} might then be viewed according to the following perspectives,
respectively rooted in \ac{DRO} and causality.
%
First, given a set of known variances -- induced by the domains -- one should expect to be able to
leverage these variances to learn a predictor able to generalise both within the convex hull
(affine combinations of elements of the set) they define as well as to those variances within the
vicinity -- proximal to -- this hull \citep{krueger2021out}.
        %
The principle here is similar to that of vicinal risk minimisation \citep{chapelle2000vicinal} as
in \cite{zhang2017mixup}, wherein data augmentation fulfils the role of a perturbation set that the
environments fulfil in \ac{DG}.
%
Second, given a set of interventions on the underlying causal graph defined by the set of
environments, one would expect to be able to recover -- if only partially -- the causal
relationship between the input features and the target such that the predictive mechanism is
unaffected by causally-independent changes (by interventions on variables not among the target
variable's causal parents).
%
This idea of treating environments as interventions and using them to perform explicit or
implicit causal inference has notably been exploited in \cite{peters2016causal} and in the
foundational (to \ac{DG}) work of \cite{arjovsky2019invariant}.
%
Indeed, in the wake of \cite{arjovsky2019invariant}, it has become common \citep{
gulrajani2020search, krueger2021out, mahajan2021domain, lin2022zin} to express the problem setup of
\ac{DG} and its desiderata in causal terms.
%

% % --------------------------------------------------------------------------------
% \subsection{Connection between Fairness and DG}\label{ssec:fairml-dg-cxn}
% % --------------------------------------------------------------------------------
% \citet{creager2021environment} cast the problem fair machine learning as one of DG, where the
% protected groups takes the role of the different domains/environments. 
% %
% In fairness literature, the learning objectives represent context-specific fairness notions, while
% in OOD literature, the learning objectives should be designed according to invariance assumptions.
% %
% A number of fair representation learning methods \citep{edwards2015censoring, madras2018learning}
% are derived from domain adaptation (DA) methods. When protected attributes are unknown, DRO and
% adversarially learning can be applied as in \citet{hashimoto2018fairness} and
% \citet{lahoti2020fairness}, respectively, to obtain a distributionally robust predictor and
% minimizing the worst-subgroup performance; the former can also be adapted to cases in which such
% information is known as in \citet{sagawa2019distributionally} 

% (\citep{krueger2021out} provide a good (albeit brief) discussion of the parallels between fairness and OOD
% generalization)

% Recent trend in the fairness literature is to consider how fairness behaves under distribution
% shift.
% %
% \begin{itemize}
%   \item \cite{schrouff2022diagnosing}
%   \item \cite{schrouff2022maintaining}
%   \item \cite{singh2021fairness}
%   \item \cite{slack2020fairness}

% \end{itemize}

% % --------------------------------------------------------------------------------
% \subsection{A (brief) taxonomy of domain-generalisation methods}
% % --------------------------------------------------------------------------------
%
% \subsection{In Search of Lost Domain Generalization \citep{gulrajani2020search}}
% %
% Machine learning systems often fail to generalise out-of-distribution (OOD), crashing in 
% spectacular way when tested outside the domain of training examples.

% \itemi\begin{itemize}
%   \item Self-driving car systems struggle to perform under conditions different to those of 
%     training, including variations in lighting \citep{dai2018dark}, weather \citep{volk2019towards}, 
%     and object poses \citep{alcorn2019strike}
%   \item Systems trained on medical data collected in one hospital do not generalise to other health
%     centres \citep{castro2020causality, albadawy2018deep}
%   \item failing to generalise is failing to capture the causal factors of variation in data, 
%     clinging instead to easier-to-fit spurious correlations, which are prone to change from 
%     training to testing domains (unstable in the face of interventions)
%   \item  Examples of spurious correlations (SC) commonly encountered in machine learning include 
%     racial biases, texture biases \citep{geirhos2018imagenet}, and object backgrounds
%     \citep{beery2018recognition} .
%   \item Alas, the capricious behaviour of machine learning systems to distributional shifts is a 
%     roadblock to their deployment in critical applications.
% \end{itemize}

% %
% The goal of DG is OOD generalisation: learning a predictor able to perform well on some unseen test
% domain when no data from the test domain is available during training -- we must assume the 
% existence of some statistical invariances across training and testing domains.
% %
% - DG differs from Domain Adaptation (DA) in that the latter assumes that the unlabelled data derived 
% from the test domain is available during training.


