% ********************************************************************************
\section{ Semi-supervised learning }\label{sec:semisl}
% ********************************************************************************
%
Given the titular reference to \ac{SemiSL}
%
\footnote{ 
  %
  Since self-supervised learning is also referenced in this thesis (primarily in Chapter
  \ref{ch:okapi}) we must depart from the typical initialism, `SSL', so as to be able to
  differentiate the two learning paradigms.
%
},
%
 it is only appropriate that a part of this background section be devoted to the
topic.
%
However, I note that the methods introduced in this thesis are not tailored for the typical
\ac{SemiSL} regime wherein one hopes to draw upon a large corpus of unlabelled data to shore up the
paucity of annotated data available for direct supervision, with the assumption being that the
unlabelled and labelled data are drawn from the same distribution.
%
Rather, the unifying theme across the constituent papers is how one can use unlabelled data to
buttress against different types of distribution shift.
%
The problem setups considered thus more closely align with those found in \ac{DG} and \ac{DA}. 

I would refer the reader to \citet{chapelle2009semi} for excellent (in both clarity and depth)
exposition of the theoretical underpinnings of \ac{SemiSL} and methods for it from the
pre-\ac{DL} era (many of which are still fundamentally applicable today, however), a book I
will reference extensively throughout this brief overview of the topic.
%
For a comprehensive and current survey of \ac{SemiSL} methods in the current \ac{DL} era, on the
other hand, I would refer the reader to \citet{yang2022survey}.
%

The premise of \ac{SemiSL} is a simple one: given that in many real-world cases collecting annotated
data is expensive (monetarily or temporally) or even prohibitive (for instance, if the data is tied
to a transient phenomenon) but collecting data in general is not, how can one exploit the
unannotated data to improve a model's predictive power?
%
Thus, we can think of our dataset as having two partitions: one corresponding to the labelled
dataset, as in \S\ref{sec:erm}, which we can use for standard \acl{SL} and which we will override
here with the notation \( \gD^{tr}_l \triangleq \{ x_i \}_{i=1}^{N^{tr}_l} \) for clarity's sake,
and a second partition corresponding to the unannotated data, which we will denote by \( \gD^{tr}_u
\triangleq \{ x_i \}_{i=1}^{N^{tr}_u} \).
%
One is generally motivated to employ some form of \ac{SemiSL} in cases when \( N^{tr}_l \ll
N^{tr}_u \), though this not need be the case and it may be that the unlabelled data can be useful
beyond simply providing more data from the same distribution (as the labelled data), as epitomised
by \ac{UDA} and as explored in other contexts throughput this thesis.
%
There exist many different branches and perspectives of \ac{SemiSL}, as a learning paradigm with deep
roots reaching as far back as the 60s \citep{scudder1965probability, fralick1967learning}. 
%
On the perspective side, one can, for instance, view \ac{SemiSL} as \ac{UL} learning subject to
constraints -- which is especially pertinent in the case of semi-supervised clustering
\citep{bair2013semi} -- though it is usually more natural to frame it from the opposite
perspective, namely, as \ac{SL} with additional information \citep{chapelle2009semi}.

%
Closely related to \ac{SemiSL} is the idea of \emph{Transductive Learning} (TL;
\citealp{gammerman1998learning}).
%
With TL, rather than pursuing the lofty goal of learning a predictor that can generalise across the
entire input domain, \( \gX \) -- reflecting the inductive process of extrapolation -- one instead
focuses on predicting well on a restricted domain defined by the test points -- reflecting the
transductive process of transferring rules between specific cases. 
%
\sidepar{Transduction}
%
That is, the optimisation problem is reduced from finding the (loss-function) minimiser over \(f
\in \gF \)  to the considerably more tractable problem of finding the minimiser over \(
f|_{\gX^{te}} \in \gF \).
%
Intuitively, it makes sense to optimise the predictor for the subset of the domain of interest,
given that one has the access to said subset and has the necessary time/resources, rather than to
take the more circuitous approach of learning general rules and applying them to specific cases
(the process of \emph{deduction}).
%
To couch TL in \ac{SemiSL} terms then simply requires equating \(\gD^{tr}_u \) with the test set
\(\gD^{te} \).
%
We can view this distinction between TL and (inductive) \ac{SemiSL} as analogous to the distinction
between \ac{DA} and \ac{DG}, in the sense that the former has the transductive goal of generalising
between two specific domains -- with the target domain given at training time -- while the latter
has the inductive goal of generalising to all possible domains.
%
Niceness of this inter-field parallel aside, I afford TL particular mention here due to its
pertinence in Chapter~\ref{ch:supmatch}, wherein we consider the possibility of using the test set
itself as a reference dataset for the proposed matching procedure.


% --------------------------------------------------------------------------------
\subsection{Justifying \acl{SemiSL}}\label{ssec:assumptions}
% --------------------------------------------------------------------------------
While \ac{SemiSL} is a tantalising prospect whenever one has a large corpus of unlabelled data
coupled with a relatively sparing labelled data, it is unfortunately not the case that whenever
there is unlabelled data available that one can mine it for additional information about the given
task. 
%
In fact, in some cases -- those in which there is an element of distribution shift -- one might
find a degradation in performance when using \ac{SemiSL}, relative to the supervised baseline trained on
a small fraction of the samples.
%
Indeed, theory prescribes that certain assumptions about the data-generating process be met in
order for the learning paradigm to bear fruit (in the sense of reduced
generalisation-error/improved sample efficiency), whatever the chosen method, though this is not to
say that one might not observe practical benefits -- such as improved convergence-rates or
stability -- detached from those assumptions.
%
I will broach the importance of the direction of causality -- that is how the data-generating
process factorises -- later in this section; to begin with, following \citep{chapelle2009semi}, I
summarise the justifying assumptions for \ac{SemiSL}.
%
These assumptions are not complementary, in the sense that they can, or need be, simultaneously
satisfied; rather they provide three different perspectives (`perspective' being perhaps the more
apposite term) engendering different classes of algorithms. 
%
The cluster and low-density-separation (LDS) assumptions most obviously form a dual-view of the same
fundamental principle, understanding `cluster' to mean `high-density-connectedness'.
%

%
To elaborate, the \emph{cluster assumption} posits that data-points that are connected by a path
through density regions should belong to the same class.
%
\sidepar{Cluster Assumption}
%
This is precisely the assumption that drives many traditional (density-based) clustering algorithms
aiming to separate the data into groups of samples, or `clusters', using density (estimable, for
instance, with kernel methods or neighbourhood graphs) as a surrogate for ground-truth labels.
%
Within \ac{SemiSL} itself, the cluster assumption is well encapsulated by the method of \emph{label
propagation} \citep{szummer2001partially} which (with great simplification) involves 1) building a
neighbourhood graph with the labelled and unlabelled samples as its vertices and the edges weighted
according to local correlation strength; 2) propagating the label distributions from the labelled
samples to the adjacent unlabelled samples in a Markovian fashion.
%

If one flips the cluster assumption, such that we instead have the axiom `data-points that are not
connected by paths through high-density regions belong to different clusters', then one obtains the
LDS assumption, though this is more commonly expressed in terms of the decision boundary, namely
that the plane separating any two classes should carve out a region of low density.
%
\sidepar{Low-density-separation assumption}
%
Despite the equivalence, the two afford very different perspectives, from an optimisation
standpoint. Indeed, the aforementioned density-based clustering algorithms, of which DBSCAN
\citep{ester1996density} is the paradigmatic example, focus on the data-points themselves --
grouping together those that are sufficiently close (dense) -- rather than on the space between
them -- that is, the problem of choosing the set of separating planes with sufficiently-low path
integrals.
%
A classical example of a \ac{SemiSL} method derived from this principle is the Transductive Support
Vector Machine (TSVM; \citealp{joachims1999transductive}), which shares the inductive Support Vector
Machine's (SVM's) aim of maximising the margin between the decision boundary and nearest
data-points (the \emph{support vectors}) -- yielding the \emph{maximum margin hyperplane} -- yet
considers both the (labelled) training and (unlabelled) test data during fitting.
%
\sidepar{Smoothness assumption}
%
The final member of the triad, the \emph{smoothness assumption}, can be viewed as imposing a kind
of local \(K\)-Lipschitz or \(\epsilon\)-isometric constraint on our function class, \( \gF \),
local in the sense of applying only to high-density regions of the input space (whereas a global
smoothness assumption would require even sparse regions of the input space to obey the constraint);
i.e.\ for a pair of inputs \( x \) and \( x^\prime \), we have
%
\begin{equation}
  %
  | d_\gY( f(x), f(x^\prime) ) - d_\gX( x, x^\prime ) | \le \epsilon,
  %
 \end{equation}
 %
 with \(d_\gY \) and \( d_\gX \) being the metrics (distances) associated with metric spaces \( \gY
 \) (the output space) and \( \gX \) (the input space), respectively.
 %
 Plainly speaking, the assumption embodies the desire to have similar inputs map to similar outputs
 (that distance should be preserved up to some relaxation factor, hearkening back to the earlier
 discussion on \ac{IF}) and from this assumption one naturally obtains the class of
 \emph{consistency-regularised} methods. 
 %
 Equally, it should be possible to smoothly interpolate between the images of \(x\) and \( x^\prime
 \) without straying into low-density regions (which, per the LDS assumption, define separating
 planes) and with this perspective, we are granted a reformulation of the cluster assumption.
 %
 %
 More generally, we can simply determine any two samples to be similar (e.g.\ based on a
 neighbourhood graph) and then seek to minimise the distance between them in \( \gY \), thereby
 partially discretising the problem but allowing for increased flexibility.
 %
 This is capitalised on in Chapter~\ref{ch:okapi} wherein we propose a consistency-regularised
 method for \ac{DG} where pairs are determined by a cross-domain causal-matching algorithm and
 consistency is enforced between the members of those pairs.
 %
% --------------------------------------------------------------------------------
 \subsection{Causal connections: when should(n't) \acl{SemiSL} work?}\label{ssec:semisl-causality}
% --------------------------------------------------------------------------------
Following \citet{scholkopf2021toward}, start by supposing that our prediction task follows the
causal factorisation \( X \to Y\), i.e.\ it is a causal, rather than an anticausal, one.
%
As discussed in \S\ref{sec:lens-of-causality}, the ICM principle states that modules in a joint
distribution's causal decomposition do not inform or influence one another; this implies that when
\(X\) is the causal parent to \(Y\), as in the supposed case, a better estimate of \(P(X)\) does
not yield a better estimate of \(P(Y|X)\) and it is the former that \ac{SemiSL} compasses to learn
using unlabelled data.
%
However, that is not to say that \ac{SemiSL} as in its totality is misguided, it merely requires the
right condition to be met, that condition (which applies to a wide range of real-world problems)
being the contrary factorisation, \(Y \to X \), which is to say that the task under
consideration is anticausal.
%
In this case, \(X\) can contain information about the labelling mechanism, as \(X\) is now the
effect, and \(Y\) is now the cause, opening up the possibility of exploiting dependencies in the
marginal distribution to better estimate the conditional distribution; in
\citet{scholkopf2012causal} the authors corroborate this hypothesis.
%
 
% While perhaps often overlooked, this requirement, in fact, well-aligned with the motivating
% arguments for \ac{SemiSL} that I discussed at the beginning of this section.
% %
% The \emph{cluster assumption} predicates that points belonging to the same cluster in \(P(X)\)
% abide by the same labelling mechanism; 
% %
% the LDS assumption predicates that the region in which \(P(Y|X)\) is maximally entropic (defining
% the decision boundary) should have low \(P(X)\), or, by invocation of Bayes' Theorem, low \( \frac{
% P(X|Y)P(Y) }{ P(Y|X) } \); 
% %
% the \emph{smoothness assumption} predicates that if two inputs in a high-density region are close,
% then their respective images (under the model) also should be;
% %
% notice that in all three of these cases the causal factorisation is implied to be \(Y \to X \).
% %
% The celebrated co-training theorem \citep{blum1998combining} similarly respects this precondition
% of anticausality in assuming that the co-trained predictors are conditionally independent given the
% label, as one would have if the label were the cause (the causal parent in the bivariate causal
% graph).
% %

% Points from \cite{castro2020causality} to integrate into the above discussion:
% \begin{itemize}
%         \item One of the notorious challenges in medical image analysis is that of scarcity of
%           labelled data, owing to the high costs (lab tests) and expertise required  for
%           annotation.
%         \item In many cases, establishing the causal direction between inputs and prediction
%           targets is non-trivial, particularly if crucial metadata is missing.
%         \item Strong requirements need to be bet for SemiSL to be fruitful, namely, the
%           distribution of inputs needs to carry information relevant to the prediction task.
%           %
%           This idea is typically articulated in terms of specific assumptions about the data:
%           similar inputs are likely to have similar labels (smoothness assumption) and will
%           naturally group into clusters with high density in the input features space (cluster
%           assumption). Lower density regions in that space in-between clusters are assumed to be
%           ideal candidates for fitting decision boundaries.
%           %
%           In this context, considering large amounts of unlabelled data together with scarce
%           labelled data may reveal such low density regions.
%           %
%           Note how this idea insinuates and interplay between the distribution of inputs, \( P(X)
%           \), and the conditional distribution \( P(Y|X) \). 
%           %
%           Recall that by the ICM principle, if the prediction task is causal, then the marginal
%           distribution \( P(X) \) is uninformative \wrt{} \( P(Y|X) \) and SSL is theoretically
%           futile.
%           %
%           Since typical semantic segmentation tasks are causal, there is likely very little hope
%           that semantic segmentation can fundamentally benefit from unlabelled data.
%           %
%           A model trained on image-derived annotations will attempt to predict some pre-imaging
%           ground-truth. 
%           It is plausible that seeing more raw images without corresponding anatomical information
%           provides no new insight about the labelling mechanism.
%           %
%           Conversely, if \( Y \to X \), as for skin lesions, then these distributions may be
%           dependent and semi-supervision has a chance of success.
%           %
%           That is not to say that SSL is completely  useless for causal tasks; there can be
%           practical algorithmic benefits.
%           %
%           Namely, under certain conditions, unlabelled data can be shown to have a regularising
%           effect and may reduce the amount of labelled data required to achieve a given performance
%           level \citep{chapelle2009semi}
%           %
%           A recent empirical study \citep{oliver2018realistic} reported that properly-tuned
%           supervised models, either pretrained on trained on only the task-relevant data, are often
%           competitive with or outperform their semi-supervised counterparts. Also shown here was
%           the potential for SemiSL to harm performance under label shift.
%           %
%           ``
%           We find that the performance of simple baselines which do not use unlabelled data is
%           often under-reported, SSL methods differ in sensitivity to the amount of labelled and
%           unlabelled data, and performance can degrade substantially when the unlabelled dataset
%           contains out-of-distribution examples
%           %
%           \begin{itemize}
%             \item When given equal budget for tuning hyperparameters, the gap in performance
%               between using SSL and using only labelled data is smaller than typically reported.
%               %
%             \item A large classifier with carefully chosen regularization trained on a small
%               labeled dataset with no unlabelled data can reach very good accuracy.
%               %
%             \item This demonstrates the importance of evaluating different SSL algorithms on the
%               same underlying model.
%               %
%             \item In some settings, pre-training a classifier on a different labelled dataset and
%               then retraining on only labelled data from the dataset of interest can outperform all
%               SSL algorithms we studied.
%               %
%             \item Performance of SSL techniques can degrade drastically when the unlabelled data
%               con â€¢ Different approaches exhibit substantially different levels of sensitivity to
%               the amount of labelled and unlabelled data.
%           ''
%           \end{itemize}
% \end{itemize}

% % --------------------------------------------------------------------------------
% \subsection{A (brief) taxonomy of semi-supervised methods}
% % --------------------------------------------------------------------------------

