\chapter{Background}\label{ch:background}


\section{Supervised Learning}
...

\subsection{I.I.D Learning}

Traditional learning algorithms usually assume (or are only optimal for) that the training and test
samples are \emph{both} variables identically-and-independently distributed (i.i.d) random
variables, such that one has $P_{tr}}(X, Y) \approx P_{te}(X, Y)$. Based on this assumption, the
method of Empirical Risk Minimisation (ERM; \citet{vapnik1991principles}) seeks the hypothesis $f:
\gX \to \gY$ that is minimiser, $f^\ast$ of the \emph{expected risk}, \emph{risk}, $\gR$, defined s
the expectation of the loss,$\ell: \gY \times \gY \rightarrow \mathbb{R}_\+$, over the training
distribution, $P_{tr}(X, Y)$. Formally, the risk is defined by: $\allall f \in \gF$,
%
\equ\begin{equation*} \label{eq:risk}
  \gR(f) := \mathbb{E}_{P_{tr}(X, Y)}\[ \ell (f(X), Y) \]
\end{equation*}
%
Since in practice one does not have access to the true generative distribution, but only a finite
set of realizations, $\gD_{tr}$ \ref{eq:risk} requires substituting with its empirical counterpart,
the \emph{empirical risk},
$\hat{\gR}$ and uses this a proxy for the aforementioned true risk:

\equ\begin{equation*} \label{eq:risk}
  \hat{\gR(f)) := n^{-1} \sum_i=1^n \ell (f(x_i), y_i)
\end{equation*}
% where we recover the stand ard (unweighted) formulation by setting $w := \{w_i}_i=1,^n$

(see \citept{vogel2020weighted} as reference for formulating the traditional ERM setup and
that of its weighted counterpart, also \citep{wang2021importance, semenova2019study,
zhai2022understanding, idrissi2022simple} 

\subsection{Importance weighting}
\citep{wang2021importance} 


\section{Types of Distribution Shift in Classification}

See \cite{moreno2012unifying} and \cite{castro2020causality} for terminology for describing
different types of distribution shift for classification tasks. See \cite{mooij2020joint} (joint
causal inference (JCI) framework) and \cite{schrouffdiagnosing}, for example, for causal treatments
of the problem, the latter focussing on fairness under different manifestations of distribution
shift.
%
\itemi\begin{itemize}
        \item Covariate Shift
        \item Label Shift -- can be decomposed further, per \cite{castro2020causality}, into
          \item Prevalence shift
          \item Annotation shift
        \item Compound Shifts
\end{itemize}

% TODO: Construct a causal diagram based on JCI to illustrate the different problems considered in
% this thesis.

\subsection{Approaches}

See \cite{zhang2013domain} for an early example (and theoretically-rigorous presentation of) domain
adaptation under (target and conditional) distribution shift.

\section{Domain Generalisation}

Discuss independence and stability of causal mechanisms, modularity, autonomy of subsystems, etc.
\citep{scholkopf2021toward, parascandolo2018learning, peters2017elements, scholkopf2012causal} 

\textbf{Independent Causal Mechanisms (ICM) principle:}
\begin{quotation}
The causal generative process of a systemâ€™s variables is composed of autonomous modules that do not
inform or influence each other. 
%
In the probabilistic case, this means that the conditional distribution of each variable given its
causes (i.e., its mechanism) does not inform or influence the other mechanisms
\citep{scholkopf2021toward}
\end{quotation}}

\textbf{Sparse Mechanism Shift (SMS):}
\begin{quotation}
Small distribution changes tend to manifest themselves in a sparse or local way in the
causal/disentangled factorization (4), i.e., they should usually not affect all factors
simultaneously \citep{scholkopf2021toward} \end{quotation}}


For references re the general problem setup and theory see:
\citet{gulrajani2020search, krueger2021out, arjovskyinvariant, ahuja2020invariant,
weber2022certifying, rosenfeld2020risks}

For a comprehensive survey on OOD generalisation see  \citet{shen2021towards}.

\subsection{In Search of Lost Domain Generalization \citep{gulrajani2020search}}
%
Machine learning systems often fail to generalise out-of-distribution (OOD), crashing in 
spectacular way when tested outside the domain of training examples.

\itemi\begin{itemize}
  \item Self-driving car systems struggle to perform under conditions different to those of 
    training, including variations in lighting \citep{dai2018dark}, weather \citep{volk2019towards}, 
    and object poses \citep{alcorn2019strike}
  \item Systems trained on medical data collected in one hospital do not generalise to other health
    centres \citep{castro2020causality, albadawy2018deep}
  \item failing to generalise is failing to capture the causal factors of variation in data, 
    clinging instead to easier-to-fit spurious correlations, which are prone to change from 
    training to testing domains (unstable in the face of interventions)
  \item  Examples of spurious correlations (SC) commonly encountered in machine learning include 
    racial biases, texture biases \citep{geirhos2018imagenet}, and object backgrounds
    \citep{beery2018recognition} .
  \item Alas, the capricious behaviour of machine learning systems to distributional shifts is a 
    roadblock to their deployment in critical applications.
\end{itemize}

The goal of DG is OOD generalisation: learning a predictor able to perform well on some unseen test
domain when no data from the test domain is available during training -- we must assume the 
existence of some statistical invariances across training and testing domains.
DG differs from Domain Adaptation (DA) in that the latter assumes that the unlabelled data derived 
from the test domain is available during training.


See \cite{valle2018deep, geirhos2020shortcut, scimeca2021shortcut, shah2020pitfalls} for extensive discussion and
theoretical treatment of shortcut-learning/simplicity bias phenomenon.

Key findings from \cite{scimeca2021shortcut}:
\\begin{enumerate}
         \item Certain cues (conducive to the visual recognition problem at hand) are preferred to
           others
         \item Solutions biased to the easy-to-learn cues tend to converge to relatively flat minima
           on the loss surface.
         \item The solutions focusing on those preferred cues are far more abundant in parameter
           space.
         \item Solutions corresponding to Kolmogorov-simple cues are abundant in parameter space
           and thus preferred by DNNs.
          \item Devise a setup where multiple cues are equally valid for solving the task at hand;
            find that DNNs attend to choose cues in a certain order (e.g. colour is preferred to
            rotation).Simple cues, based on Kolmogorov complexity, are far more frequently
            represented in the parameter space and thus are far more likely to be adopted by DNNs.
          
 \end{enumerate}
Extended notes on \cite{scimeca2021shortcut}:
\begin{itemize}
        \item In some cases, the shortcut bias arises in models that suppress ceertain streams of
          inputs: visual question answering (VQA) models often neglect whole-image cues, for one
          does not require images to answer questions like ``what question is the banana in the
          image?'' \citep{cadene2019rubi}.
        \item Shortcut biases become problematic when it comes to generalisation to more
          challenging test-time conditions, where the shortcuts are no longer valid. These biases
          also cause ethical concerns when the shortcut features correspond to sensitive attributes
          like gender or race \citep{wang2019balanced}.
\end{itemize}


\subsection{Simplicity Bias \citep{shah2020pitfalls}}
\begin{itemize}
  \item In stationary settings where there is no mismatch between the training and test 
    distributions, generalisation is usually maximised by selecting models according to the 
    statistical equivalent of Occam's Razor, of using the simplest model that explains the data 
    well. However, when the training and test distributions are not aligned in some sense, this 
    fails to hold up where the simplest models are those that latch onto spurious statistical 
    correlations in the training data and thus are not stable under non-causal interventions in 
    the marginal or conditional distributions of $p(y|x)$ (resulting in \emph{covariate} shift 
    and \emph{concept} drift respectively).
  \item On real-world datasets there are several distinct ways to discriminate between labels (e.g. 
    based on shape, colour, texture, etc.) that are (a) predictive of the label to varying extents, 
    and (b) define decision boundaries of varying complexity. 
  \item For example, in the image-classification task of swans vs. bears, a linear-like simple 
    classifier that only looks at colour could predict correctly on most instances except white 
    polar bears, while a non-linear complex classifier that infers shape could have almost perfect 
    predictive power.
\end{itemize}

\subsection{Causality}%
\subssubsection{DRO}
\subsection{Underspecification}%
\label{sub:underspecification}

The problem of underspecification can be formalised in terms of 'Rashomon sets'
\citep{semenova2019study} defined w.r.t. the validation/test set used to guide model-selection and
indeed assess the feasibility of performing a given task with a machine learning system given the
data available. The Rashomon effect \citep{breiman2001statistical} \footnote{taking its name from
the Kurosawa film in which four witnesses recount wildly differing versions of the same crime}
describes the phenomenon in which there exists a non-singular set of equally-performing predictors
from a given function class $\gF$.
%
Large Rashomon sets often occur when the machine learning pipeline is underspecified (e.g. due to 
its failure to adequately account for distribution shift occurring at deployment time).
The empirical Rashomon set is a subset of models of the hypothesis space $\gF$ forming an equivalence class
 (according to scoring function $\phi$), with some tolerance, $\epsilon$, w.r.t. the
best model in the class, $f^\ast$. 
We generalise the formulation given in \citet{semenova2019study} to allow 
performance to be measured with respect to an arbitrary dataset,  $\gD_{eval}$, by making this a 
secondary parameter of $\phi$. This generalised form is then given as
\align\begin{align*}
  \mathfrak{R} := \{ f \in \gF: \phi(F, \gD_{eval}) \leq \phi(f^\ast, \gD_{eval}) + \epsilon}
\end{align*}

\subsection{Connection between Fairness and DG}
%
\citet{creager2021environment} cast the problem fair machine learning as one of DG,
where the protected groups takes the role of the different domains/environments. In fairness
literature, the learning objectives represent context-specific fairness notions,
while in OOD literature, the learning objectives should be designed according to invariance 
assumptions.
A number of fair representation learning methods \citep{edwards2015censoring, madras2018learning}
are derived from domain adaptation (DA) methods.
When protected attributes are unknown, DRO and adversarially learning can be applied as in 
\citet{hashimoto2018fairness} and \citet{lahoti2020fairness}, respectively, to obtain a 
distributionally robust predictor and minimizing the worst-subgroup performance; the former can
also be adapted to cases in which such information is known as in \citet{sagawa2019distributionally} 

(\citep{krueger2021out} provide a good (albeit brief) discussion of the parallels between fairness and OOD
generalization)

Recent trend in the fairness literature is to consider how fairness behaves under distribution
shift.
\begin{itemize}
  \item \cite{schrouff2022diagnosing}
  \item \cite{schrouff2022maintaining}
  \item \cite{singh2021fairness}
  \item \cite{slack2020fairness}

\end{itemize}

\subsection{Taxonomy of methods}

\section{Domain Adaptation (DA)}
\begin{enumerate}
  \item Seminal theoretical works of \cite{ben2006analysis, ben2010theory}
\end{enumerate}

\section{Adversarial Learning}
\cite{GooAbaMirXuetal14}
\subsection{Move Order and Strategic Equilibria} Since the strategies are not finitely spanned, the
minimax theorem does not hold and the idea of an ``equilibrium'' becomes tenuous.

\import{background/pc}{al.tex}


\section{ Semi-supervised Learning }\label{sec:SemiSL}
\subsection{Taxonomy of methods}

\section{ Self-supervised Learning }\label{sec:SelfSL}

\section{ Normalising Flows }\label{sec:nfs}
