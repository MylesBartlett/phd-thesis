\chapter{Background}\label{ch:background}

% ********************************************************************************
\section{Supervised Learning, the I.I.D. Assumption, and its Pitfalls}\label{sec:iid}
% ********************************************************************************

Traditional learning algorithms usually assume (or are only optimal for) that the training and test
samples are \emph{both} variables identically-and-independently distributed (\iid{}) random
variables, such that one has \( P^{tr}}(X, Y) \approx P^{te}(X, Y) \), with \( P^{tr}(X, Y) \) and
\( P(^{te}(X, Y)) \) denoting the (joint) training and test distributions respectively.
%
Based on this assumption, the method of Empirical Risk Minimisation (ERM;
\citet{vapnik1991principles}) seeks the hypothesis \( f^\ast \in \gF \) that is the minimiser of
the \emph{expected risk}, $\gR$, defined as the expectation of the loss, \( \ell: \gY \times \gY
\rightarrow \mathbb{R}_\+ \), over the training distribution. 
%
To simplify exposition, we will notation notation here and throughout this chapter by allowing
functions of the form \(f: X \to Y \) to accept random and observed variables interchangeably; we
assume that the derived function classes are Borel Measurable (a trivial precondition in practice)
and as such that a function of a random variable is also a random variable. \(f\) to operate on
random variables \(X\).
%
Thus, pedantically speaking, \( f(X) \) should be read as shorthand for \( f \circ
X(\omega) \), for some event \( \omega \) drawn from sample space, \( \Omega \), while \( f(x) \)
should be read in the standard fashion, with deterministic inputs and outputs.
%
With this in mind, we can formally define the (population or true) risk as
%
\equ\begin{equation*} \label{eq:pop_risk} 
  \gR(f) := \E_{(X, Y) \sim P^{tr}(X, Y)}\[ \ell (f(X), Y) \].
\end{equation*}
%
In practice, of course, one does not have access to the true generative distribution, but only a
finite set of realizations of it forming a \emph{dataset}, \( \gD^{tr} \), consisting of observed
input-target pairs \(x, y)\).
%
The \emph{empirical risk}, \( \hat{\gR} \), as its name suggests, simply entails substituting \(
P^{tr}(X, Y) \) with this empirical counterpart, and since the we are now operating on a finite set
of instances, rather than a distribution, the expectation can be replaced with a
finite sum (with normalisation):
%
\equ\begin{equation*} \label{eq:emp_risk} 
  \hat{\gR(f)}) \triangleq |\gD^{tr}|^{-1}  \sum_{(x, y) \in \gD^{tr}} \ell (f(x), y),
\end{equation*}
%
This defines the unweighted empirical risk but in practice, datasets often exhibit a degree of
class-imbalance, or, more generally, `long-tailedness', which is to say that the marginal
distribution \( P(Y) \) is not uniform over its domain.
%
Such motivates replacing the unweighted (or, more accurately, ` uniformly weighted') objective
given by Eq.\ref{eq:emp_risk} with an importance  \emph{weighted} variant wherein the loss is
weighted by \( \P^{tr}(Y)^{-1} \), or, in the empirical case, by the inverse frequencies of the
targets, in the discrete (classification) case, or by the empirical density of the target (as given
by kernel density estimate (KDE), for instance) in the continuous (regression) case.
%
Here, we have assumed no foreknowledge of \( P^{te}(Y) \) -- this typically being the case in
practice -- with the choice of an uninformative, uniform distribution over the domain leading to
its elimination from the importance weighting term that in general takes the form \( \frac{
  P^{te}(Y) }{ P^{tr}(Y) } \) (or the empirical equivalent).

%
Using \( w \in \R \) to denote the weight assigned to instance \(x\) in \( \gD^{tr} \), we can then
generalise Eq.~\cite{eq:emp_risk} as
%
\equ\begin{equation*} \label{eq:emp_risk_weighted}
  \hat{\gR(f)}) \triangleq \sum_{(x, y) \in \gD^{tr}} w \ell (f(x), y),
\end{equation*}
%
noting that this subsumes the unweighted variant which can be recovered by simply fixing \(w\) to
\( |\gD^{tr}|^{-1} \) for all instances.
%
It is also worth noting that these weights can be adaptive; they can be iteratively
adjusted over the course of training according to some parametric or non-parametric function
\citep{wang2021importance}.
%
Instead of weighting the instance-losses, one can instead use the weights to adjust the sampling,
which has several practical advantages when training with stochastic gradient descent (SGD),
particularly: 
%
1) The procedure is non-invasive: no modification to the data-loading nor the
computation of the loss is required;
%
2) Highly-weighted samples appear in batches commensurately often; when weighting the loss, samples
belonging to the long-tail will appear in batches rarely, resulting in forgetting and poor
diversity as said samples are effectively duplicated.
%
One can achieve a similar effect by under-sampling the majority classes, groups, or their
intersections, such that they are equifrequent, and \iid{} sampling from that subset \(
\gD^{tr}_{\text{US}} \subset \gD^{tr} \) (or, conversely by duplicating instances from the minority
classes to the same end).
%
Under- and over-sampling (US and OS, respectively) have long been used as a remedy for class imbalance
\citep{chawla2002smote} but the former has recently been shown to be effective -- matching or
exceeding in performance more sophisticated algorithms -- for group robustness and
spurious-correlation problems \citep{sagawa2020investigation, idrissi2022simple} in part due to its
early-stopping effect.

Despite its intuitiveness and its long history, with roots in early statistical modelling, the
practical usefulness of importance-weighted ERM in the context of modern deep learning has recently
been impugned \citep{byrd2019effect, zhai2022understanding}.
%
\cite{byrd2019effect} demonstrate that for \emph{over-parametrised} models the effects of
importance-weighting diminish over the course of training; these effects can be partially recovered
when used in conjunction regularisation such as dropout, early-stopping and standard \(L_2\) weight
decay but without such interventions there converged-upon solution is identical for both IW-ERM and
vanilla ERM. 
%
Evidence for this was also provided by \cite{sagawa2019distributionally}, who stress the importance
of combining aggressive regularisation with (a dynamic form of) importance-weighting for
strong worst-group generalisation.
%
These empirical results were recently supported theoretically by the \cite{zhai2022understanding},
who prove that the implicit biases of these algorithms and standard ERM are indeed equivalent.
%
Summarily, while importance-weighting may be intuitive, it provably does not alter the solution to
the optimisation problem defined by the training set, which is to say, solutions that attain
zero-loss are invariant under reweighting.
%
This understanding has motivated other approaches, such as those based on polynomially-tailed
losses (for binary classification; \cite{wang2021importance}) and logit-adjustment
\citep{menon2020long}.
%
% The view of the latter is to instead aim to shift the classification boundary to be closer to the
% dominant (majority) classes and can be realised through the use of per-class margins, either
% through modifying the loss during training or post-hoc correction \cite{fawcett1996combining} 

As statistical models are only required model correlations in the data to satisfy the loss
function, they ultimately only capture a superficial representation of the true physical processes
involved.
%
In the discriminative case (that this thesis is concerned with), for a given \( X \) and \( Y \) we
are interested in approximating the conditional distribution \( P(Y|X) \); this  corresponds to
tasks like predicting the probability that a given image contains a dog (image classification), or
the probability that a given chest X-ray indicates a pulmonary infiltration, or some other thoracic
condition.
%
Indeed, the task accurately estimating \( P(Y | X) \) can be provably solved by observing a
sufficient amount of \iid{} data drawn from the joint distribution \( P(X,Y) \), yet this only
solve the problem from the aforementioned statistical perspective, and we will see that this
perspective is not always aligned with the causal one, which can lead to problems in generalisation
under certain conditions that crop up unsettlingly often in real-world applications, including
those that safety-critical.
%
This is to say, the predictions of a statistical model should only be trusted when the conditions
of the training and test distributions are sufficiently similar, and, in short, arbitrary shifts
(interventions on the data-generating distribution) can give rise to arbitrarily bad predictions
\citep{pearl2009causality, scholkopf2012causal}.

Since the true causal relationships between independent and dependent variables is, generally, not
\emph{identifiable} given the training data alone, owing to confounding variables, additional
information, as provided by interventions, or \emph{environments} \citep{peters2016causal}, is
needed to resolve the statistical ambiguity; this the typical tack pursued within the domain
generalisation generalisation literature, where each domain can be viewed as a different
intervention on the true distribution.
%
By `confounding variable', or \emph{confounder}, we mean some variable that is that is
the causal parent of two or more other variables and explains the statistical dependency between
them despite those variables not being causally related themselves; in the trivariate case this
corresponds to the fork \(X \from S \to Y \), wherein there exists a spurious (acausal) correlation
between \(X\) and \(Y\) that is eliminated by conditioning on the confounder \(S\).
%
In the shortcut learning problems addressed in Chapters 3 and 4, we will see statistical learning
breakdown in a similar way yet for essentially the opposite reason.
%
Namely, instead of having latent variable that explains the statistical dependency \(X\) between
\(Y\) in the absence of a causal dependency, we instead of have some spurious variable, \(S\) on
which \(Y\) is strongly statistically, but not causally, dependent, with \(X \to Y \) assumed to be
the true causal mechanism.
%
We will delve more deeply into what Shortcut Learning is and how the mechanisms that give rise to
it in \ref{sec:shortcut-learning}.
%
For now, however, we will move onto discussing different types of distribution shift that
statistical learning has to contend with.

% (see \citept{vogel2020weighted} as reference for formulating the traditional ERM setup and that of
% its weighted counterpart, also see \cite{shimodaira2000improving, wang2021importance,
% semenova2019study, zhai2022understanding, idrissi2022simple} 

% ------------------------------------------------------------------------------  
\subsection{Learning under Class Imbalance/Long-tail Learning}
% ------------------------------------------------------------------------------  

Notes from and on \cite{menon2020long}:
%
\begin{itemize}
        \item Real-world classification problems typically exhibit long-tailed label distributions,
          wherein most labels are associated with only a few samples.
          %
        \item Owing to this paucity, generalisation on such labels is challenging -- a classifier
          trained on such data following such a distribution is susceptible to undesirable bias
          towards the dominant labels. 
          %
          This problem has been widely studied in the literature on learning under \emph{class
          imbalance} \citep{cardie1997improving}.
          %
        \item 
          Existing approaches to coping with class imbalance modify: 
          %
          \begin{enumerate}
            \item inputs to the model, for example by over- or under-sampling
              \citep{kubat1997addressing, chawla2002smote} \item outputs (logits) of a model, for
              example by post-hoc correction of the decision threshold \citep{fawcett1996combining}
              %
            \item internals of a model, e.g. by modifying the loss function 
          \end{enumerate}'

\end{itemize}'


% ********************************************************************************
\section{A (Brief) Taxonomy of Types of Distribution Shift}
% ********************************************************************************
% --------------------------------------------------------------------------------
\subsection{Covariate shift}{ssec:covariate-shift}
% --------------------------------------------------------------------------------

See \cite{moreno2012unifying} and \cite{castro2020causality} for terminology for describing
different types of distribution shift for classification tasks. 

% --------------------------------------------------------------------------------
\subsection{Label shift}{ssec:label-shift}
% --------------------------------------------------------------------------------
\paragraph{Prevalence (concept) shift.}
%
Under prevalence shift (for anticausal tasks), the differences between datasets relate to
differences in class frequency, the marginal distribution \wrt{} \(Y\), i.e. \( P^{tr}(Y) \neq
P^{te}(Y) \).
%
This can arise from different predispositions in the training and test populations, or from
variations in environmental factors.
%
To correct for this domain-mismatch in discriminative models, one can most simply weight the
training loss, explicitly or implicitly at the level of sampling, by \( P^{tr}(Y) \), as discussed
at length in \S~\ref{sec:iid}.
%
\paragraph{Annotation (target) shift.}

%
\paragraph{Manifestation shift (conditional).}
%
Manifesting sift refers a change in the way (anticausal) prediction targets physically manifest in
(anatomical) changes between domains, or in mathematical terms: \( P^{tr}(E | Y) \neq P^{te}(E | Y)
\).
%
As with target shift, this cannot be corrected without employing strong parametric assumptions on
the nature of the interdomain differences.
%
\paragraph{Acquisition shift (domain shift).}
%
Acquisition shift refers to a change in distribution arising due to a change in the labelling
mechanism, in how the data was acquired, typically arising due to differences in measuring
equipment, such as the type of satellite, in the context of remote sensing, or camera
trap, in the context of population-monitoring.
%
One particularly prime example of such shift can be found in the medical imaging domain, where
differences in the scanning equipment used to collect the training and test data can lead to
introducing site-specific biases that hinder generalisation and cannot be straightforwardly
rectified by means of data-preprocessing \citep{glocker2019machine}.

\subsection{Causes of distribution shift}
\paragraph{Sampling/Selection/Representational bias.}
\citep{kallus2018residual}
\paragraph{Non-stationarity}

% --------------------------------------------------------------------------------
\subsection{Compound shift}
% --------------------------------------------------------------------------------

% See \cite{mooij2020joint} (joint causal inference (JCI) framework) and \cite{schrouffdiagnosing},
% for example, for causal treatments of the problem, the latter focussing on fairness under different
% manifestations of distribution shift.
%
% ********************************************************************************
\section{Shortcut Learning}\label{shortcut-learning}
% ********************************************************************************

Shortcut-learning is learning variance to the wrong thing and, consequentially, invariance to the
right things.
%
Less pithily, a shortcut solution is robust enough to achieve near-zero loss on the training set,
then there is little incentive for the model to learn alternative `views' (collections of features;
\cite{allen2020towards}).
%
For instance, if texture is a reliable classification cue given the training data
\cite{geirhos2018imagenet}, a model can latch onto that cue and ignore (be invariant to) other
higher-level semantics, like shape and global structure, that humans judgements are much more
strongly attributed.
%
High-frequency cues, such as colour and texture, are readily modulated by (unstable under) changes
in lighting, for instance, making them less reliably cues for object classification in a dynamic
environment; we are not wont, for example, to classify an object in the shape of a cat as an
elephant simply because the texture of the latter has been transplanted, \emph{ceteris paribus}, to
the former, a failure mode (in-)famously shown by \cite{geirhos2018imagenet} to apply to DNNs
trained on ImageNet.

See \cite{valle2018deep, geirhos2020shortcut, scimeca2021shortcut, shah2020pitfalls} for extensive discussion and
theoretical treatment of shortcut-learning/simplicity bias phenomenon.

Key findings from \cite{scimeca2021shortcut}:
\\begin{enumerate}
         \item Certain cues (conducive to the visual recognition problem at hand) are preferred to
           others
         \item Solutions biased to the easy-to-learn cues tend to converge to relatively flat minima
           on the loss surface.
         \item The solutions focusing on those preferred cues are far more abundant in parameter
           space.
         \item Solutions corresponding to Kolmogorov-simple cues are abundant in parameter space
           and thus preferred by DNNs.
          \item Devise a setup where multiple cues are equally valid for solving the task at hand;
            find that DNNs attend to choose cues in a certain order (e.g. colour is preferred to
            rotation).Simple cues, based on Kolmogorov complexity, are far more frequently
            represented in the parameter space and thus are far more likely to be adopted by DNNs.
          
 \end{enumerate}
Extended notes on \cite{scimeca2021shortcut}:
\begin{itemize}
        \item In some cases, the shortcut bias arises in models that suppress certain streams of
          inputs: visual question answering (VQA) models often neglect whole-image cues, for one
          does not require images to answer questions like ``what question is the banana in the
          image?'' \citep{cadene2019rubi}.
        \item Shortcut biases become problematic when it comes to generalisation to more
          challenging test-time conditions, where the shortcuts are no longer valid. These biases
          also cause ethical concerns when the shortcut features correspond to sensitive attributes
          like gender or race \citep{wang2019balanced}.
\end{itemize}

Notes on and from \cite{shah2020pitfalls}:
\begin{itemize}
        \item Well-studied approaches such as for improving group- and adversarial-robustness --
          such as ensembling and adversarial training -- do not mitigate the problem of simplicity
          bias on the proposed datasets.
          %
          The failure of ensembling is to be expected, as there if the simplicity bias is
          sufficiently strong, there is little-to-no incentive for the ensemble members to learn
          diverse views of the data \citep{allen2020towards} due to stochasticity in the weights
          and optimisation procedure, this diversity being where the presumed origin of the
          improved-robustness demonstrated by (deep) ensembles.
          %
          The failure of adversarial training is also to be expected given that the perturbations
          do nothing to intervene on the spurious feature directly.
          %
\end{itemize}

Notes on and from \cite{geirhos2020shortcut}:
\begin{itemize}
  \item
\end{itemize}

% --------------------------------------------------------------------------------
\subsection{Simplicity Bias \citep{valle2018deep, shah2020pitfalls}}
% --------------------------------------------------------------------------------
\begin{itemize}
  \item In stationary settings where there is no mismatch between the training and test 
    distributions, generalisation is usually maximised by selecting models according to the 
    statistical equivalent of Occam's Razor, of using the simplest model that explains the data 
    well. However, when the training and test distributions are not aligned in some sense, this 
    fails to hold up where the simplest models are those that latch onto spurious statistical 
    correlations in the training data and thus are not stable under non-causal interventions in 
    the marginal or conditional distributions of $p(Y|X)$ (resulting in \emph{covariate} shift 
    and \emph{concept} drift respectively).
    %
  \item On real-world datasets there are several distinct ways to discriminate between labels (e.g. 
    based on shape, colour, texture, etc.) that are (a) predictive of the label to varying extents, 
    and (b) define decision boundaries of varying complexity. 
    %
  \item For example, in the image-classification task of swans vs. bears, a linear-like simple 
    classifier that only looks at colour could predict correctly on most instances except white 
    polar bears, while a non-linear complex classifier that infers shape could have almost perfect 
    predictive power.
\end{itemize}


% ********************************************************************************
\section{Through the Lens of Causality}\label{sec:lens-of-causality}
% ********************************************************************************

We now introduce a causal formalism of the distribution-shift problem, a formalism which has been
frequently exploited in the DG and FairML literature as it provides a simple calculus with which
to reason about desired (and undesired) variances.
%
It should be noted in advance that we only draw upon this formalism in order to provide a unified
formulation of the distribution-shift problems considered in this thesis; we do not operate on the
domain of causal graphs nor attempt to perform causal inference. 
%
The background on causality is thus commensurably light and we refer the reader
to \cite{pearl2009causality} for full exposition of the topic.

While the term `domain' typically refers to the observed distributions as a whole in both DA and DG
alike (i.e. `source' versus 'target'), such terminology is somewhat rigid, as it fails to capture
that the distributions share an underlying structure and how and which variables are shifted.
%
It is more arguably flexible then, consistent with \citep{mooij2020joint}, to think of the domain
as some exogenous latent variable, which, by its conditioning, gives rise to the different observed
distributions -- or subgraphs in the discrete case -- and explains how one is transformed
(`shifted') into the other.
%
We will denote said variable as \(E\) (for `\bf{E}nvironment', as it is commonly termed in the DG
literature \citep{arjovskyinvariant}), which need satisfy only the loose requirement that it belong
to some Borel space (and thus may in theory be continuous or discrete).
%
Most simply, in the case of DA, \(E\) is simply a binary random variable, such that we have \(E:
\Omega \to \{ \text{source}, \text{target} \}\), with \( \Omega \) being the sample space.
%
We view then view variables in our prediction task as constituting the node \gV \) in a Causal
Bayesian Network (CBN) where the direction of arrows (directed edges) between nodes indicate the
direction of causality (e.g. \(\rmA \to \rmB \) means that \bf{A} causes (is a parent of) \bf{B})
while the absence of an edge between two nodes \textbf{A} and \textbf{B} indicates independence
between them when conditioned on their parents, i.e. \( \rmA | \text{Pa}(\rmA) \perp \rmB |
\text{Pa}(\rmB) \), where \( \text{Pa}(\cdot) \) denotes the causal parents of its argument node.
%
Formally, a CBG is a kind of Directed Acyclic Graph (DAG), \(\rmG \triangleq \langle \gV, \xi
\rangle \) with node-set (variables), \(\gV\), and (directed) edge-set, \(\xi\) consisting of tuples
\((ij)\) meaning \(i \to j \), or `node \(i\) is a parent of node \(j\)'.
%
Each node in \rmG then defines a probability distribution, conditional on its parents, such that
the joint distribution of \(\gV\), \(P(V)\), factorises as \( P(V) = \prod_{v \in \gV} P(v |
\text{Pa}(v)) \) where we can now define \(\text{Pa}(\cdot)\) as a function that returns all nodes
in \(\xi\) that form a pair with \(v\) as the second element, i.e. \( \{ i | i,j \in \xi, j = v \}
\).
%
Without loss of generality, for the prediction task with inputs, \(X\), and targets, \(Y\), we may
introduce the aforementioned variable \(E\) to convert the joint distribution \(P(X, Y)\) into the
conditional joint distribution \( P(X, Y) | E \); the structure of the underlying CBN determines
the factorisation of this distribution and thus the nature of the distribution shift in question.
%
One can, for example characterise the case of covariate-shift with causal \(f^\star\), as having
edges  \(E \to X\) and \( X \to Y \), giving rise to the factorisation \( \P(X, Y | E) | P(E) =
P(Y|X)P(X|E)P(E) \). 
%
In Chapters 3 and 4 we go beyond the bivariate (excepting \( E \)) and covariate case and consider
label-shift problems with an additional auxiliary label \(S\) -- corresponding to an identifier of
some subgroup or spurious feature we wish to be invariant to in the name of fairness or
generalisation --  in which \(E\) influences the joint distribution \( P(S, Y) \) but not the
marginal distribution \(P(X)\), giving rise to representation bias and, from it, spurious
correlations.
%
We illustrate in Fig.~\ref{fig:ds_cbgs} CBGs corresponding to different distribution shifts for a
causal prediction task.
%

\import{background}{dist_shift_cbgs.tex}

%
We will see that such a formulation is particularly utile when we come to discuss domain
generalisation which involves a multitude of source distributions, going beyond the dyadic
source-target setup characterising DA.

% ------------------------------------------------------------------------------
Points to incorporate from, and inspired from, \cite{scholkopf2021toward}:
%
/\begin{itemize}
  \item 
    The Independent Causal Mechanisms (ICM) principle states, in short, that the generative process
    giving rise to a system's observed variables is governed by \emph{autonomous} modules that do
    not inform (have zero mutual information \wrt{}) or influence each other.
    %
    In the probabilistic case, this means that the conditional distribution of each variable, given
    its causes (parents), does not inform or influence other mechanisms.
    %
    Applied to casual factorisation, the principle dictates that the factors should be independent
    in the sense that 
    %
    \begin{enumerate}
      \item Changing (intervening on) one mechanism in the system (CBN), \(P(i, \text{Pa}(i))\)
        does not change any of the other mechanisms in the same system, \(P(j, \text{Pa}(j))\),
        \(\forall j \neq i \).
      %
      \item Knowing information about \(P(i, \text{Pa}(i))\) does not confer us additional
        knowledge about \(P(j, \text{Pa}(j))\), i.e. \( \forall (i \neq j): \gI(i; j) = 0 \).
    \end{enumerate}
    %
    The notion of invariant, autonomous, and independent mechanisms has appeared in many guises
    throughout the history of causality research.
    %
    The \emph{invariance criterion} of Herb Simon states that the true causal order is the one that
    is invariant under the right sort of intervention.
    %
    Pearl avers that a causal mechanism remains invariant when other mechanisms are subjected to
    external (exogenous influence).
    %
    One may then derive from the tenets of the ICM principle, the Sparse Mechanism Shift (SMS)
    hypothesis, which simply postulates that small distribution shifts tend to manifest themselves
    in a sparse of local way in a causal factorisation (i.e. not all factors should be affected
    simultaneously).
    %
    A factorisation, on the other hand, that does not exhibit this behaviour can be said to be
    \emph{entangled}.

  \item 
    According to the ICM principle, independence of two mechanisms should mean  that the two
    conditional distributions do not inform or influence one another.
    %
    Note that this does not necessarily align with the notion of statistical dependency.
  \item 
    Causal structure captures the physical mechanisms that generate statistical dependencies in
    the first place.
    %
    Statistical structure is an epiphenomenon that follows if we make unexplained variables random.
    %
    By virtue of modularity, a world model based on causally factorised latents is maximally
    compressive in general, and by this we mean it provides the simplest explanation to the complex
    physical world. 
    %
    It is much more efficient, for example, to explain changes in the shape of an object as one
    moves around it by a change in vantage point, in terms of global symmetry, rather than by
    independent changes in local structure, as would be explained by an \emph{entangled} model.
    %
    The connection between causal representation learning and modern physics thus becomes clear
    when one thinks as modularity being synonymous with invariances, or \emph{symmetries}, with it
    being little exaggeration to say that modern physics is the study of \emph{symmetries} and the
    corresponding conserved quantities, as predicated by Noether's celebrated theorem
    \citep{noether1918invariante}.
    %
    In light of this, the statistically- and causally-driven approaches are manifestly at odds with
    one another in the context of shortcut learning -- shortcut features provide the simplest,
    loss-minimising solution based on the training distribution, \(P^{tr}(X, Y)\), however they do
    not provide the simplest solution in the causal sense as evidenced by the lack of
    generalisability.
    %
    Models that are faithful to the underlying causal structure of the problem are much more robust
    to distributional changes and are more protean because the learned modules can be reconfigured
    arbitrarily according to the problem at hand and can be updated locally to incorporate new
    information without degrading modules adapted for other, unrelated tasks.
    %
    \item

 \end{itemize}


One common \citep{arjovskyinvariant, krueger2021out, sagawa2019distributionally} and intuitive way
of formulating the robust-prediction problem is as one of bilevel optimisation, where the inner
loop entails computing the empirical risk over each domain and the outer loop corresponds to
finding the function that minimises the maximum of said risks.
%
We can then define, accordingly, the optimal predictor as 
%
\begin{align}\label{eq:rob-erm-obj}
 f^\ast_\text{ robust } =
 \underset{ f \in \gF }{ \text{\argmin} } 
 \underset{ e \in \gE} { \text{max} }
 \E_{ P^{tr}(X, Y | E = e) } \[ \ell ( f( X ), Y ) \].
\end{align}
%
As discussed at length before, domains/environments can be modelled as deriving from different
interventions of the causal factorisation of \(P(X, Y\)).
%
It follows that, for Eq.\ref{eq:rob-erm-obj} to lead to successful generalisation to arbitrary
domains, \(\gE^\dagger\) outside \(\gE\) (\(\gE^\dagger \cap \gE = \varrnothing \); i.e. the goal
of DG), \(\gE\) must be a representative (well-covering) set of samples from the generating
distribution \(\P(E)\) such that smooth interpolation along the underlying manifold is possible.
%
When \(\gE\) is a finite set, as above, one can think of it as \emph{perturbation set}, accordant
with the robust optimisation literature \citep{ben2009robust} .
%
While generalisation to arbitrary perturbations is provably hard (or impossible), in general
\citep{david2010impossibility}, when \(\gE\) encodes prior information about the kinds of
perturbations one expects to encounter at test-time then incorporating it into the optimisation
process can be fruitful, both for allowing interpolation within the convex hull defined by the set
and to an extrapolated region outside of it \citep{krueger2021out}.
%
Indeed, it stands to reason that by allowing the model to glean which features are and aren't
stable across environments would allow it to better approximate the true causal structure of the
prediction task. 
%
This idea has been explored extensively in recent years in both the causal discovery
\citep{peters2016causal, bengio2019meta} and domain generalisation \citep{arjovskyinvariant,
ahuja2020invariant, creager2021environment} literature, with the caveat that a degree of inductive
bias or additional information is necessary to provably identify the proper causal relations based
on it \citep{lin2022zin}.
%

% ********************************************************************************
\section{Underspecification}\label{sub:underspecification}
% ********************************************************************************
 
The problem of underspecification can be formalised in terms of \emph{Raashomon sets}
\citep{semenova2019study} defined \wrt{} the validation/test set used to guide model-selection and
indeed assess the feasibility of performing a given task with a machine learning system given the
data available. The Rashomon effect \citep{breiman2001statistical} \footnote{taking its name from
the Kurosawa film in which four witnesses recount wildly differing versions of the same crime}
describes the phenomenon in which there exists a non-singular set of equally-performing predictors
from a given function class $\gF$.
%
Large Rashomon sets often occur when the machine learning pipeline is underspecified (e.g. due to 
its failure to adequately account for distribution shift occurring at deployment time).
The empirical Rashomon set is a subset of models of the hypothesis space $\gF$ forming an equivalence class
 (according to scoring function $\phi$), with some tolerance, $\epsilon$, w.r.t. the
best model in the class, $f^\ast$. 
We generalise the formulation given in \citet{semenova2019study} to allow 
performance to be measured with respect to an arbitrary dataset,  $\gD_{eval}$, by making this a 
secondary parameter of $\phi$. This generalised form is then given as
%
\align\begin{align*}
  \gR \triangleq 
  % lhs
  \{ f \in \gF: \phi(F, \gD_{eval}) 
  \leq 
  % rhs
  \phi(f^\ast, \gD_{eval}) + \epsilon}
\end{align*}

% ********************************************************************************
\section{Domain Adaptation (DA)}\label{sec:domain-adaptation}
% ********************************************************************************

Domain adaptation is a subfield of machine learning that deals with the problem of adapting a model
trained on one distribution (the \emph{source domain}) to a different but related distribution (the
\emph{target domain}), in such a way that the relevant shared structure is exploited while nuisance
factors that are domain-specific (and not relevant to the prediction task) ignored.
%
The downstream performance of the  model is thus naturally dependent on both the performance on the
source domain and by the degree of relatedness between the source and target domains.
%
To proffer a real-world example, in building a spam detector, one might have annotated data
(emails) available for training a model sourced from a previous group of users and wish to deploy
(adapt) the detector to a new group of users in such a way that is robust to the temporal
distribution shift.
%
In the classical DA setting, one assumes the distribution shift is \emph{covariate}
\cite{david2010impossibility} in nature, that is, localised to the marginal distribution
\(P(X)\), with both the conditional, \(P(Y|X)\) (corresponding to changes in the ground-truth
labelling mechanism, \(f^\star: \X \to \Y \)), and label, \(P(Y)\), distributions consistent across
domains.
%
There is not to say that there is not a substantial body of work that addresses other types
of distribution shift, however\citep{zhao2019learning}, and the covariate-shift assumption is
perhaps stricter than one might initially presume as it assumes.
%
Indeed, it turns out that the covariate-shift assumption is only tenable in  where \(f^\star\) is
\emph{causal} (\(X \to Y\)); practically, there are many cases in wich the converse is in fact
true, that the relationship between \(X\) and \(Y\) is \emph{anticausal} (\(Y \to X\)).
%
Anticausal prediction tasks naturally arise in the medical-imaging domain for instance, where \(Y\)
is some gold-standard indicator of the presence of the disease and it is the disease that gives
rise to aberrations in the input images signalling to a classifier a positive instance. 
%
For the task of melanoma-prediction, for example, one may be interested in training a classifier to
diagnose patients based only on dermoscopic images using labels derived from (expensive and
time-consuming but reliable) histopathological analysis \citep{castro2020causality}.
%
The distinction between causal and anticausal tasks is an important one in ML generally, and we
will revisit the idea on several occasions throughout the remainder of this chapter; for SemiSL
said distinction is particularly important as the efficacy of the paradigm hinges on \(P(X\)
carrying information about \(f^\ast\), and thus the task being an anticausal one.
%

\subsection{Taxonomy of methods}
%
See \cite{zhang2013domain} for an early example (and theoretically-rigorous presentation of) domain
adaptation under (target and conditional) distribution shift.
%
% ********************************************************************************
\section{Domain Generalisation}\label{sec:domain_generalisation}
% ********************************************************************************
%
Discuss independence and stability of causal mechanisms, modularity, autonomy of subsystems, etc.
\citep{scholkopf2021toward, parascandolo2018learning, peters2017elements, scholkopf2012causal} 

\textbf{Independent Causal Mechanisms (ICM) principle:}
\begin{quotation}
The causal generative process of a systemâ€™s variables is composed of autonomous modules that do not
inform or influence each other. 
%
In the probabilistic case, this means that the conditional distribution of each variable given its
causes (i.e., its mechanism) does not inform or influence the other mechanisms
\citep{scholkopf2021toward}
\end{quotation}}

\textbf{Sparse Mechanism Shift (SMS):}
\begin{quotation}
Small distribution changes tend to manifest themselves in a sparse or local way in the
causal/disentangled factorization (4), i.e., they should usually not affect all factors
simultaneously \citep{scholkopf2021toward} 
\end{quotation}}


For references re the general problem setup and theory see:
\citet{gulrajani2020search, krueger2021out, arjovskyinvariant, ahuja2020invariant,
weber2022certifying, rosenfeld2020risks}

For a comprehensive survey on OOD generalisation see  \citet{shen2021towards}.

\subsection{In Search of Lost Domain Generalization \citep{gulrajani2020search}}
%
Machine learning systems often fail to generalise out-of-distribution (OOD), crashing in 
spectacular way when tested outside the domain of training examples.

\itemi\begin{itemize}
  \item Self-driving car systems struggle to perform under conditions different to those of 
    training, including variations in lighting \citep{dai2018dark}, weather \citep{volk2019towards}, 
    and object poses \citep{alcorn2019strike}
  \item Systems trained on medical data collected in one hospital do not generalise to other health
    centres \citep{castro2020causality, albadawy2018deep}
  \item failing to generalise is failing to capture the causal factors of variation in data, 
    clinging instead to easier-to-fit spurious correlations, which are prone to change from 
    training to testing domains (unstable in the face of interventions)
  \item  Examples of spurious correlations (SC) commonly encountered in machine learning include 
    racial biases, texture biases \citep{geirhos2018imagenet}, and object backgrounds
    \citep{beery2018recognition} .
  \item Alas, the capricious behaviour of machine learning systems to distributional shifts is a 
    roadblock to their deployment in critical applications.
\end{itemize}

The goal of DG is OOD generalisation: learning a predictor able to perform well on some unseen test
domain when no data from the test domain is available during training -- we must assume the 
existence of some statistical invariances across training and testing domains.
DG differs from Domain Adaptation (DA) in that the latter assumes that the unlabelled data derived 
from the test domain is available during training.


% --------------------------------------------------------------------------------
\subsection{Connection between Fairness and DG}\label{ssec:fairml-dg-cxn}
% --------------------------------------------------------------------------------
\citet{creager2021environment} cast the problem fair machine learning as one of DG,
where the protected groups takes the role of the different domains/environments. In fairness
literature, the learning objectives represent context-specific fairness notions,
while in OOD literature, the learning objectives should be designed according to invariance 
assumptions.
A number of fair representation learning methods \citep{edwards2015censoring, madras2018learning}
are derived from domain adaptation (DA) methods.
When protected attributes are unknown, DRO and adversarially learning can be applied as in 
\citet{hashimoto2018fairness} and \citet{lahoti2020fairness}, respectively, to obtain a 
distributionally robust predictor and minimizing the worst-subgroup performance; the former can
also be adapted to cases in which such information is known as in \citet{sagawa2019distributionally} 

(\citep{krueger2021out} provide a good (albeit brief) discussion of the parallels between fairness and OOD
generalization)

Recent trend in the fairness literature is to consider how fairness behaves under distribution
shift.
%
\begin{itemize}
  \item \cite{schrouff2022diagnosing}
  \item \cite{schrouff2022maintaining}
  \item \cite{singh2021fairness}
  \item \cite{slack2020fairness}

\end{itemize}


% --------------------------------------------------------------------------------
\subsection{A (brief) taxonomy of domain-generalisation methods}
% --------------------------------------------------------------------------------
%

% ********************************************************************************
\section{Adversarial Learning}\label{sec:adv-learning}
% ********************************************************************************
\cite{GooAbaMirXuetal14}
% --------------------------------------------------------------------------------
\subsection{Move Order and Strategic Equilibria} 
% --------------------------------------------------------------------------------
Since the strategies are not finitely spanned, the
minimax theorem does not hold and the very idea of an ``equilibrium'' becomes tenuous.

% \import{background/pc}{al.tex}

% ********************************************************************************
\section{ Semi-supervised Learning }\label{sec:SemiSL}
% ********************************************************************************
 
Given that this thesis references semi-supervised learning (SemiSL)
\footnote{
Since self-supervised learning also features prominently in this thesis (primarily in Chapter 5) we
must depart from the typical initialism, SSL, so as to be able to differentiate the two learning
paradigms.
}
in its title, it is only appropriate that a part of this background section be devoted to the
topic.
%
However, we note that the methods introduced in this thesis are not tailored for the typical SemiSL
regime wherein one hopes to draw upon a large corpus of unlabelled data to shore up the paucity of
annotated data available for direct supervision, with the assumption being that the unlabelled and
labelled data are drawn from the same distribution.
%
Rather, the theme across the constituent papers is how one can use unlabelled data to redress
representational bias and failures in out-of-distribution generalisation. 
%
The problem setups considered thus more closely align with those found in DG and DA; we have
afforded more attention to those topics for this reason.

We would refer the reader to \cite{chapelle2009semi} for excellent (in both clarity and depth)
exposition of theory underpinning SemiSL learning and methods in the pre-deep-learning era, a book
we will reference extensively throughout this brief overview of the topic.
%
For a for comprehensive and current survey of SemiSL methods in the post-deep-learning, on the
other hand, we refer the reader to \cite{yang2022survey}.

%
References to consider:
\cite{chapelle2009semi, scholkopf2021toward, lienen2021credal, sohn2020fixmatch, tarvainen2017mean,
blum1998combining, gong2021alphamatch}
%

% --------------------------------------------------------------------------------
\subsection{Cluster assumption.}\label{ssec:cluster-assumption}
% --------------------------------------------------------------------------------
%
% --------------------------------------------------------------------------------
\subsection{Low-density-separation assumption.}\label{ssec:lds-assumption}
% --------------------------------------------------------------------------------
%
% --------------------------------------------------------------------------------
\subsection{Smoothness assumption.}\label{ssec:smoothness-assumption}
% --------------------------------------------------------------------------------
% --------------------------------------------------------------------------------
\subsection{
  Causal connections: when should/shouldn't SemiSL work?
}\label{ssec:semisl-causality}

Following \cite{scholkopf2021toward}, start by supposing that our prediction task follows the
causal factorisation \( X \to Y\), i.e. it is a causal, rather than an anticausal, one.
%
As discussed before, the ICM principle states that modules in a joint distribution's causal
decomposition do not inform or influence one another, i.e. \(\gI(X, Y) = 0 \); this implies that in
the when \(X\) is the causal parent to \(Y\), as in the supposed case, a better estimate of
\(P(X)\) does not engender a better estimate of \(P(Y|X)\) and it is the former that SemiSL
compasses to learn using unlabelled data.
%
However, that is not to say that SemiSL as in its totality is misguided, it merely requires the
right condition to be met, that condition (which applies to a wide-range of real-world problems)
being the contrary factorisation, \(Y \to X \), which is to say that that the task under
consideration is anticausal.
%
In this case, \(X\) can contain information about the labelling mechanism, as \(X\) is now the
effect, and \(Y\) is now the cause opening up the possibility of exploiting dependencies in the
marginal distribution to better estimate the conditional distribution; in
\cite{scholkopf2012causal} the authors test corroborate this hypothesis.
%
While perhaps often overlooked, this requirement, in fact, well-aligned with the motivating
arguments for SemiSL that we discussed at the beginning of this section.
%
The \emph{cluster assumption} predicates that points belonging to the same cluster in \(P(X)\)
abide by the same labelling mechanism; 
%
the \emph{low-density} assumption predicates that the region in which \(P(Y|X)\) is maximally
entropic (defining the decision boundary) should have low \(P(X)\), or, by invocation of Bayes'
Theorem, low \( \frac{ P(X|Y)P(Y) }{ P(Y|X) } \); 
%
the \emph{smoothness assumption} predicates that if two inputs in a high-density region are close,
then their respective images (under the model) also should be -- this can be viewed as imposing a
kind of \(K\)-Lipschitz or \(\epsilon\)-isometric constraint on our function class;
%
notice that in all three of these cases the causal factorisation is implied to be \(Y \to X \).
%
The celebrated co-training theorem \citep{blum1998combining} similarly respects this precondition
of anticausality in assuming that the co-trained predictors are conditionally independent given the
label, as one have if the label were the cause (the causal parent in the bivariate causal graph).
%
Points from \cite{castro2020causality} to integrate into the above discussion:
\begin{itemize}
        \item One of the notorious challenges in medical image analysis is that of scarcity of
          labelled data, owing to the high costs (lab tests) and expertise required  for
          annotation.
        \item In many cases, establishing the causal direction between inputs and prediction
          targets is non-trivial, particularly if crucial metadata is missing.
        \item Strong requirements need to be bet for SemiSL to be fruitful, namely, the
          distribution of inputs needs to carry information relevant to the prediction task.
          %
          This idea is typically articulated in terms fo specific assumptions about the data:
          similar inputs are likely to have similar labels (smoothness assumption) and will
          naturally group into clusters with high density in the input features space (cluster
          assumption). Lower density regions in that space in-between clusters are assumed to be
          ideal candidates for fitting decision boundaries.
          %
          In this context, considering large amounts of unlabelled data together with scarce
          labelled data may reveal such low density regions.
          %
          Note how this idea insinuates and interplay between the distribution of inputs, \( P(X)
          \), and the conditional distribution \( P(Y|X) \). 
          %
          Recall that by the ICM principle, if the prediction task is causal, then the marginal
          distribution \( P(X) \) is uninformative \wrt{} \( P(Y|X) \) and SSL is theoretically
          futile.
          %
          Since typical semantic segmentation tasks are causal, there is likely very little hope
          that semantic segmentation can fundamentally benefit from unlabelled data.
          %
          A model trained on image-derived annotations will attempt to predict some pre-imaging
          ground-truth. 
          It is plausible that seeing more raw images without corresponding anatomical information
          provides no new insight about the labelling mechanism.
          %
          Conversely, if \( Y \to X \), as for skin lesions, then these distributions may be
          dependent and semi-supervision has a chance of success.
          %
          That is not to say that SSL is completely  useless for causal tasks; there can be
          practical algorithmic benefits.
          %
          Namely, under certain conditions, unlabelled data can be shown to have a regularising
          effect and may reduce the amount of labelled data required to achieve a given performance
          level \citep{chapelle2009semi}
          %
          A recent empirical study \citep{oliver2018realistic} reported that properly-tuned
          supervised models, either pretrained on trained on only the task-relevant data, are often
          competitive with or outperform their semi-supervised counterparts. Also shown here was
          the potential for SemiSL to harm performance under label shift.
          %
          ``
          We find that the performance of simple baselines which do not use unlabelled data is
          often under-reported, SSL methods differ in sensitivity to the amount of labelled and
          unlabelled data, and performance can degrade substantially when the unlabelled dataset
          contains out-of-distribution examples
          %
          \begin{itemize}
            \item When given equal budget for tuning hyperparameters, the gap in performance
              between using SSL and using only labelled data is smaller than typically reported.
              %
            \item A large classifier with carefully chosen regularization trained on a small
              labeled dataset with no unlabelled data can reach very good accuracy.
              %
            \item This demonstrates the importance of evaluating different SSL algorithms on the
              same underlying model.
              %
            \item In some settings, pre-training a classifier on a different labelled dataset and
              then retraining on only labelled data from the dataset of interest can outperform all
              SSL algorithms we studied.
              %
            \item Performance of SSL techniques can degrade drastically when the unlabelled data
              con â€¢ Different approaches exhibit substantially different levels of sensitivity to
              the amount of labelled and unlabelled data.
          ''
\end{itemize}

% --------------------------------------------------------------------------------

% --------------------------------------------------------------------------------
\subsection{A (brief) taxonomy of semi-supervised methods}
% --------------------------------------------------------------------------------

% ********************************************************************************
\section{ Self-supervised Learning }\label{sec:SelfSL}
% ********************************************************************************

% ********************************************************************************
\section{ Normalising Flows and Invertible Neural Networks }\label{sec:nfs_and_inns}
% ********************************************************************************
