\chapter{Background}\label{ch:background}

% ********************************************************************************
\section{Supervised Learning, the I.I.D. Assumption, and its Pitfalls}\label{sec:iid}
% ********************************************************************************

Traditional learning algorithms usually assume (or are only optimal for) that the training and test
samples are \emph{both} variables identically-and-independently distributed (\iid{}) random
variables, such that one has \( P^{tr}}(X, Y) \approx P^{te}(X, Y) \), with \( P^{tr}(X, Y) \) and
\( P(^{te}(X, Y)) \) denoting the (joint) training and test distributions respectively.
%
Based on this assumption, the method of Empirical Risk Minimisation (ERM;
\citet{vapnik1991principles}) seeks the hypothesis \( f^\ast \in \gF \) that is the minimiser of
the \emph{expected risk}, $\gR$, defined as the expectation of the loss, \( \ell: \gY \times \gY
\rightarrow \mathbb{R}_\+ \), over the training distribution. 
%
To simplify exposition, we will notation notation here and throughout this chapter by allowing
functions of the form \(f: X \to Y \) to accept random and observed variables interchangeably; we
assume that the derived function classes are Borel Measurable (a trivial precondition in practice)
and as such that a function of a random variable is also a random variable. \(f\) to operate on
random variables \(X\).
%
Thus, pedantically speaking, \( f(X) \) should be read as shorthand for \( f \circ
X(\omega) \), for some event \( \omega \) drawn from sample space, \( \Omega \), while \( f(x) \)
should be read in the standard fashion, with deterministic inputs and outputs.
%
With this in mind, we can formally define the (population or true) risk as
%
\equ\begin{equation*} \label{eq:pop_risk} 
  \gR(f) := \E_{(X, Y) \sim P^{tr}(X, Y)}\[ \ell (f(X), Y) \].
\end{equation*}
%
In practice, of course, one does not have access to the true generative distribution, but only a
finite set of realizations of it forming a \emph{dataset}, \( \gD^{tr} \), consisting of observed
input-target pairs \(x, y)\).
%
The \emph{empirical risk}, \( \hat{\gR} \), as its name suggests, simply entails substituting \(
P^{tr}(X, Y) \) with this empirical counterpart, and since the we are now operating on a finite set
of instances, rather than a distribution, the expectation can be replaced with a
finite sum (with normalisation):
%
\equ\begin{equation*} \label{eq:emp_risk} 
  \hat{\gR(f)}) \triangleq |\gD^{tr}|^{-1}  \sum_{(x, y) \in \gD^{tr}} \ell (f(x), y),
\end{equation*}
%
This defines the unweighted empirical risk but in practice, datasets often exhibit a degree of
class-imbalance, or, more generally, `long-tailedness', which is to say that the marginal
distribution \( P(Y) \) is not uniform over its domain.
%
Such motivates replacing the unweighted (or, more accurately, `uniformly weighted') objective
given by Eq.\ref{eq:emp_risk} with an importance  \emph{weighted} variant wherein the loss is
weighted by \( \P^{tr}(Y)^{-1} \), or, in the empirical case, by the inverse frequencies of the
targets, in the discrete (classification) case, or by the empirical density of the target (as given
by kernel density estimate (KDE), for instance) in the continuous (regression) case.
%
Here, we have assumed no foreknowledge of \( P^{te}(Y) \) -- this typically being the case in
practice -- with the choice of an uninformative, uniform distribution over the domain leading to
its elimination from the importance weighting term that in general takes the form \( \frac{
  P^{te}(Y) }{ P^{tr}(Y) } \) (or the empirical equivalent).

%
Using \( w \in \R^+ \) to denote the weight assigned to instance \(x\) in \( \gD^{tr} \), we can
then generalise Eq.~\cite{eq:emp_risk} as
%
\equ\begin{equation*} \label{eq:emp_risk_weighted}
  \hat{\gR(f)}) \triangleq \sum_{(x, y) \in \gD^{tr}} w \ell (f(x), y),
\end{equation*}
%
noting that this subsumes the unweighted variant which can be recovered by simply fixing \(w\) to
\( |\gD^{tr}|^{-1} \) for all instances.
%
It is also worth noting that these weights can be adaptive; they can be iteratively
adjusted over the course of training according to some parametric or non-parametric function
\citep{wang2021importance}.
%
Instead of weighting the instance-losses, one can instead use the weights to adjust the sampling,
which has several practical advantages when training with stochastic gradient descent (SGD),
particularly: 
%
1) The procedure is non-invasive: no modification to the data-loading nor the
computation of the loss is required;
%
2) Highly-weighted samples appear in batches commensurately often; when weighting the loss, samples
belonging to the long-tail will appear in batches rarely, resulting in forgetting and poor
diversity as said samples are effectively duplicated.
%
One can achieve a similar effect by under-sampling the majority classes, groups, or their
intersections, such that they are equifrequent, and \iid{} sampling from that subset \(
\gD^{tr}_{\text{US}} \subset \gD^{tr} \) (or, conversely by duplicating instances from the minority
classes to the same end).
%
Under- and over-sampling (US and OS, respectively) have long been used as a remedy for class imbalance
\citep{chawla2002smote} but the former has recently been shown to be effective -- matching or
exceeding in performance more sophisticated algorithms -- for group robustness and
spurious-correlation problems \citep{sagawa2020investigation, idrissi2022simple} in part due to its
early-stopping effect.

Despite its intuitiveness and its long history, with roots in early statistical modelling, the
practical usefulness of importance-weighted ERM in the context of modern deep learning has recently
been impugned \citep{byrd2019effect, zhai2022understanding}.
%
\cite{byrd2019effect} demonstrate that for \emph{over-parametrised} models the effects of
importance-weighting diminish over the course of training; these effects can be partially recovered
when used in conjunction regularisation such as dropout, early-stopping and standard \(L_2\) weight
decay but without such interventions there converged-upon solution is identical for both IW-ERM and
vanilla ERM. 
%
Evidence for this was also provided by \cite{sagawa2019distributionally}, who stress the importance
of combining aggressive regularisation with (a dynamic form of) importance-weighting for
strong worst-group generalisation.
%
These empirical results were recently supported theoretically by the \cite{zhai2022understanding},
who prove that the implicit biases of these algorithms and standard ERM are indeed equivalent.
%
Summarily, while importance-weighting may be intuitive, it provably does not alter the solution to
the optimisation problem defined by the training set, which is to say, solutions that attain
zero-loss are invariant under reweighting.
%
This understanding has motivated other approaches, such as those based on polynomially-tailed
losses (for binary classification; \cite{wang2021importance}) and logit-adjustment
\citep{menon2020long}.
%
% The view of the latter is to instead aim to shift the classification boundary to be closer to the
% dominant (majority) classes and can be realised through the use of per-class margins, either
% through modifying the loss during training or post-hoc correction \cite{fawcett1996combining} 

As statistical models are only required model correlations in the data to satisfy the loss
function, they ultimately only capture a superficial representation of the true physical processes
involved.
%
In the discriminative case (that this thesis is concerned with), for a given \( X \) and \( Y \) we
are interested in approximating the conditional distribution \( P(Y|X) \); this  corresponds to
tasks like predicting the probability that a given image contains a dog (image classification), or
the probability that a given chest X-ray indicates a pulmonary infiltration, or some other thoracic
condition.
%
Indeed, the task accurately estimating \( P(Y | X) \) can be provably solved by observing a
sufficient amount of \iid{} data drawn from the joint distribution \( P(X,Y) \), yet this only
solve the problem from the aforementioned statistical perspective, and we will see that this
perspective is not always aligned with the causal one, which can lead to problems in generalisation
under certain conditions that crop up unsettlingly often in real-world applications, including
those that safety-critical.
%
This is to say, the predictions of a statistical model should only be trusted when the conditions
of the training and test distributions are sufficiently similar, and, in short, arbitrary shifts
(interventions on the data-generating distribution) can give rise to arbitrarily bad predictions
\citep{pearl2009causality, scholkopf2012causal}.

Since the true causal relationships between independent and dependent variables is, generally, not
\emph{identifiable} given the training data alone, owing to confounding variables, additional
information, as provided by interventions, or \emph{environments} \citep{peters2016causal}, is
needed to resolve the statistical ambiguity; this the typical tack pursued within the domain
generalisation generalisation literature, where each domain can be viewed as a different
intervention on the true distribution.
%
By `confounding variable', or \emph{confounder}, we mean some variable that is that is
the causal parent of two or more other variables and explains the statistical dependency between
them despite those variables not being causally related themselves; in the trivariate case this
corresponds to the fork \(X \from S \to Y \), wherein there exists a spurious (acausal) correlation
between \(X\) and \(Y\) that is eliminated by conditioning on the confounder \(S\).
%
In the shortcut learning problems addressed in Chapters 3 and 4, we will see statistical learning
breakdown in a similar way yet for essentially the opposite reason.
%
Namely, instead of having latent variable that explains the statistical dependency \(X\) between
\(Y\) in the absence of a causal dependency, we instead of have some spurious variable, \(S\) on
which \(Y\) is strongly statistically, but not causally, dependent, with \(X \to Y \) assumed to be
the true causal mechanism.
%
We will delve more deeply into what Shortcut Learning is and how the mechanisms that give rise to
it in \ref{sec:shortcut-learning}.
%
For now, however, we will move onto discussing different types of distribution shift that
statistical learning has to contend with.

% (see \citept{vogel2020weighted} as reference for formulating the traditional ERM setup and that of
% its weighted counterpart, also see \cite{shimodaira2000improving, wang2021importance,
% semenova2019study, zhai2022understanding, idrissi2022simple} 

% ------------------------------------------------------------------------------  
\subsection{Learning under Class Imbalance/Long-tail Learning}
% ------------------------------------------------------------------------------  

Notes from and on \cite{menon2020long}:
%
\begin{itemize}
        \item Real-world classification problems typically exhibit long-tailed label distributions,
          wherein most labels are associated with only a few samples.
          %
        \item Owing to this paucity, generalisation on such labels is challenging -- a classifier
          trained on such data following such a distribution is susceptible to undesirable bias
          towards the dominant labels. 
          %
          This problem has been widely studied in the literature on learning under \emph{class
          imbalance} \citep{cardie1997improving}.
          %
        \item 
          Existing approaches to coping with class imbalance modify: 
          %
          \begin{enumerate}
            \item inputs to the model, for example by over- or under-sampling
              \citep{kubat1997addressing, chawla2002smote} \item outputs (logits) of a model, for
              example by post-hoc correction of the decision threshold \citep{fawcett1996combining}
              %
            \item internals of a model, e.g. by modifying the loss function 
          \end{enumerate}'

\end{itemize}'


% ********************************************************************************
\section{A (Brief) Taxonomy of Types of Distribution Shift}
% ********************************************************************************
In this section, we provide a brief taxonomy of the types of distribution shift that arise in the
statistical-learning literature and discuss how and in what contexts they might practically emerge.
%
To this end, we draw heavily upon the works of \cite{moreno2012unifying} and
\cite{castro2020causality} in our definitions, noting that the ML literature is not of a
single mind regarding the terminology and its semantics.
%
In \S \ref{sec:lens-of-causality} we will reframe these distribution shifts in causal terms by
introduction of an exogenous variable -- allowing for an elegant formulation of the distribution
shift problem and its relation to invariance -- but we leave that aside for now and seek to present
them in more general terms.
% --------------------------------------------------------------------------------
\subsection{Covariate shift}{ssec:covariate-shift}\label{ssec:covariate-shift}
% --------------------------------------------------------------------------------
The most well-studied of the shifts, simply put, covariate shift refers to a change in the marginal
distribution of the inputs, that is to say we have \(P^{tr}(X) \neq P^{te}(X) \) while the
conditional distribution remains (effectively) unchanged, i.e. \)\(P^{tr}(Y|X) = P^{te}(Y|X) \).
%
% NOTE: still undecided about the below; maybe this (and target shift, conversely) don't have 
% much meaning unless we presuppose the direction of causality.
Departing from \cite{moreno2012unifying}, we do not restrict its definition to problems of a causal
(\( X \to Y\)) nature and do away with the distinction between covariate shift and its anticausal
(\( Y \to X \)) analogue in \emph{prior shift} to simplify exposition.
%
Changes in the distribution of the target variable, \(Y\), will be referred to as \emph{target
shift}, as explained later in this section.
%
This is not to say that we disregard the importance of distinguishing between the two causal
directions; in the context of DA and SemiSL, we will discuss at some length the dependence of these
paradigms on this characteristic of the problem.
%
Indeed, a common assumption in DA is that the source and target domains are separated by covariate
shift \cite{david2010impossibility}, however this assumption breaks down when the problem is
anticausal (when we have what \cite{moreno} term prior shift).

% --------------------------------------------------------------------------------
\subsection{Target shift}\label{ssec:label-shift}
% --------------------------------------------------------------------------------
% NOTE: As noted in the note above, we might want to specify that this type of shift only applies
% to anticausal tasks
Diametric to the above, target shift describes, as the name suggests, a shift in the
marginal distribution of the targets, \(Y\), i.e. \( P^{tr}(Y) \neq P^{te}(Y) \).
%
In the classification setting, this means that classes do not appear equifrequently in the training
and test data; many real-world datasets used for training exhibit long tails, \wrt{} the classes
(or targets generally), in which the most-frequent class can appear orders-of-magnitude more
frequently than the least-frequent class, while the test data has more even coverage.
%
As discussed in \S~\ref{iid}, a classic approach to rectifying this kind of shift, in the case of
the discriminative models we are usually concerned with, is to  importance-weight the instance
losses or, near-equivalently, the sampling mechanism, using the ratio \(\frac{P^{te}(Y}{P^{tr}(Y)}
\), or simply by the denominator should \( P^{te}(Y)\) not be reliably estimable (as is often the
case).
%
% This can arise from different predispositions in the training and test populations, or from
% variations in environmental factors.
%
% \subsection{Manifestation shift (conditional).}\label{ssec:manifestation-shift}
% %
% Manifestation shift refers a change in the way prediction targets physically manifest in
% between domains.
% %
% Denoting the random variable corresponding to the domain, or environment, as \(E\), we can then
% couch manifestation shift in mathematical terms as shift as inducing the inequality \( P^{tr}(E |
% Y) \neq P^{te}(E | Y)\).
% %
% As with target shift, this type of shift irremediable unless one employs strong parametric
% assumptions on the nature of the interdomain differences.

\subsection{Concept shift.}\label{ssec:concept-shift}
%
To complete the triad of bivariate distribution shifts (we will later revisit distribution shift
under the influence of an exogenous domain or environment variable) we have \emph{concept shift},
referring to changes in the conditional distributions, \( P(Y|X) \) or \( P(X|Y) \), while the
respective (\( P(X) \) and \( P(Y) \)) marginal distributions are preserved.
%
Thus, in the classification setting, concept shift corresponds to a change in the mechanism used to
label the data; this might entail, for example, changes in the class definitions, differences in
annotation protocol or grading scales between sites,  or different proclivities/standards in the
annotators in the case of human-driven annotation should the task possess an element of
subjectivity (AI-alignment via RLHF \citep{bai2022training} being a prime and topical example of
such a task).
%
In addition to the shifts discussed, one can naturally also consider their composition, giving rise
to \emph{compound shifts}, in which both the marginal and conditional distributions are subject to
change. Such shifts, however. are unusual in the literature, and, perhaps more pertinently,
impossible to solve unless one can draw upon strict assumptions, due to the need to decouple the
constituent shifts (a problem of identifiability).
%
% \subsection{Acquisition shift (domain shift).}\label{ssec:acquisition-shift}
% %
% Domain or acquisition shift refers to a change in distribution arising due to a change in how or
% where the data was acquired; this commonly stems from differences in measuring
% equipment, such as the type of satellite, in the context of remote sensing, or camera
% trap, in the context of ecology.
% %
% A prime example of such a shift can be found in the medical imaging domain, where differences in
% the scanning equipment used to collect the training and test data can lead to introducing
% site-specific biases that hinder generalisation and cannot be readily rectified by means of
% data-preprocessing \citep{glocker2019machine}.

% --------------------------------------------------------------------------------
\subsection{Sampling/Selection/Representational bias}\label{ssec:sampling-bias}
% --------------------------------------------------------------------------------
Sampling (which we use synonymously with \emph{selection} and \emph{representational}) bias refers
to distribution shifts that arise due to systematic flaws in the data-collection process that cause
training samples being selected in a non-uniform fashion from the general population being
modelled.
%
That is to say, the data is not missing at random but rather conditionally, and most notably when
the conditioning is on the target or some other characteristic, such as a particular demographic.
%
Thus, sampling bias, isn't a type of distribution shift per se, but rather a mechanism by which the
above-described distribution shifts can emerge, and it is particularly germane to Chapters 3 and 4
of this thesis in which we consider extreme cases of it in which certain demographics, or outcomes
for certain demographics, are omitted from the training data, promoting spurious correlations
between said demographics and the outcome.
%
To give an example, in conducting a local survey there will invariably be subsets of the general
population which are under-represented, or altogether excluded, from data-collection due to
availability, willingness, and applicability to the research being conducted; if the locale in
question were a University, then we would expect the population to be significantly younger and
more liberal than on average.
%
Indeed, this a problem is particularly well-noted in experimental psychology, in which cohorts
overwhelmingly consist of a very narrow band of individuals from the so-called WEIRD (White,
Educated, Industrialized, Rich, Democratic)\citep{henrich2010weirdest} group.
%
The experimental data obtained from such homogeneous cohorts has been used  by numerous journal
papers to support broad claims about the general population.
%
A prominent yet more subtle, mechanistically, example of sampling bias can be found in the credit
scoring literature, in which no feedback is obtained from previously rejected candidates; this
leads to bias amplification (as the model's past decisions directly shape the training data at
future iterations) and in the context of fairness, demographic biases incurred due to such feedback
models has been studied under the guises of Delayed Impact \citep{liu2018delayed} and Residual
Unfairness \citep{kallus2018residual}.
%
Indeed, the Residual Unfairness problem introduced by \cite{kallus2018residual} was one of the
chief motivators for the setup considered in Chapter 3, such that in the case of a binary decision
system -- one designed for automated hiring, for instance -- and a population comprised of two
subgroups, only positive outcomes are observed for the advantaged subgroup, while only negative
outcomes are observed for the disadvantaged subgroup.

% ********************************************************************************
\section{Shortcut Learning}\label{shortcut-learning}
% ********************************************************************************

While the notoriety of shortcut-learning (SCL) in ML sphere is relatively recent, the phenomenon
underpinning it is a fundamental one in statistics, one that may be summed up with the age-old
aphorism \emph{cum hoc ergo propter hoc}, oft translated as \emph{correlation does not imply
causation}.
%
DNNs define deeply expressive function classes, yet the solutions encoded by their parameter space
need not be commensurately complex; in fact, it is well established that these models -- in the
absence of an countervailing (inductive) bias -- exhibit a \emph{simplicity bias} (SB;
\cite{valle2018deep}), that is, the tendency to favour simpler solutions should those solutions
serve sufficiently well for the task (as defined by the training set and loss function) at hand.
%
While the spate of failures following the deep-learning revolution were surprising -- and at the
very least spirit-dampening to the those with lofty hopes for ML -- it is, given thought,
\emph{not} surprising that SB should exist and beget SCL, for while SB alone is not alone a
precondition for SCL, the second precondition  of SCL is a problem (on the data side) that has long
challenged statistical modelling: \emph{sampling bias}.
%
It is the combination of simplicity bias and acausal or spurious, correlations generated by
sampling bias that give rise to SCL, but sampling bias is not something trivially redressed, even
if the seemingly-straightforward recourse of `collect more data' does exist, which it often doesn't
due to physical constraints (e.g. the data may only have been available within a given period of
time) or limited (human or monetary) resources.
%
Although the problem may stem from the data-collection side, one is not without recourse on the
modelling side, so long as certain assumptions or criteria can be met;
%
indeed ,both DG and FairML are active -- more so than ever -- subfields of ML that contend with
different flavours of the problem and have successfully developed mitigation strategies for them.
%
The now-canonical example of shortcut-learning in the DG literature -- which we will also invoke
here for its simplicity -- is due to \cite{beery2018}, wherein the task is one of distinguishing
between cows and camels (binary classification). 
%
Since camels preponderate on sandy backgrounds, while, by contrast, cows preponderate on grassy
backgrounds -- owing to their natural habitats -- the background is a viable shortcut solution
based on which examples from the training set can be reliably predicted while taking the path of
least resistance, something the model can hardly be blamed for in absence of the requisite
inductive bias to disentangle the true and spurious features.
%
While the brittleness of the shortcut solution will not be exposed if the test set consistently
suffers the same sampling bias as the training set, it is perfectly conceivable that a cow could
appear on a beach -- a common sight on the island of Corsica, for example -- and our model would
mispredict in such a case because it does not grasp what the concept of a cow truly is -- to it,
`grassy' and `cow' are synonymous.
%
Of course, this is a relatively trivial and benign example, but there are many real-world cases
where this behaviour could lead to life-endangering failures, perhaps most obviously in the medical
data domain where one could have a pneumonia classifier that has learned to predict pneumonia from
X-ray images with near-perfect accuracy based solely on a hospital-specific token and the
hospitals' pneumonia-prevalence rate, as observed by \cite{zech2018variable}.
%
There are also obvious ethical concerns that arise when the spurious features in question
correspond to protected characteristics like `race' and `gender' \citep{buolamwini2018gender,
wang2019balanced}, regardless of aggregate downstream performance. 
%
The landmark study by \cite{buolamwini2018gender}, for example, revealed significant disparities in
the performance of face analysis algorithms on individuals from marginalised (dark skin, female)
vs. non-marginalised (light skin, male).

There are two sides to shortcut-learning that impair generalisation.
%
The obvious one, which we have already belaboured, is `variance` ` to spurious features -- features
that are statistically but not causally related to the target; the second one, however, is more
subtle and a consequence of the first one, that being \emph{feature suppression}, in that the model
is not simply variant to the `wrong' features but invariant to the `right' ones -- it is not simply
a matter of a difference in importance but in reality a more pernicious matter of
inclusion/exclusion.
%
This is to say, if a shortcut solution is robust enough to achieve near-zero loss on the training
set, then there is little incentive -- owing to gradient starvation \citep{pezeshki2021gradient}
and the probably-flatter minima of shortcut solutions \citep{scimeca2021shortcut} -- for the model
to learn alternative `views' (collections of features; \cite{allen2020towards}).
%
For instance, if texture is a reliable classification cue given the training data
\cite{geirhos2018imagenet}, a model can latch onto that cue and ignore (be invariant to) other
higher-level semantics, like shape and global structure, that human judgements are much more
strongly attributed to.
%
High-frequency cues, such as colour and texture, are readily modulated by (unstable under) changes
in lighting, for instance, making them less reliably cues for object classification in a dynamic
environment; we are not wont, for example, to classify an object in the shape of a cat as an
elephant simply because the texture of the latter has been transplanted, \emph{ceteris paribus}, to
the former, a failure mode (in)famously shown by \cite{geirhos2018imagenet} to apply to DNNs
trained on ImageNet.

With the above in mind, it is obvious why more traditional approaches to improving group- and
adversarial-robustness fail. 
%
The power of ensembles, for instance, resides in their combining of different views of the data --
engendered by stochasticity in the weights and optimisation procedure -- yet shortcut solutions
produce correspond to such a strong (easy-to-learn and potent) and stable attractor that all
ensemble members simply converge onto that one, corresponding view.
%
Domain adversarial learning -- popularised by \cite{ganin2016domain} and a mainstay throughout the
DA, DG, and FairML literature alike -- on the other hand suffers from the problem that for the
features of the model to be statistically independent of the spurious feature, so must it be
statistically independent of the target since the target and spurious feature are themselves
strongly correlated, as defined by the SCL problem.
%
% % --------------------------------------------------------------------------------
% \paragraph{Simplicity Bias \citep{valle2018deep, shah2020pitfalls}}
% % --------------------------------------------------------------------------------
% \begin{itemize}
%   \item In stationary settings where there is no mismatch between the training and test 
%     distributions, generalisation is usually maximised by selecting models according to the 
%     statistical equivalent of Occam's Razor, of using the simplest model that explains the data 
%     well. However, when the training and test distributions are not aligned in some sense, this 
%     fails to hold up where the simplest models are those that latch onto spurious statistical 
%     correlations in the training data and thus are not stable under non-causal interventions in 
%     the marginal or conditional distributions of $P(Y|X)$ (resulting in \emph{covariate} shift 
%     and \emph{concept} drift respectively).
%     %
%   \item On real-world datasets there are several distinct ways to discriminate between labels (e.g. 
%     based on shape, colour, texture, etc.) that are (a) predictive of the label to varying extents, 
%     and (b) define decision boundaries of varying complexity. 
%     %
%   \item For example, in the image-classification task of swans vs. bears, a linear-like simple 
%     classifier that only looks at colour could predict correctly on most instances except white 
%     polar bears, while a non-linear complex classifier that infers shape could have almost perfect 
%     predictive power.
% \end{itemize}


% Key findings from \cite{scimeca2021shortcut}:
% \\begin{enumerate}
%          \item Certain cues (conducive to the visual recognition problem at hand) are preferred to
%            others
%          \item Solutions biased to the easy-to-learn cues tend to converge to relatively flat minima
%            on the loss surface.
%          \item The solutions focusing on those preferred cues are far more abundant in parameter
%            space.
%          \item Solutions corresponding to Kolmogorov-simple cues are abundant in parameter space
%            and thus preferred by DNNs.
%           \item Devise a setup where multiple cues are equally valid for solving the task at hand;
%             find that DNNs attend to choose cues in a certain order (e.g. colour is preferred to
%             rotation).Simple cues, based on Kolmogorov complexity, are far more frequently
%             represented in the parameter space and thus are far more likely to be adopted by DNNs.
          
%  \end{enumerate}
% Extended notes on \cite{scimeca2021shortcut}:
% \begin{itemize}
%         \item In some cases, the shortcut bias arises in models that suppress certain streams of
%           inputs: visual question answering (VQA) models often neglect whole-image cues, for one
%           does not require images to answer questions like ``what question is the banana in the
%           image?'' \citep{cadene2019rubi}.
%         \item Shortcut biases become problematic when it comes to generalisation to more
%           challenging test-time conditions, where the shortcuts are no longer valid. These biases
%           also cause ethical concerns when the shortcut features correspond to sensitive attributes
%           like gender or race \citep{wang2019balanced}.
% \end{itemize}

% Notes on and from \cite{shah2020pitfalls}:
% \begin{itemize}
%         \item Well-studied approaches such as for improving group- and adversarial-robustness --
%           such as ensembling and adversarial training -- do not mitigate the problem of simplicity
%           bias on the proposed datasets.
%           %
%           The failure of ensembling is to be expected, as there if the simplicity bias is
%           sufficiently strong, there is little-to-no incentive for the ensemble members to learn
%           diverse views of the data \citep{allen2020towards} due to stochasticity in the weights
%           and optimisation procedure, this diversity being where the presumed origin of the
%           improved-robustness demonstrated by (deep) ensembles.
%           %
%           The failure of adversarial training is also to be expected given that the perturbations
%           do nothing to intervene on the spurious feature directly.
%           %
% \end{itemize}

% Notes on and from \cite{geirhos2020shortcut}:
% \begin{itemize}
%   \item Despite DNNs being touted as achieving super-human-level performance on complex tasks
%     across myriad domains, we are currently observing a large number of failure cases stemming from
%     small -- imperceptible even -- changes in the inputs or or changes in different background
%     contexts that should be incidental to the task.
%     %
%   \item DNNs can generate plausible captions for images and yet do so without looking at the image
%     in sooth \citep{cadene2019rubi}. 
    
%     %
%   \item DNNs can accurately recognise faces but achieve significantly higher error rates for faces
%     from marginalised groups \citep{buolamwini2018gender}.
%   %
%   \item DNNs can predict hiring decisions based on resumes but the predictions are biased towards
%     selecting men \citep{dastin2018amazon}.
%   \item One central observation is that many failure cases are not independent phenomena but
%     instead are interconnected in the respect that they are the result of DNNs adopting unintended
%     `shortcut' strategies.
%     %
%     While superficially successful, these strategies typically fail under slightly different
%     circumstances; for instance, a DNN may appear to caption an image perfectly well, but describes
%     a typical grass landscape as 'a herd of grazing sheep'

%   \item At a principal level, shortcut learning is not a novel phenomenon. 
%     The field of ML has long aspired to develop a formal understanding of shortcut learning, which
%     has led to an increasing body of research under different names, such as `learning under
%     covariate shift', `anticausal learning', `dataset bias' and the `Clever Hans effect'.

%   \item Shortcuts are decision rules that perform well on \iid{} test data but fail on o.o.d. test
%     data, revealing a mismatch between the intended and learned solutions.
% \end{itemize}

% ********************************************************************************
\section{Underspecification}\label{sub:underspecification}
% ********************************************************************************
 
As we discussed in the previous section, one of the major dangers posed by SCL is its
diaganosability: since data used for testing the model during development is generally drawn from
the same distribution as the training data -- and thus inherits the same biases -- it is only once
the model is deployed on \ood{} data that the potential shortcut-dependencies are revealed, unless
one takes measures to properly vet the model's solutions using tools from the interpretability
literature, such as feature attribution.
%
Thus, there is an element of what is termed \emph{underspecification} involved in the SCL problem,
and in \ood{} generalisation generally, and is what makes this class of problem so challenging, or
even ill-posed without sufficient inductive bias of the underlying problem or foreknowledge of the
downstream distribution.
%
Underspecification simply refers to settings in which the \iid{} test data used for model selection are
insufficient proxies for the data on which the selected model is ultimately to be deployed.
%
By `insufficient' we mean not only is there a discrepancy in the performance of any given model
when evaluated on the \iid{} and \ood{} test sets -- which is to be expected -- but that there also
a discrepancy in how the performance the ranking of different models on the two datasets.
%
In a special, but typical, case, it may also be that a number of models achieve equitable
performance on the \iid{} test set but perform wildly differently when exposed to \ood{} data.
%
To tie this in with our cows vs. camels conceit from the last section, given two classifiers such
that one is shortcut-exploiting and one is shortcut-avoiding (causally-concordant)- and their
evaluation on a subset of the training data, it could well be the case that the two models are
performantly equivalent despite embodying very different solutions (the former relying on
background, the latter relying on some non-linear combination of high-level features like head/body
shape, posture, part-whole relations, etc.), one of which will fail in the \ood{} case in which the
spurious correlation fuelling the shortcut solution no longer holds.

%
One can formalise the problem underspecification in terms of \emph{Raashomon sets}
\citep{semenova2019study}.
% defined \wrt{} the test set used to guide model-selection and indeed
% assess the feasibility of performing a given task with a machine learning system given the data
% available. 
The Rashomon effect \citep{breiman2001statistical},
%
\footnote{taking its name from the Kurosawa film
in which four witnesses recount wildly differing versions of the same crime}
%
which lends Rashomon sets their name, describes the phenomenon in which there exists a
non-singular set of equally-performing predictors from a given function class $\gF$.
%
 The empirical Rashomon set is a subset of models of the hypothesis space $\gF$ forming an
 equivalence class (according to scoring function $\phi$), with some tolerance, $\epsilon \in \R^+$, \wrt{}
 the best model in the class, $f^\ast$. 
%
We can generalise the formulation given in \citet{semenova2019study} to allow performance to be
measured \wrt{} to an evaluation set,  $\gD^{eval}$, by making this a second parameter of
$\phi$, such that we have
%
\align\begin{align*}
  % lhs
  \frak{R} \triangleq 
  \{ f \in \gF: \phi(f, \gD^{eval}) 
  \leq 
  % rhs
  \phi(f^\ast, \gD_{eval}) + \epsilon}.
\end{align*}

It is perhaps interesting to note that (deep) ensembling hinges on the assumption that for many
problems Rashomon sets exist, combining seemingly-equivalent (in terms of \iid{} performance)
models to construct a composite predictor that draws upon a wider set of views and is thus more
robust to distribution shift.
%
However, as noted before, in the case of the SCL problem, this Rashomon set could well be a
singleton (in that all models are functionally equivalent) when the spurious feature is of low
enough complexity (and thus easy to learn) and strongly predictive of the target, thus nullifying
traditional ensemble-based approaches.
%
For \ood{} problems more generally, such as the one addressed in Chapter 5, the idea of
underspecification, and Rashomon sets with it, is a pertinent one.
%
Indeed, the problem of model selection, due to underspecification, was well noted in
\cite{gulrajani2020search} in the context of domain generalisation and how to best conduct it is
still very much an open question.

% ********************************************************************************
\section{Through the Lens of Causality}\label{sec:lens-of-causality}
% ********************************************************************************

We now introduce a causal formalism of the distribution-shift problem, a formalism which has been
frequently exploited in the DG and FairML literature as it provides a simple calculus with which
to reason about desired (and undesired) variances.
%
It should be noted in advance that we only draw upon this formalism in order to provide a unified
formulation of the distribution-shift problems considered in this thesis; we do not operate on the
domain of causal graphs nor attempt to perform causal inference. 
%
The background on causality is thus commensurably light and we refer the reader
to \cite{pearl2009causality} for full exposition of the topic.

While the term `domain' typically refers to the observed distributions as a whole in both DA and DG
alike (i.e. `source' versus 'target'), such terminology is somewhat rigid, as it fails to capture
that the distributions share an underlying structure and how and which variables are shifted.
%
It is more arguably flexible then, consistent with \citep{mooij2020joint}, to think of the domain
as some exogenous latent variable, which, by its conditioning, gives rise to the different observed
distributions -- or subgraphs in the discrete case -- and explains how one is transformed
(`shifted') into the other.
%
We will denote said variable as \(E\) (for `\bf{E}nvironment', as it is commonly termed in the DG
literature \citep{arjovskyinvariant}), which need satisfy only the loose requirement that it belong
to some Borel space (and thus may in theory be continuous or discrete).
%
Most simply, in the case of DA, \(E\) is simply a binary random variable, such that we have \(E:
\Omega \to \{ \text{source}, \text{target} \}\), with \( \Omega \) being the sample space.
%
We view then view variables in our prediction task as constituting the node \gV \) in a Causal
Bayesian Network (CBN) where the direction of arrows (directed edges) between nodes indicate the
direction of causality (e.g. \(\rmA \to \rmB \) means that \bf{A} causes (is a parent of) \bf{B})
while the absence of an edge between two nodes \textbf{A} and \textbf{B} indicates independence
between them when conditioned on their parents, i.e. \( \rmA | \text{Pa}(\rmA) \perp \rmB |
\text{Pa}(\rmB) \), where \( \text{Pa}(\cdot) \) denotes the causal parents of its argument node.
%
Formally, a CBG is a kind of Directed Acyclic Graph (DAG), \(\rmG \triangleq \langle \gV, \xi
\rangle \) with node-set (variables), \(\gV\), and (directed) edge-set, \(\xi\) consisting of tuples
\((ij)\) meaning \(i \to j \), or `node \(i\) is a parent of node \(j\)'.
%
Each node in \rmG then defines a probability distribution, conditional on its parents, such that
the joint distribution of \(\gV\), \(P(V)\), factorises as \( P(V) = \prod_{v \in \gV} P(v |
\text{Pa}(v)) \) where we can now define \(\text{Pa}(\cdot)\) as a function that returns all nodes
in \(\xi\) that form a pair with \(v\) as the second element, i.e. \( \{ i | i,j \in \xi, j = v \}
\).
%
Without loss of generality, for the prediction task with inputs, \(X\), and targets, \(Y\), we may
introduce the aforementioned variable \(E\) to convert the joint distribution \(P(X, Y)\) into the
conditional joint distribution \( P(X, Y) | E \); the structure of the underlying CBN determines
the factorisation of this distribution and thus the nature of the distribution shift in question.
%
One can, for example characterise the case of covariate-shift with causal \(f^\star\), as having
edges  \(E \to X\) and \( X \to Y \), giving rise to the factorisation \( \P(X, Y | E) | P(E) =
P(Y|X)P(X|E)P(E) \). 
%
In Chapters 3 and 4 we go beyond the bivariate (excepting \( E \)) and covariate case and consider
label-shift problems with an additional auxiliary label \(S\) -- corresponding to an identifier of
some subgroup or spurious feature we wish to be invariant to in the name of fairness or
generalisation --  in which \(E\) influences the joint distribution \( P(S, Y) \) but not the
marginal distribution \(P(X)\), giving rise to representation bias and, from it, spurious
correlations.
%
We illustrate in Fig.~\ref{fig:ds_cbgs} CBGs corresponding to different distribution shifts for a
causal prediction task.
%

\import{background}{dist_shift_cbgs.tex}

%
We will see that such a formulation is particularly utile when we come to discuss domain
generalisation which involves a multitude of source distributions, going beyond the dyadic
source-target setup characterising DA.

% ------------------------------------------------------------------------------
Points to incorporate from, and inspired from, \cite{scholkopf2021toward}:
%
/\begin{itemize}
  \item 
    The Independent Causal Mechanisms (ICM) principle states, in short, that the generative process
    giving rise to a system's observed variables is governed by \emph{autonomous} modules that do
    not inform (have zero mutual information \wrt{}) or influence each other.
    %
    In the probabilistic case, this means that the conditional distribution of each variable, given
    its causes (parents), does not inform or influence other mechanisms.
    %
    Applied to casual factorisation, the principle dictates that the factors should be independent
    in the sense that 
    %
    \begin{enumerate}
      \item Changing (intervening on) one mechanism in the system (CBN), \(P(i, \text{Pa}(i))\)
        does not change any of the other mechanisms in the same system, \(P(j, \text{Pa}(j))\),
        \(\forall j \neq i \).
      %
      \item Knowing information about \(P(i, \text{Pa}(i))\) does not confer us additional
        knowledge about \(P(j, \text{Pa}(j))\), i.e. \( \forall (i \neq j): \gI(i; j) = 0 \).
    \end{enumerate}
    %
    The notion of invariant, autonomous, and independent mechanisms has appeared in many guises
    throughout the history of causality research.
    %
    The \emph{invariance criterion} of Herb Simon states that the true causal order is the one that
    is invariant under the right sort of intervention.
    %
    Pearl avers that a causal mechanism remains invariant when other mechanisms are subjected to
    external (exogenous influence).
    %
    One may then derive from the tenets of the ICM principle, the Sparse Mechanism Shift (SMS)
    hypothesis, which simply postulates that small distribution shifts tend to manifest themselves
    in a sparse of local way in a causal factorisation (i.e. not all factors should be affected
    simultaneously).
    %
    A factorisation, on the other hand, that does not exhibit this behaviour can be said to be
    \emph{entangled}.

  \item 
    According to the ICM principle, independence of two mechanisms should mean  that the two
    conditional distributions do not inform or influence one another.
    %
    Note that this does not necessarily align with the notion of statistical dependency.
  \item 
    Causal structure captures the physical mechanisms that generate statistical dependencies in
    the first place.
    %
    Statistical structure is an epiphenomenon that follows if we make unexplained variables random.
    %
    By virtue of modularity, a world model based on causally factorised latents is maximally
    compressive in general, and by this we mean it provides the simplest explanation to the complex
    physical world. 
    %
    It is much more efficient, for example, to explain changes in the shape of an object as one
    moves around it by a change in vantage point, in terms of global symmetry, rather than by
    independent changes in local structure, as would be explained by an \emph{entangled} model.
    %
    The connection between causal representation learning and modern physics thus becomes clear
    when one thinks as modularity being synonymous with invariances, or \emph{symmetries}, with it
    being little exaggeration to say that modern physics is the study of \emph{symmetries} and the
    corresponding conserved quantities, as predicated by Noether's celebrated theorem
    \citep{noether1918invariante}.
    %
    In light of this, the statistically- and causally-driven approaches are manifestly at odds with
    one another in the context of shortcut learning -- shortcut features provide the simplest,
    loss-minimising solution based on the training distribution, \(P^{tr}(X, Y)\), however they do
    not provide the simplest solution in the causal sense as evidenced by the lack of
    generalisability.
    %
    Models that are faithful to the underlying causal structure of the problem are much more robust
    to distributional changes and are more protean because the learned modules can be reconfigured
    arbitrarily according to the problem at hand and can be updated locally to incorporate new
    information without degrading modules adapted for other, unrelated tasks.
    %
    \item

 \end{itemize}


One common \citep{arjovskyinvariant, krueger2021out, sagawa2019distributionally} and intuitive way
of formulating the robust-prediction problem is as one of bilevel optimisation, where the inner
loop entails computing the empirical risk over each domain and the outer loop corresponds to
finding the function that minimises the maximum of said risks.
%
We can then define, accordingly, the optimal predictor as 
%
\begin{align}\label{eq:rob-erm-obj}
 f^\ast_\text{ robust } =
 \underset{ f \in \gF }{ \text{\argmin} } 
 \underset{ e \in \gE} { \text{max} }
 \E_{ P^{tr}(X, Y | E = e) } \[ \ell ( f( X ), Y ) \].
\end{align}
%
As discussed at length before, domains/environments can be modelled as deriving from different
interventions of the causal factorisation of \(P(X, Y\)).
%
It follows that, for Eq.\ref{eq:rob-erm-obj} to engender successful generalisation to arbitrary
domains, \(\gE^\dagger\) outside \(\gE\) (\(\gE^\dagger \cap \gE = \varrnothing \); i.e. the goal
of DG), \(\gE\) must be a representative (well-covering) set of samples from the generating
distribution \(\P(E)\) such that smooth interpolation along the underlying manifold is possible.
%
When \(\gE\) is a finite set, as above, one can think of it as \emph{perturbation set}, accordant
with the robust optimisation literature \citep{ben2009robust} .
%
While generalisation to arbitrary perturbations is provably hard (or impossible), in general
\citep{david2010impossibility}, when \(\gE\) encodes prior information about the kinds of
perturbations one expects to encounter at test-time then incorporating it into the optimisation
process can be fruitful, both for allowing interpolation within the convex hull defined by the set
and to an extrapolated region outside of it \citep{krueger2021out}.
%
Indeed, it stands to reason that by allowing the model to glean which features are and aren't
stable across environments would allow it to better approximate the true causal structure of the
prediction task. 
%
This idea has been explored extensively in recent years in both the causal discovery
\citep{peters2016causal, bengio2019meta} and domain generalisation \citep{arjovskyinvariant,
ahuja2020invariant, creager2021environment} literature, with the caveat that a degree of inductive
bias or additional information is necessary to provably identify the proper causal relations based
on it \citep{lin2022zin}.
%

% ********************************************************************************
\section{Domain Adaptation (DA)}\label{sec:domain-adaptation}
% ********************************************************************************

Domain adaptation is a subfield of machine learning that deals with the problem of adapting a model
trained on one distribution (the \emph{source domain}) to a different but related distribution (the
\emph{target domain}), in such a way that the relevant shared structure is exploited while nuisance
factors that are domain-specific (and not relevant to the prediction task) ignored.
%
The downstream performance of the  model is thus naturally dependent on both the performance on the
source domain and by the degree of relatedness between the source and target domains.
%
To proffer a real-world example, in building a spam detector, one might have annotated data
(emails) available for training a model sourced from a previous group of users and wish to deploy
(adapt) the detector to a new group of users in such a way that is robust to the temporal
distribution shift.
%
In the classical DA setting, one assumes the distribution shift is \emph{covariate}
\cite{david2010impossibility} in nature, that is, localised to the marginal distribution
\(P(X)\), with both the conditional, \(P(Y|X)\) (corresponding to changes in the ground-truth
labelling mechanism, \(f^\star: \X \to \Y \)), and label, \(P(Y)\), distributions consistent across
domains.
%
This is not to say that there is not a substantial body of work that addresses other types
of distribution shift, however\citep{zhao2019learning}, and the covariate-shift assumption is
perhaps stricter than one might initially presume as it assumes.
%
Indeed, it turns out that the covariate-shift assumption is only tenable in cases where \(f^\star\)
is \emph{causal} (\(X \to Y\)); practically, there are many cases forh which the converse in fact
holds true true, that the relationship between \(X\) and \(Y\) is \emph{anticausal} (\(Y \to X\)).
%
Anticausal prediction tasks naturally arise in the medical-imaging domain for instance, where \(Y\)
is some gold-standard indicator of the presence of the disease and it is the disease that gives
rise to aberrations in the input images signalling to a classifier a positive instance. 
%
For the task of melanoma-prediction, for example, one may be interested in training a classifier to
diagnose patients based only on dermoscopic images using labels derived from (expensive and
time-consuming but reliable) histopathological analysis \citep{castro2020causality}.
%
The distinction between causal and anticausal tasks is an important one in ML generally, and we
will revisit the idea on several occasions throughout the remainder of this chapter; for SemiSL
said distinction is particularly important as the efficacy of the paradigm hinges on \(P(X\)
carrying information about \(f^\ast\), and thus the task being an anticausal one.
%

\subsection{Taxonomy of methods}
%
See \cite{zhang2013domain} for an early example (and theoretically-rigorous presentation of) domain
adaptation under (target and conditional) distribution shift.
%
% ********************************************************************************
\section{Fair Machine Learning (FairML)}\label{sec:fairml}
% ********************************************************************************

% ********************************************************************************
\section{Domain Generalisation}\label{sec:domain_generalisation}
% ********************************************************************************
While closely-related to DA, Domain Generalisation (DG) is distinct in the respect that the task is
fundamentally, as the name suggests, one of \ood{} generalisation rather than one of adaptation.
%
By this we mean that while in (U)DA one is given a labelled dataset, \( \gD^{source} \triangleq
\{x_i, y_i \}_{i=1}^{N^{source}} \), belonging to the source domain, along with an unlabelled
dataset \( gD^{target} \triangleq \{ x_i \}_{i=1}^{N^{target}} \) belonging to the target domain,
and the goal is to train a classifier to generalise from the former to the latter (which entails a
degree of invariance), DG is more general, in that one is instead given datasets from multiple
domains and seeks to train a classifier that can generalise to previously unseen ones. 
%
That is, given the meta distribution \( \frak{D} \triangleq \{\gD_e \}_{e \in \gE} \) consisting of
\( |\gE| \) distributions drawn from different domains, or \emph{environments}, denoted by the
index set \( \gE \subset \sN \), the goal is to train a classifier that will perform optimally, or
with minimal degradation, when presented with distribution \( \gD_{e^{te}} \) from a novel
domain,\( e^{te} \notin  \gE \).
%
},
%
While we would ideally have a model that could generalise to any arbitrary environment (assuming
the task remains consistent), this is sadly impossible given finite data
\cite{david2010impossibility}, and so our expectations must be tempered to being able to generalise
within some region the training distribution.
%
The justification of DG can then be viewed from two perspectives: 1) It stands to reason that we should
be able to exploit information about known the known set of variances -- due to the domain -- in
order to learn a predictor that can generalise within the convex set (affine combinations of those
variances) they define as well of those variances that are close by.
%
The principle here is similar to that of vicinal risk minimisation \citep{chapelle2000vicinal} as
in \cite{zhang2017mixup}, wherein data augmentation fulfils the role of a perturbation set that the
environments fulfil in the case of DG.
%
2) Given a set of interventions on the underlying causal graph defined by the set of environments,
recover the causal relationship between the input features and the target such that the predictive
mechanism is unaffected by causally-independent changes (by interventions on variables not among
the target variable's causal parents).
%
This idea of treating environments as interventions and using them to perform explicit or implicit
causal inference has notably been exploited in \cite{peters2016causal} and in the foundational (to
DG) work of \cite{arjovsky2019invariant}.
%
Indeed, in the wake of \cite{arjovsky2019invariant}, it has become common \citep{
gulrajani2020search, krueger2021out, mahajan2021domain, lin2022zin} to express the problem setup of
DG and its desiderata in causal terms, and we will do so ourselves in
\S~\ref{sec:lens-of-causality} in order to provide a more unified perspective of the distribution
shift problems discussed thus far.

\subsection{In Search of Lost Domain Generalization \citep{gulrajani2020search}}
%
Machine learning systems often fail to generalise out-of-distribution (OOD), crashing in 
spectacular way when tested outside the domain of training examples.

\itemi\begin{itemize}
  \item Self-driving car systems struggle to perform under conditions different to those of 
    training, including variations in lighting \citep{dai2018dark}, weather \citep{volk2019towards}, 
    and object poses \citep{alcorn2019strike}
  \item Systems trained on medical data collected in one hospital do not generalise to other health
    centres \citep{castro2020causality, albadawy2018deep}
  \item failing to generalise is failing to capture the causal factors of variation in data, 
    clinging instead to easier-to-fit spurious correlations, which are prone to change from 
    training to testing domains (unstable in the face of interventions)
  \item  Examples of spurious correlations (SC) commonly encountered in machine learning include 
    racial biases, texture biases \citep{geirhos2018imagenet}, and object backgrounds
    \citep{beery2018recognition} .
  \item Alas, the capricious behaviour of machine learning systems to distributional shifts is a 
    roadblock to their deployment in critical applications.
\end{itemize}

%
The goal of DG is OOD generalisation: learning a predictor able to perform well on some unseen test
domain when no data from the test domain is available during training -- we must assume the 
existence of some statistical invariances across training and testing domains.
%
- DG differs from Domain Adaptation (DA) in that the latter assumes that the unlabelled data derived 
from the test domain is available during training.


% --------------------------------------------------------------------------------
\subsection{Connection between Fairness and DG}\label{ssec:fairml-dg-cxn}
% --------------------------------------------------------------------------------
\citet{creager2021environment} cast the problem fair machine learning as one of DG,
where the protected groups takes the role of the different domains/environments. In fairness
literature, the learning objectives represent context-specific fairness notions,
while in OOD literature, the learning objectives should be designed according to invariance 
assumptions.
%
A number of fair representation learning methods \citep{edwards2015censoring, madras2018learning}
are derived from domain adaptation (DA) methods.
When protected attributes are unknown, DRO and adversarially learning can be applied as in 
\citet{hashimoto2018fairness} and \citet{lahoti2020fairness}, respectively, to obtain a 
distributionally robust predictor and minimizing the worst-subgroup performance; the former can
also be adapted to cases in which such information is known as in \citet{sagawa2019distributionally} 

(\citep{krueger2021out} provide a good (albeit brief) discussion of the parallels between fairness and OOD
generalization)

Recent trend in the fairness literature is to consider how fairness behaves under distribution
shift.
%
\begin{itemize}
  \item \cite{schrouff2022diagnosing}
  \item \cite{schrouff2022maintaining}
  \item \cite{singh2021fairness}
  \item \cite{slack2020fairness}

\end{itemize}


% --------------------------------------------------------------------------------
\subsection{A (brief) taxonomy of domain-generalisation methods}
% --------------------------------------------------------------------------------
%

% ********************************************************************************
\section{Adversarial Learning}\label{sec:adv-learning}
% ********************************************************************************
\cite{GooAbaMirXuetal14}
% --------------------------------------------------------------------------------
\subsection{Move Order and Strategic Equilibria} 
% --------------------------------------------------------------------------------
Since the strategies are not finitely spanned, the
minimax theorem does not hold and the very idea of an ``equilibrium'' becomes tenuous.

% \import{background/pc}{al.tex}

% ********************************************************************************
\section{ Semi-supervised Learning }\label{sec:SemiSL}
% ********************************************************************************
 
Given that this thesis references semi-supervised learning (SemiSL)
\footnote{
Since self-supervised learning also features prominently in this thesis (primarily in Chapter 5) we
must depart from the typical initialism, SSL, so as to be able to differentiate the two learning
paradigms.
}
in its title, it is only appropriate that a part of this background section be devoted to the
topic.
%
However, we note that the methods introduced in this thesis are not tailored for the typical SemiSL
regime wherein one hopes to draw upon a large corpus of unlabelled data to shore up the paucity of
annotated data available for direct supervision, with the assumption being that the unlabelled and
labelled data are drawn from the same distribution.
%
Rather, the theme across the constituent papers is how one can use unlabelled data to redress
representational bias and failures in out-of-distribution generalisation. 
%
The problem setups considered thus more closely align with those found in DG and DA; we have
afforded more attention to those topics for this reason.

We would refer the reader to \cite{chapelle2009semi} for excellent (in both clarity and depth)
exposition of theory underpinning SemiSL learning and methods in the pre-deep-learning era, a book
we will reference extensively throughout this brief overview of the topic.
%
For a for comprehensive and current survey of SemiSL methods in the post-deep-learning, on the
other hand, we refer the reader to \cite{yang2022survey}.

%
References to consider:
\cite{chapelle2009semi, scholkopf2021toward, lienen2021credal, sohn2020fixmatch, tarvainen2017mean,
blum1998combining, gong2021alphamatch}
%

% --------------------------------------------------------------------------------
\subsection{Cluster assumption.}\label{ssec:cluster-assumption}
% --------------------------------------------------------------------------------
%
% --------------------------------------------------------------------------------
\subsection{Low-density-separation assumption.}\label{ssec:lds-assumption}
% --------------------------------------------------------------------------------
%
% --------------------------------------------------------------------------------
\subsection{Smoothness assumption.}\label{ssec:smoothness-assumption}
% --------------------------------------------------------------------------------
% --------------------------------------------------------------------------------
\subsection{
  Causal connections: when should/shouldn't SemiSL work?
}\label{ssec:semisl-causality}

Following \cite{scholkopf2021toward}, start by supposing that our prediction task follows the
causal factorisation \( X \to Y\), i.e. it is a causal, rather than an anticausal, one.
%
As discussed before, the ICM principle states that modules in a joint distribution's causal
decomposition do not inform or influence one another, i.e. \(\gI(X, Y) = 0 \); this implies that in
the when \(X\) is the causal parent to \(Y\), as in the supposed case, a better estimate of
\(P(X)\) does not yield a better estimate of \(P(Y|X)\) and it is the former that SemiSL
compasses to learn using unlabelled data.
%
However, that is not to say that SemiSL as in its totality is misguided, it merely requires the
right condition to be met, that condition (which applies to a wide-range of real-world problems)
being the contrary factorisation, \(Y \to X \), which is to say that that the task under
consideration is anticausal.
%
In this case, \(X\) can contain information about the labelling mechanism, as \(X\) is now the
effect, and \(Y\) is now the cause opening up the possibility of exploiting dependencies in the
marginal distribution to better estimate the conditional distribution; in
\cite{scholkopf2012causal} the authors test corroborate this hypothesis.
%
While perhaps often overlooked, this requirement, in fact, well-aligned with the motivating
arguments for SemiSL that we discussed at the beginning of this section.
%
The \emph{cluster assumption} predicates that points belonging to the same cluster in \(P(X)\)
abide by the same labelling mechanism; 
%
the \emph{low-density} assumption predicates that the region in which \(P(Y|X)\) is maximally
entropic (defining the decision boundary) should have low \(P(X)\), or, by invocation of Bayes'
Theorem, low \( \frac{ P(X|Y)P(Y) }{ P(Y|X) } \); 
%
the \emph{smoothness assumption} predicates that if two inputs in a high-density region are close,
then their respective images (under the model) also should be -- this can be viewed as imposing a
kind of \(K\)-Lipschitz or \(\epsilon\)-isometric constraint on our function class;
%
notice that in all three of these cases the causal factorisation is implied to be \(Y \to X \).
%
The celebrated co-training theorem \citep{blum1998combining} similarly respects this precondition
of anticausality in assuming that the co-trained predictors are conditionally independent given the
label, as one have if the label were the cause (the causal parent in the bivariate causal graph).
%
Points from \cite{castro2020causality} to integrate into the above discussion:
\begin{itemize}
        \item One of the notorious challenges in medical image analysis is that of scarcity of
          labelled data, owing to the high costs (lab tests) and expertise required  for
          annotation.
        \item In many cases, establishing the causal direction between inputs and prediction
          targets is non-trivial, particularly if crucial metadata is missing.
        \item Strong requirements need to be bet for SemiSL to be fruitful, namely, the
          distribution of inputs needs to carry information relevant to the prediction task.
          %
          This idea is typically articulated in terms of specific assumptions about the data:
          similar inputs are likely to have similar labels (smoothness assumption) and will
          naturally group into clusters with high density in the input features space (cluster
          assumption). Lower density regions in that space in-between clusters are assumed to be
          ideal candidates for fitting decision boundaries.
          %
          In this context, considering large amounts of unlabelled data together with scarce
          labelled data may reveal such low density regions.
          %
          Note how this idea insinuates and interplay between the distribution of inputs, \( P(X)
          \), and the conditional distribution \( P(Y|X) \). 
          %
          Recall that by the ICM principle, if the prediction task is causal, then the marginal
          distribution \( P(X) \) is uninformative \wrt{} \( P(Y|X) \) and SSL is theoretically
          futile.
          %
          Since typical semantic segmentation tasks are causal, there is likely very little hope
          that semantic segmentation can fundamentally benefit from unlabelled data.
          %
          A model trained on image-derived annotations will attempt to predict some pre-imaging
          ground-truth. 
          It is plausible that seeing more raw images without corresponding anatomical information
          provides no new insight about the labelling mechanism.
          %
          Conversely, if \( Y \to X \), as for skin lesions, then these distributions may be
          dependent and semi-supervision has a chance of success.
          %
          That is not to say that SSL is completely  useless for causal tasks; there can be
          practical algorithmic benefits.
          %
          Namely, under certain conditions, unlabelled data can be shown to have a regularising
          effect and may reduce the amount of labelled data required to achieve a given performance
          level \citep{chapelle2009semi}
          %
          A recent empirical study \citep{oliver2018realistic} reported that properly-tuned
          supervised models, either pretrained on trained on only the task-relevant data, are often
          competitive with or outperform their semi-supervised counterparts. Also shown here was
          the potential for SemiSL to harm performance under label shift.
          %
          ``
          We find that the performance of simple baselines which do not use unlabelled data is
          often under-reported, SSL methods differ in sensitivity to the amount of labelled and
          unlabelled data, and performance can degrade substantially when the unlabelled dataset
          contains out-of-distribution examples
          %
          \begin{itemize}
            \item When given equal budget for tuning hyperparameters, the gap in performance
              between using SSL and using only labelled data is smaller than typically reported.
              %
            \item A large classifier with carefully chosen regularization trained on a small
              labeled dataset with no unlabelled data can reach very good accuracy.
              %
            \item This demonstrates the importance of evaluating different SSL algorithms on the
              same underlying model.
              %
            \item In some settings, pre-training a classifier on a different labelled dataset and
              then retraining on only labelled data from the dataset of interest can outperform all
              SSL algorithms we studied.
              %
            \item Performance of SSL techniques can degrade drastically when the unlabelled data
              con • Different approaches exhibit substantially different levels of sensitivity to
              the amount of labelled and unlabelled data.
          ''
\end{itemize}

% --------------------------------------------------------------------------------

% --------------------------------------------------------------------------------
\subsection{A (brief) taxonomy of semi-supervised methods}
% --------------------------------------------------------------------------------

% ********************************************************************************
\section{ Self-supervised Learning }\label{sec:SelfSL}
% ********************************************************************************

% ********************************************************************************
\section{ Normalising Flows and Invertible Neural Networks }\label{sec:nfs_and_inns}
% ********************************************************************************
