% % ********************************************************************************
\section{Invertible neural networks}\label{sec:inns}
% % ********************************************************************************
Chapter~\ref{ch:nifr} of this thesis explores the application of \acp{INN} to \ac{FRL} and so will
afford some brief discussion to their basic workings here.
%
\sidepar{Bijectivity}
%
An \acl{INN} \citep{kobyzev2020normalizing}, as the name would so suggest, refers to neural
networks for which both the usual forward mapping, \(f(\cdot)\), and its inverse \(f^{-1}(\cdot)\) are
defined, with the assumed property that both are differentiable and as such that the function
belongs to the class of \emph{diffeomorphisms}, \(f \in \text{Diff}(\gX)\). 
%
Thus, we have a function that is an invertible bicontinuous map from input space, \(\gX \subset
\R^d \) to latent space \(\gZ \subset \R^d \), noting that the domain and codomain are
equidimensional, as presupposed by the function's bijectivity.
%
It is obvious, but nonetheless worth stating, that if \(f\) is composed of subfunctions \(f
\triangleq f_L \circ \dots \circ f_2 \circ f_1\) and each individual subfunction is diffeomorphic,
then \(f\) in its totality, also satisfies this property, allowing us to build arbitrarily complex
\acp{INN} by chaining together layers defining these subfunctions.
%

\Acp{INN} are foremost used for their density-estimation -- and by complement, generative modelling
-- capabilities due to their hallmark diffeomorphic property that makes it possible for densities
under the models to be calculated \emph{exactly}, in contrast to variational methods that only do
so up to a lower bound (the so-called \ac{ELBO}). 
%
\sidepar{\Aclp{NF}}
%
This calculation is enabled by the change-of-variables theorem, allowing one to track how the
density of the distribution changes as the \ac{INN} warps a known (and tractable) base distribution
into a complex, highly-multimodal, one.
%
Like with \aclp{VAE} (\acsp{VAE}; \citealp{kingma2014auto}), the base density, \(P(Z)\), is generally
taken to be an Isotropic Gaussian distribution; the posterior density, \(P(X)\), `flows' through
the network -- in a manner reminiscent of a Galton Board -- into this normalised base density,
earning this class of methods the name \aclp{NF} (\acsp{NF}; \citealp{rezende2015variational,
kobyzev2020normalizing}).
%
Practically, for a given sample \(x\), its log-likelihood under the \ac{INN}, \(f\), with base
density \(\gN(\cdot; 0, \sI)\) the aforementioned Gaussian distribution, can be computed as
%
\begin{equation*}
%
    \begin{aligned}
        %
        &\log P(X=x) = 
        %
        \log P(Z=z) + \sum_{l=1}^L \log \left| \det\left( \frac{\diff f_l}{ f_{l-1}}\right)
        \right|, \\
        %
        &P(Z=z) = \gN(z; 0, \sI),
        %
    \end{aligned}
\label{eq:cov}
%
\end{equation*}
%
and training the model simply amounts to maximising this quantity over the empirical training
distribution in the usual fashion.
%
As with \acp{GAN} and \acp{VAE}, to sample from \(P(X)\), one needs only draw a random sample from
the corresponding base density, \(z \sim P(Z) \), and push that sample through \(f\).


\sidepar{Losslessness}
%
In Chapter~\ref{ch:nifr}, however, it is not the foregoing density-estimation capabilities of
\acp{INN} that we are interested in, rather the diffeomorphic property itself, insofar as it
guarantees the learned representations are \emph{lossless} \wrt{} the inputs, as well as a means to
visualise the factors of said representations due to its having an exact inverse (whereas
auto-encoders have only an approximate inverse (the decoder) that must be trained separately from
the encoder).
%
That is to say, while \(f\) may deform manifold \(\gX\) in arbitrarily non-linear ways, since each
point is mapped uniquely from the domain to codomain only the form of the information contained in
the input can change, not its extent. 
%
This is in contrast to conventional architectures that define \emph{surjective} mappings that embed
inputs into spaces much smaller than \(\R^d\) (in line with the \emph{Manifold Hypothesis}
\citep{fefferman2016testing}).
%
Other works have also capitalised on this information-preservation explicitly, e.g.\ both
\citet{hoogeboom2019integer} and \citet{xie2021enhanced} explore the natural suitability of
\acp{INN} for lossless image compression.
%
Contrastingly, in work postdating that done in Chapter~\ref{ch:supmatch}, \acp{NF} have been
applied to \ac{AF} problems with the insight that one can leverage the exact-density computation to
define an optimal adversary \citep{balunovic2021fair, cerrato2022fair}. 
%
This allows for obtaining provably fair representations while also obviating the optimisation
challenges that accompany (parametric) adversarial training, though at the cost of an independent
\ac{INN} for each of the sensitive groups.

\sidepar{Practical drawbacks of INNs}
%
As discussed, \acp{INN} have a number of unique and compelling properties that would seem to make them
the choice method for many generative purposes; \acp{INN} do have their share of practical
shortcomings, however. 
%
Notably, bijectivity does not come without cost; while there are some ways of mitigating it, such as
factoring out parts of the representation at intermittent steps \citep{hoogeboom2019integer}, one
is constrained to having a latent space that is equidimensional to the input space and when the
latter is large, as in the case of images, training an \ac{INN} can be computationally challenging.
%
Conventional (non-invertible) architectures do not suffer this problem as they can make free use of
coarsening (downsampling) operations throughout their extent.
%
This drawback is further compounded by the fact that the layers making up an \ac{INN} are
necessarily less expressive than their non-invertible counterparts, and thus more of them are
needed to achieve comparable levels of expressiveness in composition. 
%
The coupling layers that constitute the atomic building blocks in \citet{dinh2014nice} restrict
their non-linear, non-invertible functions, to only a subset of the input dimensions at a time so
that the layer as a whole remains invertible, thus limiting the degree to which interdependencies
between the input dimensions can be modelled.
%
Finally, without proper parametric constraints (e.g.\ bidirectional \(K\)-Lipschitzness), the
optimisation of \acp{INN} can be prone to instabilities that can render them \emph{numerically}
non-invertible, despite their design, and thereby invalidate computations made according to
Eq.~\ref{eq:cov} \citep{behrmann2021understanding}.
