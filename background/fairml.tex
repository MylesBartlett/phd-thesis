% ********************************************************************************
\section{Fair Machine Learning (FairML)}\label{sec:fairml}
% ********************************************************************************
%
While there are various strands of FairML defining different criteria for what it means for a
predictor to be \emph{fair}, in this thesis statistical group definitions of fairness
\citep{barocas2019fairness}, where `group' corresponds to some demographic group, such as gender or
ethnicity, that is to be \emph{protected}.
%
We will denote group membership using the random variable \( S: \Omega \to \gS \) -- which are
observed as \(s\) -- and will assume that this variable along with the target variable are
discrete; this is the most common setup considered in the FairML literature though there are works
that extend notions of, and methods for enforcing, fairness to continuous settings (\wrt{} both
\(S\) and \(Y\).
%

The term bias is often used to refer to demographic disparities in algorithmic systems that are
objectionable for societal reasons.

\subsection{Metrics for detecting bias}\label{ssec:bias-metrics}
%
\subsubsection{Independence}
%
For binary classification, one measure popular independence is \emph{demographic parity} (DP;
\cite{zemel2013learning, feldman2015certifying}),
known also as statistical parity, group fairness, disparate impact, inter alia. 
%
While \(s\) is commonly binary itself, DP can be generally defined for categorical variables as the
requirement that the probability of a positive prediction should be equal (at parity) across all
demographic groups. 
%
This is equivalent to requiring, according to the standard notion of statistical independence, that
the conditional and marginal distributions be equal, i.e.
%
\begin{equation}
    P( \hat{Y} = 1 | S) \seq P( \hat{Y} = 1 )
\end{equation}
%
where \(\hat{Y}\) denotes the random variable corresponding to the predictions of a given
predictor, \(f\) .
%
Since requiring that this condition be satisfied exactly is generally overly strict, it is common
to introduce some slack factor, \(\epsilon\), that defines the feasible region for a relaxation of
the constraint:
%
\begin{equation} \forall s \in \gS: P(\hat{Y}=1|S=s) P(S=s)
    %
    \in 
    %
[ 1 - \epsilon, 1 + \epsilon ]. \end{equation}
%
In practice, we can then measure fairness (one notion of it, at least) of a predictor by evaluating
by how much it violates this condition, either in terms of differences (DPDiff) or ratios
(DPRatio), which in non-binary cases can be computed pairwise and then optionally summarised by
taking the maximum over the resulting set.

The above notion of (statistical) independence can be expressed generally  in terms of the mutual
information (MI) between \(Y\) and \(S\), defined as the Kullback-Leibler (KL) divergence between
the joint and product of the marginal distributions
%
\begin{equation}
    \gI(Y;S) \triangleq D_{KL} \Bigl( P(Y, S) \Vert P(Y) \otimes P(S) \Bigr).
\end{equation}
%
If, and only if, \( \gI(Y;S)\), \( \gI(Y;S) = 0  \) can the random variables said to be
statistical independent.
%
MI admits various decompositions into sums of marginal and joint/conditional entropies that make it
particularly amenable to optimisation. 
%
In invariant representation learning (encompassing fair representation learning, domain adaptation,
and domain generalisation), for example, a common method for imparting independence, \(Z \perp S \)
-- which is sufficient for \(Y \perp S\), given a predictor head, \(c: \gZ \to \gY \) -- between
the representations learned by encoder \(g: \gX \to \gZ \) and the sensitive attribute to train
\(g\) to maximise the conditional entropy \( H(\hat{S}|Z) \) generated by an adversarial predictor,
\(a: \gZ \to \bigtriangleup^{|\gS|}\) (trained via MLE to maximise \( P(\hat{S}|Z) \)

%
% ------------------------------------------------------------------------------ 
\subsection{Going beyond the fairness-accuracy trade-off with minmax group
fairness}\label{ssec:minmax-fairness}
% ------------------------------------------------------------------------------ 

Some well-known definitions of group fairness attempt to make algorithms whose predictions are
independent of the sensitive populations \citep{zemel, feldman2015certifying}, or algorithms whose
outputs are independent of the sensitive attribute given the ground-truth (Equality of Opportunity,
Equal Odds; \cite{hardt2016equality, woodworth2017learning}).
%
Notions of individual fairness have also been proposed \citep{dwork2012fairness}.
%
These can be appropriate in many scenarios, but in domains where quality of service is paramount,
such as healthcare, it is necessary to strive for models that are close to fair as possible without
introducing unnecessary harm \citep{ustun2019fairness}.

Alternative: group fairness in terms of predictive risk disparities, i.e. \emph{minimax group
errors}, as originally proposed by \cite{martinez2020minimax} in the context of fair
classification.
%
Minimax group fairness has the noteworthy property that any model achieving it is guaranteed to
Pareto dominate an equalised-error model \wrt{} group error rates.
Equalised errors can only be achieved by deliberately inflating the error of one or more groups in
the minimax solution.
%
Put another way, one technique for finding a solution that is optimal in terms of equalised error
rates is to first find a minimax solution and then to artificially  inflate the group error rate on
any group that does not saturate the minimax constraint.

Optimality is often discussed in the fairness literature but often in the context of
accuracy-fairness trade-offs \citep{kearns2018preventing, kearns2019ethical}.
The conflict between fairness and optimality has been previously studied in
\cite{kaplow1999conflict}.
%
Formulate fairness as a Multi-objective optimisation (MOO) problem and use Pareto optimality
\citep{sawaragi1985theory} to
define the set of all efficient classifiers, meaning that the increase in predictive risk in one
group is concomitant with a decrease of the predictive risk in another group.
Goal is to find the classifier with the smallest maximum group risk among all efficient models.
This implies that a system's risk is as good as its worst-group performance -- do not enforce
zero-risk disparity if the disadvantaged group does not benefit from it.

Equality of error rates (accuracy) is one of the most intuitive and well-studied group fairness
notions, and in enforcing it one often implicitly hopes that higher error rates on disadvantaged
groups will be reduced towards that of the majority group.
In practice, however, equalised error rates and similar notions may require artificially inflating
error on easier-to-predict groups, without necessarily decreasing the error rate for the
harder-to-predict of the groups. 
%
This is undesirable in many social applications, for instance:
\begin{enumerate}
        \item predicting domestic situations where children might be at risk of physical or
            emotional harm. If equality of error rates can only be improved by error rates for the
            advantaged group then arguably we will have only worsened overall social welfare, since
            this is not a Robin Hood scenario of ''taking from the rich and giving to the poor``
        \item predictive modelling in healthcare
\end{enumerate}

In such scenarios, it may be preferable to consider the alternative fairness criterion of minimax
group error in which one seeks to minimise the largest error rate over the groups -- that is, to
make the worst-off group as well-off as possible.

%
A metric that has risen in popularity in both DG and FairML (under the minmax perspective) alike --
both for evaluation and as a quantity to be optimised for by proxy -- is \emph{Robust Accuracy}.
%
For convenience and clarity, we first restate standard accuracy (Acc) as 
%
\begin{equation}
    \text{Acc}(f, \gD^{eval}) \triangleq 
    \E_{ (x, y) \in \gD^{eval}}[ \delta_{f(x)y} ],
 \end{equation}
%
with \( \gD^{eval} \) the dataset over which the metric is being computed, \( \delta \) the
Kronecker delta function that evaluates to \(1\) under equality of \(f(x)\) and \(y\) (and \(0\)
otherwise).
%
The robust version (RobAcc) is then simply the minimum accuracy computed over all subsets of \(
\gD^{eval} \) created by conditioning on each \(s \in \gS \), that is
%
\begin{equation}
    % \text{RobAcc}(f, \gD^{eval}) \triangleq 
    % \underset{s \in \gS}{\text{min}} \,
    % \E_{ (x, y) \in \gD^{eval}_{S=s} }[ \delta_{f(x)y} ],
    \text{RobAcc}(f, \gD^{eval}) \triangleq 
    \underset{s \in \gS}{\text{min}} \, \text{Acc}(f, \gD^{eval}_{S=s}),
 \end{equation}
%
 with \( \gD^{eval}_{S=s} \) denoting the \(s\)th one of such subsets.


It is perhaps obvious, yet well-established theoretically, that a predictive distribution cannot
simultaneously satisfy all three standard notions of fairness dictated by DP, EqOp and EO
\cite{kleinberg2016inherent}.
 %
Furthermore, in general, satisfying a given fairness criterion based on equalised rates connotes a
trade-off in utility (typically measured in accuracy (or its complement, error rate)).


 texts to cite and discuss: \cite{saravanakumar2020impossibility, zhao2022inherent,
 kallus2018residual, kkku}


% \subsection{Notes}
% Notes from \cite{saravanakumar2020impossibility}:
 \\begin{itemize}
          \item A report published by ProPublica (Angwin et al., 2016) revealed that the automated
              recidivism prediction system COMPAS was biased against the African-American community
              in its predictions 

  \end{itemize}
% Notes from \cite{barocas2019fairness}:
% %
% Amazon uses a data-driven system to determine the neighborhoods in which to offer free same-day
% delivery. A 2016 investigation found stark disparities in the demographic makeup of these
% neighborhoods: in many U.S. cities, White residents were more than twice as likely as Black
% residents to live in one of the qualifying neighborhoods
% %
% Some patterns in the training data (smoking is associated with cancer) represent knowledge that we
% wish to mine using machine learning, while other patterns (girls like pink and boys like blue)
% represent stereotypes that we might wish to avoid learning. But learning algorithms have no general
% way to distinguish between these two types of patterns, because they are the result of social norms
% and moral judgments. Absent specific intervention, machine learning will extract stereotypes,
% including incorrect and harmful ones, in the same way that it extracts knowledge.
% %
% Another common reason why machine learning might perform worse for some groups than others is
% sample size disparity. If we construct our training set by sampling uniformly from the training
% data, then by definition we’ll have fewer data points about minorities. Of course, machine learning
% works better when there’s more data, so it will work less well for members of minority groups,
% assuming that members of the majority and minority groups are systematically different in terms of
% the prediction task.
% %
% Worse, in many settings minority groups are underrepresented relative to population statistics. For
% example, minority groups are underrepresented in the tech industry. Different groups might also
% adopt technology at different rates, which might skew datasets assembled form social media. If
% training sets are drawn from these unrepresentative contexts, there will be even fewer training
% points from minority individuals.
% %
% When we develop machine-learning models, we typically only test their overall accuracy; so a “5%
% error” statistic might hide the fact that a model performs terribly for a minority group. Reporting
% accuracy rates by group will help alert us to problems like the above example

% A major limitation of machine learning is that it only reveals correlations, but we often use its
% predictions as if they reveal causation. This is a persistent source of problems. For example, an
% early machine learning system in healthcare famously learned the seemingly nonsensical rule that
% patients with asthma had lower risk of developing pneumonia. This was a true pattern in the data,
% but the likely reason was that asthmatic patients were more likely to receive in-patient care
% \citep{caruana2015intelligible}. So it’s not valid to use the prediction to decide whether or not
% to admit a patient. We’ll discuss causality in Chapter 5.

