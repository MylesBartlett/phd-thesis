% ********************************************************************************
\section{Fair machine learning}\label{sec:fairml}
% ********************************************************************************
Research into algorithmic fairness, FairML, has flourished in recent years, the subfield growing
from what was once an arguably niche one to one of great prominence, borne by ML's proliferation --
and thus increased capacity to affect real lives consequentially -- in all sectors of society.
%
Indeed, a multitude of high-profile cases/studies have highlighted the discriminatory (unfair)
nature of unchecked ML systems -- \cite{kasperkevic2015google}, \cite{angwin2016machine},
\cite{dastin2018amazon}, and \cite{buolamwini2018gender} -- further spurring research to develop
better-aligned algorithms and methods for validating them, and thereby regain public trust.

%
There are many strands of FairML following different criteria for what it means for a
predictor to be \emph{fair}; in this thesis we focus on those corresponding to group definitions of
fairness \citep{barocas2019fairness}, where `group' refers to some demographic group, such as
gender or ethnicity, that is considered \emph{sensitive} or \emph{protected} (these two terms are
often used interchangeably throughout the literature; we will favour the former).
%
We will denote group membership using the random variable \( S: \Omega \to \gS \) -- with
realisation \(s\) -- and will assume that this variable along with the target variable are discrete
(and in most cases binary); this is the most common setup considered in the FairML literature --
and toward which many standard metrics of fairness are geared \citep{feldman2015certifying,
hardt2016equality, woodworth2017learning} -- though there are works that extend notions of -- and
methods for enforcing -- fairness to settings with categorical and continuous (\( S \) and \( Y \))
attributes.
%
Though the focus is on group fairness, at the tail-end of the section, we will touch briefly on the
premise of individual fairness \citep{dwork2012fairness} to take the opportunity to draw a parallel
between it and the idea of consistency/smoothness in SemiSL.

%
Particularly, we will consider two particular kinds of group-oriented notions of fairness: 1) the
family of notions predicated on equalised rates, imposing constraints on the predictive
distribution; 2) the notion of \emph{minimax fairness} predicated on maximising the worst-group
performance, and which imposes no constraints on the relative performance between groups.
%
It is interesting to note that, from an optimisation perspective, the latter partially subsumes the
former: by solving its entailed problem, one can then readily solve the former by artificially
inflating the error on the advantaged group (systematically flipping the predicted labels until the
relevant constraint on the predictive rates is met).

%

Simply put, the unified goal of FairML is to learn some predictor, \(\Gamma: \gX \to \gY \) that
is, according to some definition, non-discriminatory -- that does not deprive a given individual of
opportunities by dint of belonging to a particular sensitive group.
%
A frequently-invoked example is that of financial institution employing an automated
decision-making system to determine which loan applicants should/should not have their applications
approved.
%
In addition to financial information (e.g. credit-score history) the input features to \(\Gamma\),
\(\gX\), may encode, explicitly or implicitly, sensitive information such as an individual's gender
or ethnicity, with the general assumption being that such information is acausal to the task at
hand (one's gender should have no impact on one's eligibility for a loan).
%
By `implicitly`, we mean that such information may be inferable (or predictable with above random
accuracy) from other features, such as one's location, housing history, or level of education, such
that solving problems of fairness is not so straightforward as simply excising the elements of
\(\gX\) that directly correspond to the \(\gS\) -- an approach referred to as \emph{Fairness
Through Unawareness} (FTU).
%

\subsection{Equalised rates}\label{sec:equalised-rates}
%
A long-standing and vigorously-debated problem in FairML spheres is how exactly one should define
the concept of `fairness`; indeed what constitutes a `fair` decision depends both on the
(individual, social, or institutional) value system and the context in question.
%
Much of the fairness literature has focussed on notions of fairness based on enforcing the
predictor to output equal rates (e.g. of positive or correct predictions) across the sensitive
groups.
%
Here, we briefly discuss and formulate \emph{Demographic Parity}, \emph{Equal of Opportunity}, and
\emph{Equalised Odds} as the standard triad of metrics based on this tenet of equalisation of
inter-group rates, and do so for the binary-classification regime to which they are most commonly
applied, even though generalisations exist \citep{woodworth2017learning}. 
%
These metrics naturally share similarity of form, only differing in their conditioning and bear
direct relevance to Chapter 3, the constituent paper of which being fairness-oriented and adopting
these metrics as measures of invariance, though the method is applicable to spurious correlation
problems in general, decontextualised from fairness.
%
Chapter 4 adopts a perspective in line with the domain generalisation literature
\cite{sagawa2019distributionally}, a perspective which is known under the guise of \emph{minimax
fairness} in the FairML literature.
%
While perhaps obvious, it is relevant to note that that a predictive distribution provably cannot
simultaneously satisfy all three notions of fairness dictated by DP, EqOp and EO
\citep{kleinberg2016inherent}.
%
It is also relevant to note that these metrics presuppose scenarios in which the budget, the pool
of allocatable resources, is limited (a cap on the number of grantable loans or the number of
people that can be employed, for example), such that any resources allocated to an individual of
one group are concomitantly being withheld from the other group(s) -- fairness in this context thus
corresponds to a type of resource allocation problem, from an econometric perspective.
%

\marginpar{\textbf{Demographic Parity}}
%
The simplest of the triad, \emph{Demographic parity} (DP; \cite{zemel2013learning,
feldman2015certifying}) -- known also as statistical parity, group fairness, disparate impact,
inter alia -- demands that the probability of a positive prediction (positive rate) be uniform
(at parity) across all sensitive groups. 
%
That is
%
\begin{equation}
    \forall s \in \gS: P( \hat{Y} = 1 | S = s ) \seq \text{constant},
\end{equation}
%
where \(\hat{Y}\) denotes the random variable corresponding to the predictions of a given
predictor, \(f\), and `constant` denotes some placeholder constant value, noting that this could be
any arbitrary value and does not take into account utility, such that a majority or randomly
classifier would degenerately satisfy the condition. 
%
This above constraint is equivalent to requiring, according to the standard notion of statistical
independence, namely the equality of the conditional and marginal distributions, i.e.
%
\begin{equation}
    P( \hat{Y} = 1 | S) \seq P( \hat{Y} = 1 ).
\end{equation}
%
%
Since requiring that this condition be satisfied exactly is generally overly strict when doing
constrained optimisation, it is common to introduce some relaxation factor, \(\epsilon\), that
expands the constraint to a feasible region so that with some minor rearrangement we may instead
write:
%
\begin{equation}
    \forall s \in \gS: P(\hat{Y}=1|S=s) P(S=s) \in [ 1 - \epsilon, 1 + \epsilon ]. 
%
\end{equation}
%
We can measure fairness (one notion of it, at least) of a predictor by evaluating by how much it
violates this condition (or any of the conditions in this section), either in terms of differences
or ratios  which in non-binary cases can be computed pairwise and then optionally summarised by
taking the maximum over the resulting set.

%
\marginpar{\textbf{Mutual-information minimisation}}
%
This idea of statistical independence, upon which DP hinges, can be expressed generally in terms of
the mutual information (MI) between \(Y\) and \(S\), itself expressible as the KL divergence
between the joint and product of the marginal distributions:
%
\begin{equation}
    \gI(Y;S) \triangleq D_{KL} \Bigl( P(Y, S) \Vert P(Y) \otimes P(S) \Bigr).
\end{equation}

Iff \( \gI(Y;S)\), \( \gI(Y;S) = 0  \) can the random variables said to be statistical independent.
%
MI admits various decompositions into sums of marginal and joint/conditional entropies that make it
particularly amenable for optimisation purposes. 
%
In invariant representation learning (encompassing fair representation learning, domain adaptation,
and domain generalisation), for example, a common method for imparting independence, \(Z \perp S \)
-- which is sufficient for \(Y \perp S\), given a predictor head, \(c: \gZ \to \gY \) -- between
the representations learned by encoder \(g: \gX \to \gZ \) and the sensitive attribute to train
\(g\) to maximise the conditional entropy \( H(\hat{S}|Z) \) generated by an adversarial predictor,
\(a: \gZ \to \bigtriangleup^{|\gS|}\) (itself trained via MLE).

%
\marginpar{\textbf{Equality of Opportunity}}
%
Equality of Opportunity (EO) relaxes this desideratum of unconditional statistical independence,
\(\hat{Y} \perp S\), to one of \emph{separation} (or conditional independence), dictating that
\(\hat{Y}\) and \(S\) need only be independent conditioned on the ground-truth label, \(Y\) (albeit
only in the positive case).
%
To phrase this conversely: whenever the outputs of our predictor are dependent on \(S\), such must be
justified by a dependence on \(Y\) for the predictor to be a fair one.
%
Thus, EO can be written, as above, as
%
\begin{equation}
    %
    \forall s \in \gS: P( \hat{Y}=1|Y=1, S=s ) = P(\hat{Y}=1|Y=1),
    %
\end{equation}
%
\marginpar{\textbf{Equalised Odds}}
%
By making the above symmetric \wrt{} \(Y\), such that not only do we demand parity of the TPRs but
also false-positive rates (FPRs) we obtain the final of the ERs-based metrics, \emph{Equalised
Odds} (EqOd; \cite{hardt2016equality}) -- also known as disparate mistreatment:
% Plainly, EqOd is tantamount to requiring that no additional information about \(S\) should be
% gleanable from observing \(\hat{Y}\) given that we have already observed \(Y\). 
% %
%
\begin{equation}
    %
    \forall y s \in \gY, \forall s \in \gS: P( \hat{Y}=1|Y=y, S=s ) = P( \hat{Y}=1|Y=y )
\end{equation}
%
\marginpar{\textbf{Fairness-accuracy trade-off}}
%
It has long been understood that there exists an inherent trade-off between the utility, as
measured by aggregate performance and fairness under conceptions of fairness based on ERs
\citep{kaplow1999conflict}; with accuracy being the principal measure of said utility in the
context of classification, this trade-off is commonly referred to as the \emph{accuracy-fairness
trade-off}, though as we will see in the next subsection, such a trade-off does not apply to
fairness in general, as in the case for notions of fairness defined by \emph{minimax fairness}
where one seeks to maximise worst-group utility rather than group parity.
%
Given the existence of such a trade-off, the problem of learning a useful classifier subject to ER
constraints induces not one optimal solution (or one equivalence class of optimal solutions, more
accurately) -- as one would have when focussed on only the utility -- but rather a set of
\emph{Pareto optimal} solutions, the discovery of which is the remit of multi-objective
optimisation (MOO; \cite{sawaragi1985theory, deb2013multi}). 
%
MOO has appeared both implicitly and explicitly throughout the FairML literature, the latter only
relatively recently (e.g. in \cite{navon2020learning}). 
%
Indeed, examples of the former include the methods of \cite{louizos2015variational} and
\cite{madras2018learning} which entail learning a \emph{linear scalarised} solution
\citep{boyd2004convex},  with position of this solution on the \emph{Pareto frontier} controlled by
a linear weighting of loss terms optimising for utility and fairness separately.
%
% ------------------------------------------------------------------------------ 
\subsection{Going beyond the fairness-accuracy trade-off with minimax group
fairness}\label{ssec:minimax-fairness}
% ------------------------------------------------------------------------------ 
The foregoing notions are as intuitive as they are well-studied, and there is a wide
range of applications to which they may be reasonably used as a lodestone for fairness.
%
However, the fact that they require degrading performance of the advantaged groups is problematic
for applications where the quality of service -- the utility of the model -- is cardinal, or --
phrased econometrically -- scenarios not characterised by limited resources  and thus to which the
`Robin Hood' principle of `stealing from the rich to give to the poor` is inapplicable.
%
Healthcare defines a whole host of applications to which this consideration applies, where
accurately detecting positive cases can be a matter of life-or-death, and any marked degradation in
this respect in the name of fairness is unacceptable. 
%
Rather than satisfying constraints on predictive parity, a more reasonable tact in such scenarios
is instead to aim to maximise fairness while incurring minimal (ideally, no) degradation in the
performance on any given group \citep{ustun2019fairness}.
%
This is the remit of \emph{minimax fairness}, as originally formulated by
\citep{martinez2020minimax} in Pareto-optimal terms, though the same idea has long existed in
distributionally robust optimisation (DRO), and an idea which has been inherited by domain
generalisation as an instantiation of DRO. 
%

The `distributionally robust' part of DRO corresponds to the desire to find a solution that works
well not on only a single distribution, or particular instantiation of a problem, but that works
well over a range of proximal distributions/problems (also referred to as a \emph{perturbation
set} in some texts \citep{ben2009robust}). 
%
This desire naturally arises in any regime which naturally contends with some kind of meta
distribution -- a distribution over distributions -- such as meta-learning \citep{collins2020task},
domain generalisation (as already noted; \cite{sagawa2019distributionally}), or, indeed, fairness.
%
Given a meta distribution \(\mathfrak{P}(X, Y)\), from which we sample the bivariate distributions \(
P(X, Y) \), the minimax (ERM-based) DRO objective can be expressed as 
%
\begin{equation}
    \underset{\Gamma}{\text{inf}} 
    \underset{P(X, Y) \sim \mathfrak{P}(X, Y)}{\text{sup}}
    \E_{(X, Y) \sim P(X, Y)}[\gL(\Gamma(X), Y) ],
 \end{equation}
%
recalling that we use \(\gL(\cdot, \cdot)\) to denote a loss function of random variables.
%
Thus, in contrast to standard ERM, which would optimise over the `flattened' meta distribution, the
DRO objective defines a bilevel optimisation problem in which only the supremum of the divergence
over \( \mathfrak{P}(X, Y) \) contributes to the overall objective to be minimised by our
predictor, \(\Gamma\).
%
One can view this as a sparse form of IW-ERM where the weighting function is the indicator function
returning \(1\) if a sample belongs to the `worst` (most divergent) distribution and \(0\)
otherwise.

%
For evaluation of classification tasks, one can align the standard notion of accuracy with minimax
fairness by conditioning on the sensitive attributes and taking the minimum (worst) over the
resulting set of \(|\gS|\) values. 
%
In the domain-generalisation/group-robustness literature this quantity is commonly known as
\emph{Robust Accuracy} (RobAcc).
%
For convenience and clarity, we first restate standard (non-conditional) accuracy as 
%
\begin{equation}
    \text{Acc}(f, \gD^{eval}) \triangleq 
    \E_{ (x, y) \in \gD^{eval}}[ \delta_{f(x)y} ],
 \end{equation}
%
with \( \gD^{eval} \) denoting the dataset over which the metric is being computed, \( \delta
\) Kronecker delta function that evaluates to \(1\) under equality of \(f(x)\) and \(y\) (and \(0\)
otherwise).
%
The `robust' version (RobAcc) is then simply the minimum accuracy computed over all subsets of \(
\gD^{eval} \) created by conditioning on each \(s \in \gS \)
%
\begin{equation}
    \text{RobAcc}(f, \gD^{eval}) \triangleq 
    \underset{s \in \gS}{\text{min}} \, \text{Acc}(f, \gD^{eval}_{S=s}),
 \end{equation}
%
 with \( \gD^{eval}_{S=s} \) denoting the \(s\)th one of such subsets.

% ------------------------------------------------------------------------------ 
\subsection{Individual fairness}\label{ssec:individual-fairness}
% ------------------------------------------------------------------------------ 
While notions of group fairness consider fairness at the level of demographic groups,
\emph{individual fairness} focusses -- as expected of the name -- on fairness at the individual
level, and may be pithily summed up with the apophthegm `similar individuals should receive similar
treatments'.
%
As alluded to before, the premise of individual fairness greatly resembles the smoothness
assumption from SemiSL. 
%
We mentioned in its associated section that the smoothness assumption can be seen as a
\(K\)-Lipschitz constraint on our function class and this is the manner in which the \emph{Fairness
Through Awareness} (FTA) -- the most general concept of individual fairness -- is couched in
\cite{dwork2012fairness}, the name standing in contrast to FTU.
%
The central question implied by FTA then is what constitutes an appropriate measure of similarity
-- in the input and output spaces separately -- for the population and task under consideration.

% ------------------------------------------------------------------------------ 
\subsection{Bias propagation and systematic censoring}\label{ssec:residual-unfairness}
% ------------------------------------------------------------------------------ 
We conclude this section with a brief discussion of the residual unfairness, as termed by
\cite{kallus2018residual}, and the problem setting giving rise to it, owing to its pertinence to
both Chapter 3 and Chapter 4.
%
Residual unfairness refers to lingering inter-group disparities, stemming from sampling bias -- in
the fairness-contextualised sense of resulting from prejudiced historical policies engendered by
limited initial data, heterogeneous decision-makers, or statistically discriminatory rules -- after
attempts to correct for those disparities, due to mismatches between the training and test
populations.
%
When data collected subject to such a mechanism is used to inform a decision policy -- automated or
otherwise -- that then informs future policies, the enforcer is liable to creating positive
(self-reinforcing) feedback loops that lead to the progressive amplification of already-grievous
systemic biases, to the extent of \emph{systematically censoring} certain outcomes for certain
demographics.

%
To ground this, consider a recruitment policy giving preferential treatment to male software
developers (such that men are significantly more likely to be hired than women) by virtue of men
historically preponderating over women in the technology sector.
%
Crucially, the nature of the process means that one only observes outcomes -- performance metrics
-- for those individuals that are hired, i.e. we do not have access to counterfactual (what would
have been had \(A\) happened instead of \(B\)) information that would provide an avenue for natural
equilibration.
%
Over several recruitment cycles with `dynamics' governed by the aforementioned policy -- and
potentially updates to that policy incorporating data from new hirees -- one would expect to end up
with a training population remote from that of the `true' population, potentially to the point of
women vanishing -- being censored -- from the pool of hirees, while the rejected pool consists of a
mixture of men and women.
%
\cite{kallus2018residual} show that in such cases, enforcing fairness on the training distribution
provably does not guarantee fairness on the true population.






% We consider a model of biased data where systematic censoring affects whether or not entire
% observations appear in the training dataset. In such cases, the available data are not
% representative of the eventual realworld “test” population to which any resulting learned policy
% will be applied.

% This important issue arises in almost all settings where fair machine learning has been studied:
% (1) Data on loan default can only be collected on those loan applicants who were historically
% approved but is used to learn approval policies applied to all applicants. (2) Arrest data help
% build predictive policing models but these data are disproportionately collected on individuals in
% highly patrolled areas and may be subject to further prejudice at the individual level, including
% racial (Lum & Isaac, 2016). (3) Risk assessment and intervention tools in child welfare agencies
% are trained on cases which have been “screened in” by caseworkers based on external reports
% (Chouldechova et al., 2018). (4) Defendants may only flee and fail to appear if not detained, so
% any flight risk score used for setting bail can only be learned from data on defendants who were
% not detained. (5) Convict recidivism is impacted by sentence applied but learned risk scores are
% applied to all convicts.

% Such censoring may reflect historical decisions made with limited access to information,
% heterogeneous decision-makers, or the application of statistically discriminatory rules (Arrow,
% 1973). Despite the intermediate screening, domain-level restrictions may require ensuring fairness
% of any decision or prediction policy with respect to the original population. We formalize this
% mechanism as a data setting (Fig. 1) where a historical decision policy Z 2 {0, 1} specifies
% whether an instance will be included in the dataset. Systematic censoring may induce covariate
% shift on population-level estimands, such as true positive rate, as outcomes are observed in the
% training data only where Z = 1.

% Our work characterizes the problem of residual unfairness, which arises when policies learned from
% biased datasets are adjusted for fairness but remain unfair in practice. We study a general setting
% where the dataset is generated under a prejudiced historical policy, which captures the structure
% of many problem settings where fairness has been considered. We prove that the same prejudices will
% be reflected in the supposedly-fairness-adjusted policy


 % texts to cite and discuss: \cite{saravanakumar2020impossibility, zhao2022inherent,
 % kallus2018residual, kkku}


% \subsection{Notes}
% Notes from \cite{saravanakumar2020impossibility}:
 % \\begin{itemize}
 %          \item A report published by ProPublica (Angwin et al., 2016) revealed that the automated
 %              recidivism prediction system COMPAS was biased against the African-American community
 %              in its predictions 

 %  \end{itemize}
% Notes from \cite{barocas2019fairness}:
% %
% Amazon uses a data-driven system to determine the neighborhoods in which to offer free same-day
% delivery. A 2016 investigation found stark disparities in the demographic makeup of these
% neighborhoods: in many U.S. cities, White residents were more than twice as likely as Black
% residents to live in one of the qualifying neighborhoods
% %
% Some patterns in the training data (smoking is associated with cancer) represent knowledge that we
% wish to mine using machine learning, while other patterns (girls like pink and boys like blue)
% represent stereotypes that we might wish to avoid learning. But learning algorithms have no general
% way to distinguish between these two types of patterns, because they are the result of social norms
% and moral judgments. Absent specific intervention, machine learning will extract stereotypes,
% including incorrect and harmful ones, in the same way that it extracts knowledge.
% %
% Another common reason why machine learning might perform worse for some groups than others is
% sample size disparity. If we construct our training set by sampling uniformly from the training
% data, then by definition we’ll have fewer data points about minorities. Of course, machine learning
% works better when there’s more data, so it will work less well for members of minority groups,
% assuming that members of the majority and minority groups are systematically different in terms of
% the prediction task.
% %
% Worse, in many settings minority groups are underrepresented relative to population statistics. For
% example, minority groups are underrepresented in the tech industry. Different groups might also
% adopt technology at different rates, which might skew datasets assembled form social media. If
% training sets are drawn from these unrepresentative contexts, there will be even fewer training
% points from minority individuals.
% %
% When we develop machine-learning models, we typically only test their overall accuracy; so a “5%
% error” statistic might hide the fact that a model performs terribly for a minority group. Reporting
% accuracy rates by group will help alert us to problems like the above example

% A major limitation of machine learning is that it only reveals correlations, but we often use its
% predictions as if they reveal causation. This is a persistent source of problems. For example, an
% early machine learning system in healthcare famously learned the seemingly nonsensical rule that
% patients with asthma had lower risk of developing pneumonia. This was a true pattern in the data,
% but the likely reason was that asthmatic patients were more likely to receive in-patient care
% \citep{caruana2015intelligible}. So it’s not valid to use the prediction to decide whether or not
% to admit a patient. We’ll discuss causality in Chapter 5.

