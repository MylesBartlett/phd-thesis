% ********************************************************************************
\section{Adversarial learning}\label{sec:advl}
% ********************************************************************************
\Ac{AdvL} is a general paradigm characterising -- in game-theoretical parlance --
non-co{\"o}perative (competitive) systems, or `games' in which two or more `players' compete over
shared, or at least interdependent, \emph{utility} \citep{fudenberg1991game}.
%
\sidepar{Minimax}
%
In two-player cases -- accounting for the majority of \ac{AdvL} setups (notable exceptions to this
include those based on self-play in which one computes the best response against a mixture of
adversarial policies \citep{silver2017mastering, vinyals2019grandmaster}) and those that I shall
discuss here -- the game can be formulated as a (zero-sum) minimax problem in which one player,
\(\mu_{max}\), plays the role of the `maximiser', the other player, \(\mu_{min}\), the role of the
minimiser; given that in \ac{ML} we optimise some parametric model to minimise a loss function (via
gradient descent), it is natural to view \(\mu_{min}\) as the model of interest, or `learner', and
\(\mu_{max}\) as the `adversary` which frustrates \(\mu_{min}\) in order to improve its own payoffs
(but with the end goal of the game ultimately being to improve \(\mu_{min}\) in some respect, such
as robustness or fairness).
%

Such a problem can alternatively be viewed as an alternating (turn-based) bilevel optimisation
problem, where at the \(t\)th iterate the learner selects (via some optimisation procedure) from \(
\Sigma_{min} \), the strategy that gives the best response, \(\sigma_{min, t}^{\ast}\), to the best
response of the adversary at the previous iterate, i.e.\ \(
\sigma_{max}^\ast|_{\sigma_{min,t-1}^\ast} \).
%
\sidepar{Bilevel optimisation}
%
The best response is determined by each players respective utility, as measured by the
player-specific function \(\pi: \Sigma \to \R^+\), with \(\Sigma\) denoting the space of strategic
profiles, a strategic profile itself referring to tuple of chosen strategies characterising a game
state.
%
A hallmark of the adversarial regime is that the strategy of maximiser is dependent on the
state of the minimiser and if the state of the minimiser changes so does the best-response, unless 
in a state of \emph{Nash Equilibrium} (NE; or at least a local one).  
%
\sidepar{Nash equilibria}
%
The concept of a NE is fundamental to game theory, referring to strategic profiles, \(\sigma^\ast \in
\Sigma^\ast\), from which no player can unilaterally deviate and achieve greater payoff, or,
mathematically (treating \(\mu_{min}\) as the reference player)
%
\begin{equation}
  \forall \sigma_{min} \in \Sigma_{min}:\, \pi_{min} ( \sigma^\ast_{min}, \sigma^\ast_{max} ) \ge \pi_{min}(
  \sigma_{min}, \sigma^\ast_{max} ),
\end{equation}
%
where \(\sigma^\ast_{min}\) and \(\sigma^\ast_{max} \) are NE strategies for \(\mu_{min}\) and \( \mu_{max}
\), that by their pairing make up some NE strategic profile, \(\sigma^\ast\).
%
% using \(\pi_\cdot\), denote the payoff and the strategy of the subscripted player, respectively
%
By this non-strict definition the set of NE solutions can be non-singleton -- by virtue of the
inclusive inequality -- such that for any given game one may have a set of one or more NE strategic
profiles.
%
The notion of a NE is closely related to the idea of Pareto optimality in MOO, noting, however,
that a NE strategic profile need not be a \ac{PO} one, \ac{PO} being defined \wrt{} the maximum
theoretically-achievable utility for each player (given the utilities of every other player), NE
solely \wrt{} the relative utilities of the players and the resulting fixed points.

%
The minimax formulations presented in the context of minimax fairness and (worst-group)
distributional robustness comply -- perhaps subtly -- with this definition as the strategy of the
maximiser can be interpreted as a weighting function whose best-response is the one under which
only the loss of the highest-loss group contributes to the overall loss. 
%
Practically, computing the best response for each player over the entire dataset for each iterate
is computationally intractable for most non-trivial cases, especially so if said players are
\acp{DNN} and many iterates are required for convergence, and so some degree of approximation is
required.
%
This usually translates to performing only a fixed budget of updates in the minimising/maximising
direction over random subsets of the data.
%
In fact, \citet{ganin2016domain} demonstrated that, in practice, one can ignore best-response
dynamics altogether and obtain approximately domain-invariant representations with minimal overhead
through concurrent (single-step) updates to the players' strategies.

Notable concrete applications of \ac{AdvL} include artificial curiosity
\citep{schmidhuber1992learning}, Generative Adversarial Networks (\ac{GAN};
\citealp{goodfellow14generative}), self-play \citep{silver2018general}, adversarial robustness
\citep{szegedy2013intriguing}, and, most germanely to this thesis, domain-invariant learning
\citep{ganin2016domain, zhao2019learning} and \ac{FRL} \citep{edwards2015censoring,
madras2018learning}.
%
\sidepar{Adversarial infomin}
%
In the latter applications, \ac{AdvL} is frequently leveraged as an engine for
(mutual) information-minimisation -- or \emph{infomin} -- where the information to be minimised is that
related to the domain or sensitive group in \ac{DA}/\ac{DG} and \ac{AF}, respectively.
%
For this, both players take the form of a neural network with strategies defined by their
parameters -- \(\theta_{min} \in \Theta_{min}\) and \(\theta_{max} \in \Theta_{max}\) for the
learner and adversary, respectively -- which they play according to their respective architectures,
together constituting the actions \(a_{min}: \gX \times \Theta_{min} \to \gZ \) and \(a_{max}: \gZ
\times \Theta_{max} \to \R \).
%
I note, incidentally, that due to the continuity and non-convexity of the players in this case, it
is unfortunately not possible to guarantee the existence of Nash Equilibria for the resulting
games, only ones that are locally defined \citep{unterthiner2018coulomb}.
%
The game in question can then be couched in terms of the following countervailing objectives,
respective to the learner and its adversary:
%
\begin{itemize}
  %
  \item 
    %
    \textbf{learner}: create a censored version (representation) of the input, \(z\), that
    maximally minimises the amount of information about \(s\) that can be extracted.
    %
  \item 
    %
    \textbf{adversary}: maximise the likelihood of a correct determination of the true value of
    \(s\) associated with a given input \(x\), while having access to only the censored version of
    the input, \(z\), given by the learner.
    %
\end{itemize}
%
This game gives rise to the following minimax objective function, for some dataset \(\gD\)
made up of pairs of inputs, \(x\), and attribute(s) to be censored, \(s\)
%
\begin{align}
  %
  \underset{\theta_{min} \in \Theta_{min}}{\text{min}}\; \underset{\theta_{max} \in
  \Theta_{max}}{\text{max}}\,
%
-\E_{(x, s) \sim \gD} [ \ell(a_{max}( a_{min}(x, \theta), \psi ), s ) ],
%
\label{eq:adv-infomin}
\end{align}
%
where I have negated the expectation (converting the loss into utility) to remain consistent with
the idea of the adversary being the maximiser and defined the optima over the parameter spaces to
make clear the idea that the parameters define the chosen strategies. 
%
When \(s\) is discrete, \(\ell\) is typically taken to be the standard cross-entropy loss; the
fixed point of the objective function is attained when the outputs of \(a_{max}\), with codomain
the appropriate probability simplex, are maximally entropic (the derived predictions no better than
random), which one hopes holds for all \(\sigma_{max} \in \Sigma_{max}\) and connotes the
invariance \(Z \perp S\).
%
\sidepar{Practical challenges}
%
Having the learner play this game without any additional objectives (maximising for utility \wrt{}
the task of interest) is, of course, inauspicious if the goal is to have a representation that is
useful for some task (other than foiling the adversary) -- a trivial solution on the part of
\(\mu_{min}\) would, for instance, be to simply ignore the input and output a constant
representation.
%
In many cases, however -- especially those arising in \ac{AF} and \ac{DA} -- there is competition not
only between the adversary and the learner, but between the objectives themselves \wrt{} the latter
alone.
%

Assume, for instance, that the task of interest is a classification one, such that we have on top
of the infomin objective defined by Eq.~\ref{eq:adv-infomin} an \emph{infomax} objective \wrt{} the
ground-truth label \(y\).
%
Only if the condition \( \gI(S; Y) \approx 0 \) (i.e.\ the target labels are uniformly distributed
across \(\gS\)) holds can the infomin and infomax objectives be simultaneously satisfied, which is
to say, in optimisation terms, that the inner product of the gradients of the two respective losses
(\wrt{} \(\theta_{min}\)) is consistently non-negative and there is no trade-off, governed by the
preference direction, leading to a \ac{PF} and suggesting treatment with \ac{MOO} methods.
%
More generally, unconditional infomin is problematic whenever there is conditional shift between
the training (source) and test (target) sets, as anatomised by \citet{zhao2019learning} in the
context of \ac{UDA}. 
%
Given full observability of \(y\) and \(s\) and consistent support of their joint distribution,
i.e.\ \(\text{supp}(\hat{P}^{tr}(S, Y)) = \text{supp}(\hat{P}^{te}(S, Y))\), one can realign the
objective by computing the infomin component class-conditionally (practically, importance weighting
based on the empirical distribution \(\hat{P}^{tr}(Y)\)). 
%
When this observability does not hold, generally or for a subset of the data (the target domain in
the case of \ac{UDA}), then matters are complicated, however, and approximations are required
(e.g.\ by clustering).
%
In Chapter~\ref{ch:supmatch} we consider a problem of this nature and recast the problem as one of
aligning the supports of the training -- which is systematically missing certain combinations of
\(s\) and \(y\) -- and deployment -- which is presumed unlabelled, as in \ac{UDA} -- sets.

I have already alluded to some of the challenges entailed in adversarial infomin
approaches; here, I summarise the two most salient ones.
%
First, the strength of information-minimisation is proportional to the strength adversary used to
drive it, and, moreover, a fixed point attained by one adversary is not guaranteed to hold for any
other adversary (differing in architecture, optimisation scheme, etc.) unless the fixed point
corresponds to the desired invariance. 
%
\sidepar{Approximation errors}
%
Theory dictates that one computes the best response of each player at each iteration, however this
is generally infeasible when working with models with many parameters and datasets with many
samples. 
%
Thus, approximations are required -- e.g.\ by limiting the number of updates per iteration and by
bootstrapping the best response from the previous one -- but these can lead to unstable training
dynamics or entrapment in bad optima. 
%
Indeed, a number of recent studies have shown that many adversarial approaches fail to faithfully
produce infomin representations when probed \citep{moyer2018invariant, feng2019learning,
balunovic2021fair}.
%
\sidepar{Cyclic dynamics}
%
Second, adversarial setups are generally susceptible to cycles (oscillations) in strategy space,
e.g.\ where two players repeatedly switch between two non-NE strategies because doing so is
mutually locally optimal. 
%
In reinforcement learning, for instance, this has led to the development of fictitious self-play
algorithms \citep{brown1951iterative, heinrich2015fictitious, vinyals2019grandmaster} wherein each
player has its best response computed against a uniform mixture of past opposing strategies (this
mixture provably converging to a NE); while this allows for avoiding the aforementioned cycles it
comes at a significant computational cost.
%
In Chapter~\ref{ch:nifr}, we recognise the pitfall of cyclic dynamics and propose a middle ground
of training against a stochastic ensemble of adversaries.
% 

% If a pure strategy is strictly dominated by another pure strategy then it cannot be played with
% positive probability at a Nash equilibrium.
% %
% Thus, for the purpose of finding Nash equilibria, one can disregard all the strictly dominated
% strategies and focus on the resulting game

% The associated equilibria can be expressed in terms of so-called minmax strategies, defining the
% best responses to the entailed bilevel optimisation problem respective to each player.
% %
% Of note is that fact that any convex combination of any equilibria is also an equilibrium, implying
% that any given game for which the notion of an equilibrium applies has either one or
% infinitely-many equilibria.


% The natural interpretation of the standard GAN setup is as an infinite game where pay-offs are
% defined over all possible weight parameters for the respective networks.
% With this view, we do not obtain existence of saddle points.
% %
% This is why the notion of local Nash equilibria (LNE) have arisen in the literature.
% %
% Broadly, an LNE is a strategic profile where neither player can improve in a small neighbourhood of
% that profile. 
% %
% In finite games, every LNE is an NE, as whenever there is a global deviation, one can always
% deviate locally in the space of mixed strategies towards a pure best response.


% % \cite{GooAbaMirXuetal14}
% % % --------------------------------------------------------------------------------
% % \subsection{Move Order and Strategic Equilibria} 
% % % --------------------------------------------------------------------------------
% % Since the strategies are not finitely spanned, the
% % minimax theorem does not hold and the very idea of an ``equilibrium'' becomes tenuous.

% % % \import{background/pc}{al.tex}

% Papers to reference:
% \cite{edwards2015censoring, madras2018learning, zhao2019learning, long2018conditional,
% szegedy2013intriguing, goodfellow14generative, oliehoek2017gangs, chen2022scalable}

% \cite{moyer2018invariant}:
% Existing approaches cast the trade-off between task performance and invariance in an adversarial
% way, using an iterative minimax optimization. We show that adversarial training is unnecessary and
% sometimes counter-productive.

% Use of MMD to encourage the statistical moments to be close for all subgroups.

% The removal of covariate factors from scientific data has a long history. Observational studies in
% the sciences cannot control for every factor influencing subject.s

% Group-theoretic perspective on removal of covariate information: draw equivalences between finding
% invariant features and finding the quotient space (equivalence class) of the domain over a covariate
% group action.

% Removal of unwanted information is a surprisingly common task. Transform-invariant features in
% computer vision, fair encodings from the algorithmic fairness community, domain-invariant
% representations from the domain adaptation and domain generalisation communities, and so on and so
% forth. In the context of representation learning, we wish to map an input to an encoding that is
% uninformative about \(s\) yet also optimal for the given task.

% Terms: Minimax framework, critic, best response, pure strategy, mixed strategy, local/\(\eps\) Nash
% equilibrium
