% ********************************************************************************
\section{Adversarial Learning}\label{sec:adv-learning}
% ********************************************************************************
Adversarial learning (AdvL) is a general, multi-field-spanning learning paradigm, characterising --
borrowing game-theoretical parlance -- non-cooperative systems, or `games' in which two or more
`players' compete over a shared (or interdependent) objective. 
%
In two-player cases -- accounting for the majority of AdvL setups (notable exceptions to this
include those based on self-play in which one computes the best response against a mixture of
adversarial policies \citep{silver2017mastering, vinyals2019grandmaster}) and those that we shall
discuss here -- the game can be formulated as a minimax problem in which one player, \(a\), plays
the role of the `maximiser', the other player, \(f\), the role of the minimiser; given that in ML
we optimise some parametric model to minimise a loss function (via gradient descent), it is natural
to view \(f\) as the model of interest, or `learner' and \(a\) as the `adversary` which frustrates
\(a\) in order to improve its own payoffs (but with the end goal of the game ultimately being to
improve \(f\) in some respect, such as its robustness or fairness).
%

Such a problem can alternatively be viewed as an alternating (turn-based) bilevel optimisation
problem, where at the \(t\)th iterate, the parameters, \(\theta \in \Theta\), of the learner are
updated to give the best response, \(\theta^{\ast}_t\), to the best response of the adversary at
the previous iterate, i.e. \(a^\ast|_{\theta^\ast_{t-1}}\).
%
A hallmark of the \emph{adversarial} regime is that the strategy of maximiser is dependent on the
state of the minimiser and if the state of the minimiser changes so does the best-response save if
in a state of \emph{Nash Equilibrium} (NE; or at least a local one).  
%
The concept of a NE is fundamental to game theory, referring to strategic profiles from which no
player can unilaterally deviate and achieve greater payoff, or, mathematically
%
\begin{equation}
  \forall s_i \in \gS_i:\, \pi_i ( s^\ast_i, s^\ast_{-i} ) \ge \pi_i( s^\ast_i, s^\ast_{-i} ),
\end{equation}
%
using \(\pi_i\) and \(\gS_i\) to denote the payoff and set of possible strategies, respectively for
player \(i\).
%
This general (non-strict) NE is non-unique -- by the virtue of the inclusive inequality -- such
that for any given game one may have a set of NE strategic profiles.
%
The notion of a NE is closely related to the idea of \emph{Pareto Optimality} (PO) in
multi-objective optimisation, noting, however, that a NE strategic profile need not be a Pareto
optimal one, PO being defined in terms of the maximum theoretically-achievable utility for each
player (given the utilities of every other player), NE solely in terms of the relative utilities of
the players and the resulting fixed points.

%
The minimax formulations we saw in the context of minimax fairness and (worst-group) distributional
robustness comply -- perhaps subtly -- with this definition as the strategy of the maximiser can be
interpreted as a weighting function whose best-response is the one under which only the loss of the
highest-loss group contributes to the overall loss. 
%
Practically, computing the best response for each player over the entire dataset at each iterate is
not computationally tractable in most non-trivial cases, especially so if said players are deep
neural networks and many iterates are required for convergence, and so some degree of approximation
is required.
%
This usually translates to performing only a fixed budget of updates in the minimising/minimising
direction over random subsets of the data.

Notable concrete applications of adversarial learning include Artificial Curiosity
\citep{schmidhuber1992learning}, Generative Adversarial Networks (GANs;
\cite{goodfellow14generative}), (fictitious) Self-Play \citep{silver2018general}, Adversarial
Robustness \citep{szegedy2013intriguing}, and, most germanely to this thesis, domain-invariant
\citep{ganin2016domain, zhao2019learning} and Fair \citep{edwards2015censoring, madras2018learning}
Representation Learning.
%
In the latter applications, adversarial learning is frequently leveraged as an engine for
(mutual) information-minimisation -- or \emph{infomin} -- where the information to be minimised is that
related to the domain or sensitive group in DA/DG and FairML, respectively.
%
For this, both players take the form of NNs with strategies defined by their parameters, \(\theta\)
and \(\psi \in \Psi\) for the learner and adversary, respectively. 
%
Due to the continuity and non-convexity of (multi-layer) NNs, it is unfortunately not possible to
guarantee the existence of Nash Equilibria for the resulting games, only ones that are locally
defined \citep{unterthiner2018coulomb}.
%
The game in question can then be couched in terms of the following countervailing objectives:
%
\begin{itemize}
  %
  \item \textbf{adversary}: maximise the likelihood of a correct determination of the true value of
    \(s\) associated with given a input \(x\), while having access to only the version
    (representation) of the input censored by \(f\).
    %
  \item \textbf{learner}: create a censored version of the input that maximally minimises the
    amount of information about \(s\) determinable from it. 
  %
\end{itemize}
%
This game then gives rise to the following minimax objective function
%
\begin{align}\label{eq:minimax-obj}
  %
\underset{\theta \in \Theta}{\text{min}}\; \underset{\psi \in \Psi}{\text{max}}\,
%
-\E_{(x, s) \sim \gD} [ \ell(a_\psi( f(x) ), s ) ]
%
\end{align}
%
where we have negated the expectation (converting the loss into utility) to remain consistent with
the idea of the adversary being the maximiser and defined the optimisations over the parameter
spaces to make clear the idea that the parameters define the chosen strategies; when \(s\) is
discrete, \(\ell\) is typically taken to be the standard cross-entropy loss.
%
Having the learner play this game without any additional objectives (maximising for utility \wrt{}
the task of interest) is, of course, inauspicious if the goal is to have a representation that is
useful for some task (other than eluding the adversary) -- a trivial solution on the part of \(f\)
would, for instance, be to simply ignore the input and output a constant representation.
%



%
% Such a minimax scenario can be formulated, in general terms, for the standard (bivariate) supervised
% learning case as
%
% \begin{align}\label{eq:minimax-obj}
%   %
% \underset{\theta \in \Theta}{\text{min}}\; \underset{\psi \in \Psi}{\text{max}}\,
% %
% \E_{(x, y) \sim \gD^{tr}} a_\psi(\ell, f,  x, y)
% %
% \end{align}
% %
% recalling \(\ell\) denotes the empirical loss, or pay-off associated with the strategy profiles \(
% (\theta, \psi) \), corresponding, for instance, to the weights of a neural network or to the
% direction/magnitude of a perturbation.
% %
% For generality, we have formulated the above such that the adversary may define any fixed
% algorithm, modulated by \(psi\), that can operate on any of the supplied variables.
%

% ChatGPT nonsense
% begin
% Adversarial learning is a type of machine learning technique where the goal is to learn from the
% competition between two or more agents, each of which has opposing objectives. It finds use in a
% wide range of applications, including image and speech recognition, natural language processing,
% and game playing.
% %
% In game theory, the concept of a Nash equilibrium is used to describe a situation where no agent
% can improve its outcome by changing its strategy. Similarly, in adversarial learning, the goal is
% to achieve a robust model that cannot be easily fooled by an adversary. This is analogous to a Nash
% equilibrium, where the model's strategy (i.e., its decision-making process) cannot be improved by
% the adversary.

% Formally, we can describe adversarial learning as a game between two players, denoted as P1 and P2.
% P1 is the learner, and P2 is the adversary. Each player has a set of actions and objectives. The
% learner's objective is to minimize the loss function L, which measures the discrepancy between the
% learner's predicted output and the actual output. The adversary's objective is to maximize the loss
% function L. The game is repeated for a finite number of rounds, and both players take turns
% choosing their actions.

% Distinct from games treated by classical Game Theory, in adversarial learning, the strategies
% adopted by the players are generally not discrete but continuous.

% In game theory, the Nash equilibrium is a concept used to describe a situation where no player can
% improve their outcome by changing their strategy, assuming that all other players are following
% their strategies. Similarly, in adversarial learning, we aim to reach a point where the learner
% cannot be improved further by the adversary.

% Formally, we can define a Nash equilibrium for the adversarial learning game as a pair of
% strategies (s1, s2), where s1 is the strategy of the learner, and s2 is the strategy of the
% adversary. This pair of strategies is a Nash equilibrium if and only if:

% The learner's strategy s1 minimizes the loss function L, given that the adversary is using strategy
% s2. The adversary's strategy s2 maximizes the loss function L, given that the learner is using
% strategy s1. In other words, neither player can improve their outcome by changing their strategy,
% assuming that the other player is using their strategy.

% end

%
% Adversarial learning (AL) refers to any  a system in which the objective of our learner is
% frustrated by some adversarial policy, which may or may not itself be parameterised.

% - Adversarial robustness (in the context of non-parametric adversarial policies)
% - Best response
% - Deeply-rooted in game theory \citep{von1944theory}
% - Can be framed as a bilevel optimisation problem with the dynamics of the inner loop governed by
% some adversarial policy
% - GANs can be viewed as a particular instantiation of Predictability Minimisation in which the
% outputs of the controller, \(C\) are randomly replaced with samples from the true distribution,
% with some probability, as inputs to the minimiser, \(M\) \citep{schmidhuber1992learning, schmidhuber2020generative}
% - Since the strategies are not finitely spanned, the minimax theorem does not hold and the very
% idea of an ``equilibrium'' becomes tenuous.

% A fundamental concept in game theory is the Nash Equilibrium (NE), defined as a strategy profile
% which no player can deviate from to improve their pay-off.
% %
% Nash famously be showed that every finite game has at least one NE in mixed strategies.
% %
% The notion of a NE is closely related to the idea of Pareto Optimality for multi-objective problems
% composed of competing (`non-cooperative') objectives, noting, however, that a NE strategic profile
% need not be Pareto optimal one, as demonstrated by the textbook example of the Prisoner's Dilemma.

% If each player has chosen a strategy and no player can increase one's own expected pay-off by
% unilaterally changing their strategy, then the current set of strategy profile constitutes a Nash
% equilibrium.


% \begin{theorem}
%     (Minimax theorem) In a zero-sum game, with maximising player \(s_{max}\) and minimising
%     player \(s_{min}\) we have the order of the min and max operations are exchangeable.
% \end{theorem}

% Let \(s_i \in \gS\) denote the pure strategy of the \(i\)th player and \(\sigma_i \in \Sigma_i \)
% their mixed strategy, with \(\sigma(s_i)\) representing the probability of choosing strategy
% \(s_i\).
% %
% If a pure strategy is strictly dominated by another pure strategy then it cannot be played with
% positive probability at a Nash equilibrium.
% %
% Thus, for the purpose of finding Nash equilibria, one can disregard all the strictly dominated
% strategies and focus on the resulting game

% The associated equilibria can be expressed in terms of so-called minmax strategies, defining the
% best responses to the entailed bilevel optimisation problem respective to each player.
% %
% Of note is that fact that any convex combination of any equilibria is also an equilibrium, implying
% that any given game for which the notion of an equilibrium applies has either one or
% infinitely-many equilibria.


% The natural interpretation of the standard GAN setup is as an infinite game where pay-offs are
% defined over all possible weight parameters for the respective networks.
% With this view, we do not obtain existence of saddle points.
% %
% This is why the notion of local Nash equilibria (LNE) have arisen in the literature.
% %
% Broadly, an LNE is a strategic profile where neither player can improve in a small neighbourhood of
% that profile. 
% %
% In finite games, every LNE is an NE, as whenever there is a global deviation, one can always
% deviate locally in the space of mixed strategies towards a pure best response.


% % \cite{GooAbaMirXuetal14}
% % % --------------------------------------------------------------------------------
% % \subsection{Move Order and Strategic Equilibria} 
% % % --------------------------------------------------------------------------------
% % Since the strategies are not finitely spanned, the
% % minimax theorem does not hold and the very idea of an ``equilibrium'' becomes tenuous.

% % % \import{background/pc}{al.tex}

% Papers to reference:
% \cite{edwards2015censoring, madras2018learning, zhao2019learning, long2018conditional,
% szegedy2013intriguing, goodfellow14generative, oliehoek2017gangs, chen2022scalable}

% Excerpts from \cite{chen2022scalable}: Adversarial training is effective with a strong adversary,
% however it is often challenging to train the adversary thoroughly in practice [to yield the best
% response] due to temporal or numerical barriers. 
% In fact, recent studies have revealed that adversarial approaches may not faithfully produce an
% infomin representation \citep{moyer2018invariant, feng2019learning, balunovic2021fair}.

% \cite{moyer2018invariant}:
% Existing approaches cast the trade-off between task performance and invariance in an adversarial
% way, using an iterative minimax optimization. We show that adversarial training is unnecessary and
% sometimes counter-productive.

% Use of MMD to encourage the statistical moments to be close for all subgroups.

% The removal of covariate factors from scientific data has a long history. Observational studies in
% the sciences cannot control for every factor influencing subject.s

% Group-theoretic perspective on removal of covariate information: draw equivalences between finding
% invariant features and finding the quotient space (equivalence class) of the domain over a covariate
% group action.

% Removal of unwanted information is a surprisingly common task. Transform-invariant features in
% computer vision, fair encodings from the algorithmic fairness community, domain-invariant
% representations from the domain adaptation and domain generalisation communities, and so on and so
% forth. In the context of representation learning, we wish to map an input to an encoding that is
% uninformative about \(s\) yet also optimal for the given task.

% Terms: Minimax framework, critic, best response, pure strategy, mixed strategy, local/\(\eps\) Nash
% equilibrium
