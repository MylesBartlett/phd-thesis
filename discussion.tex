% \chapter{Discussion}\label{ch:discussion} 
\epigraph{
    %
    \emph{
        %
        ``I do not care what comes after; I have seen the dragons on the wind of morning.''
        %
} 
%
}
{The Farthest Shore\\Ursula K. Le Guin}
%
\section*{Preamble}
%
And so we come to story's end, but the end of every story begets the beginning of a new one, or
maybe the continuation of same one, and so the story may never truly end, but only perpetually
change.
%
% Through the varied isles of ML we have sailed;
Behind every story there is another story of how that story came to be, and together a greater
story they may make;
%
for I might tell of the seeking of wisdom, from those sages that came before and those that
in far-off realms now dwell, ever weaving, layer upon layer, their spells of computation to make
magic of machine;
%
of bearding twin serpents, Byas and Koraleishen and seeing them reconciled with unlabelled aid;
%
and of parleying with the Wardens of the great Sancta of Knowledge -- Aykl\'ir, Ays\^iml, Esesevi,
N\"ur\`ipsa, and time-honoured S\'iv\={i}piar, not least of all-- to earn entrance to their
hallowed halls.
% the sister moons
% of iris Aykleer and fuchsia Frijeyai bloomed full.
%
\section*{Of what was said and the silence between}\label{sec:what-was-said}
%
I have introduced three methods, each a solution to a different problem, though with all problems
conjoined by a notion of distributional-robustness.
%
\marginpar{\textbf{Synopsis}}
%
To briefly recount, and thereby set the scene for the discussion to follow: in Chapter 3 we
proposed an INN-based transfer-learning approach -- transferring invariance from a
partially-labelled representative set to the training set -- for solving a spurious correlation
problem where for training samples the target is completely determined by the sensitive attribute
and because the latter is easier to learn it constitutes a shortcut;
%
in Chapter 4 we proposed to match in representation-space the support (of intersectional groups) of
the training and deployment sets in order to overcome a relaxed version of aforesaid
spurious-correlation where the deployment set is a dataset representative (in support) of the test
set, but for which no annotations are required unlike the representative set featured in Chapter 3;
%
finally, in Chapter 5 we grappled with the problem of semi-supervised DG -- by which I mean the
problem of how to make effective use of unlabelled data drawn from extra domains to bolster
out-of-distribution performance -- and proposed a consistency-regularised approach employing a
robust, causally-inspired algorithm as a match-generation engine, with the matches bootstrapped
from encodings guided (in optimisation) by past matches.
%
Again, all of these works are mine but not unshared, for I owe much gratitude to all my
co{\"a}uthors -- my colleagues, my advisors, my friends -- for allowing this thesis to become what
it has thus become.

%
\marginpar{\textbf{Limitations}}
%
I shall begin by discussing, with the gift of hindsight and the wisdom which comes with being
humbled by experience, the limitations of the works presented, and I will do so candidly: for as I
said at the outset, I do not fear the `lesser elements', for every candle lit there is a shadow
cast, but it is because of the shadow that we may see the light and we may learn to never cast the
same shadow twice. %or otherwise accept that is the shadow is right for the light.
%
Among these limitations, I shall begin with the most fundamental one that pervades all the works
and is itself bipartite, comprising the assumed availability of subgroup (I, arbitrarily, use this
term to cover the myriad names for \(S\) here) annotations, and the assumed discrete property of
these annotations.

%
In journeying through this thesis, we have witnessed a progressive relaxation of the former `part',
such that in Chapter 4 we need only know in advance all possible (\wrt{} the test set) sources (and
know that the deployment set contains them); by Chapter 5 we need only be able to partition the
data into two disjoint sets, across which the matching is performed.
%
We argued in Chapter 3 that partially-labelled data, for which the subgroup- but not the target-
attribute is provided, is generally more `readily available'. 
%
While this may be true for certain domains and applications, it is not true for others, and it is
largely contingent on what the target and subgroup attributes have been determined to be, and,
moreover, how they interact (their relative complexity).
%
It is reasonable in the case of a face dataset like CelebA to assume that gender information can be
explicitly, or implicitly, gleaned -- for instance, by virtue of the images appearing in
gender-specific catalogues -- whereas `Smiling' is not a feature to innately partition by (and we
would assume that in the forgoing catalogue case that most models will be smiling), and these two
attributes would indeed realistically serve well as the subgroup and target attributes,
respectively, for Chapter 3's framework; it is less easy to intuit whether a shortcut would emerge
though we contend there is no harm in taking the precaution if using a lossless encoder.
%
If it were `Hair Colour' that we sought invariance to -- which seems quite plausible -- and `Age'
we were targeting, however, matters would not pan out so neatly.

%
The problem of learning distributional-robust models from biased data, and circumventing spurious
correlations thereof, when the distributions (subgroups) in question are unknown has attracted
considerable attention as of late --  \cite{HasSriNamLia18, SohDunAngGuetal20,
creager2021environment, liu2021just, pezeshki2021gradient, taghanaki2022masktune, kim2022learning},
inter alia.
%
However, one must unavoidably rely on certain assumptions (inductive biases derived from prior
knowledge of the task/domain) to compensate for the lack of information and the referenced methods
can fall flat (in the sense of underperforming the ERM baseline) should such not be satisfiable.
%
Chapter 4 entails this to a degree, in that the sources contained in the deployment set need be
discovered for constructing support-representative batches, although the (annotated) sources in the
training set can subserve this process of discovery, for it is not entire subgroups or classes that
are excluded from annotation but their intersections.
%
Nonetheless, this discovery can fail to align with expectation, and we assume in Chapter 4 that the
subgroups/targets are sufficiently salient to be well-clustered by un-/semi-supervised means -- one
would not expect a source formed from gender/pulmonary infiltration (subgroup and target
attributes, respectively) to lend itself to natural clusters, to adduce a fail-case from our
experimentation. 

Regarding the latter `part', we have throughout assumed that the subgroup can be represented,
innately or by some preprocessing, discretely; while this is accords with much of the literature
(because of its simplicity and prevalence) there are nonetheless cases where this assumption is
untenable.
%
This is particularly germane to the method proposed in Chapter 4 which expects the data to both be
clusterable and for the sources to be finite (and, practically, tractably-sized) sets, such that
one can balance batches \wrt{} them -- how one might extend the method to continuous subgroups is
an open question -- the notion perhaps not even sensible -- and in order to maintain tractability
in pursuit of generality it would seem necessarily give up the theoretical guarantees established.
%
The adversarial-infomin approach adopted in Chapter 3, on the other hand, can be readily adapted to
continuous subgroups, for instance, by substituting cross-entropy with the
Hirschfeld-Gebelein-R\'enyi (HGR) Maximum Correlation Coefficient (we use HGR in said chapter not
as an objective but as a fairness metric for tasks involving \emph{categorical} subgroups).

\marginpar{\textbf{The idiosyncrasies and iniquities of adversarial infomin}}
%
Seeing this as opportunity to dovetail (half-cohesively) into fuller discussion of AdvL: I have
spoken before of the practical deficiencies of the paradigm in a DL context -- namely the fragility
of optimisation, disposition to cyclical dynamics, architectural dependency, and the loss of
guarantees incurred by estimating the best response dynamics with a finite (typically small) number
of steps -- but it is apposite that we revisit these points again here, retrospectively instead of
prospectively.
%
Indeed -- as again spoken of before but of which there is again no harm recalling -- said
deficiencies have been well noted infomin literature, motivating attempts to develop
non-adversarial approaches, taking advantage of, for instance, the exact-density-estimation
afforded by normalising flows \citep{balunovic2021fair}, sliced mutual information
\citep{goldfeld2021sliced} to scalably target the infomin objective directly
\citep{chen2022scalable}, or the information-bottleneck principle \citep{tishby2015deep,
moyer2018invariant} for which the subgroup attribute plays the part of the \emph{nuisance factor},
to reconcile the parlance.
%
While cognisant of, and taking measures (e.g. ensembling, limiting the volume of the latent space)
to ameliorate, these deficiencies, the making of the aforesaid chapters was in no small amount
harried by them, often demanding careful and exhaustive tuning to coax the respective methods into
working as desired, the `tuning' itself problematic due to the underspecified nature of the problem
setups.
%
We found the inherent instability of AdvL particularly pronounced in the making of Chapter 3,
specifically, owing to the compounding instabilities imparted by the invertible architectures, a
problem which itself would not be addressed \emph{in toto} until \cite{behrmann2021understanding}.
%
In light of the forgoing, exploring non-adversarial methods, of the kind referenced, as alternative
infomin engines for the higher-level methods proposed in the germane chapters is certainly
well-founded (for those wishing to apply, or undertake further research, on said methods), though I
cannot -- despite their theoretical appeal -- attest to their practical efficacy in these contexts,
\emph{a priori}; I would note in defence of the chapters, however, that for neither higher-level
method is the form of said engine integral to its identity, and is in fact modular, such that we
may conceivably freely interchange engines subject to their respective requirements.
%
In Chapter 5 we induce invariance by non-adversarial means ourselves, namely by enforcing
similarity between matching samples from different domains within representation space -- the
adversary substituted with a co\"operative, non-parametric match-generator -- and we thereby avert
many of the optimisation difficulties plaguing the aforementioned chapters, though the problem of
judicious hyperparameter-selection lingers, or arguably amplified by the non-parametricity -- we
point in the paper to adaptive-configuration of the calipers being an obvious (in motivation but
not implementation) extension to the existing algorithm.
%

\marginpar{\raggedleft\textbf{On the problem of identifiability}}
%
A central question that I have perhaps given shorter shrift to than due, is that of the
identifiability of bias -- a question of two aspects: 1) diagnosing those cases warranting
methodological (or data-sided) interventions, such as those delineated; 2) and evaluating the
success of those interventions -- given the element of underspecification
\citep{semenova2019study}.
%
To the first aspect of the question, the obtuse answer would be that it need not be answered, or
answerable, presupposing that the model in question (figuratively) is to be deployed regardless
with or without intervention (the control), for comparisons should be made \wrt{} the latter rather
than \wrt{} an ideal, a gold-standard that may (and often will not be) practically realisable --
one need only ensure that performance does not degrade based on what is evaluable.
%
%
This ethos of `do the best we can with what's available, regardless of the outcome', however is
patently not an admirable one, however, and realistically one (a practitioner or collective) does,
or should, aim to deploy models subject to their being sensible/unbiased (if only for reputation's
sake) and to thoroughly diagnose, and attempt to remedy failure cases via well-measured and
iterative processes.
%
The second aspect, the problem of underspecification -- of validating models without access to data
representative of that to-be-encountered at deployment time, as defines DG -- remains an
outstanding one, that has been discussed broadly and in the context of DG specifically, where it
risks invalidating inter-method comparisons \citep{gulrajani2020search}.
%
\marginpar{\raggedleft\textbf{Interpretability and Oversight}}
%
In absence of a such validation set by which to quantify biases, one may draw upon methods from the
explainability/interpretability literature \citep{gunning2019xai} to determine the learned
solutions align with the intended solutions; one may, for instance, readily diagnose the use of
background as a shortcut, per the now-canonical example from \cite{beery2018recognition}, with a
standard-method-in-that-literature in Grad-cam \cite{selvaraju2017grad}, allowing for a targeted
intervention (e.g. by augmentation) -- one may even use the resulting attribution maps directly for
this purpose as proposed by \cite{taghanaki2022masktune}.
%
It is for this reason that we emphasise the interpretability aspect in Chapter 3, for even if we
cannot sufficiently debias a model (if it need be debiased), we might glean when this is the case
and for what reason without need for quantification.
%
This is, of course, easier for some domains than others -- images being naturally interpretable due
to their underlying structure and familiarity (the window through which we, quite literally, see
the world), whereas tabular data generally affords no such luxuries -- but the bottom line of this
excursis is my advocacy (which I am certainly not unique for) for human oversight, and the rigorous
model-vetting it should beget, irrespective of how trustable the concerned methods and data are, in
principle.
%

\marginpar{\raggedleft\textbf{Learning from human preferences and scaling supervision}}
%
Recent work on AI-alignment has proposed `scaling supervision' (understanding `supervision' to mean
what I called before `oversight') of generative-language models by using human-aligned LLMs both
not only as the model-to be-supervised, but also as the supervisor, acting according to a set of
values (in the form of prompts), or `constitution', specified by the practitioner
\citep{bowman2022measuring, bai2022constitutional}.
%
Such supervisors exhibit the ability to accurately, and well-calibratedly, detect biases violating
said constitution in generated responses and provide feedback for redressing them; this feedback
may be used to further align the supervised model, as substitute for the human-generated feedback
fuelling the eponymous Reinforcement Learning from Human Feedback (RLHF; \cite{christiano2017deep,
stiennon2020learning, bai2022training}).
%
While RLHF does not allow for sidestepping of annotations (which has been ameliorated by
unprecedented levels of crowdsourcing), tuning based on preferences (given for pairs of responses)
allows for increased expressivity compared with traditional supervised approaches, pertinent when
the bias exists on a (difficult-to-quantify) spectrum or is fundamentally subjective in nature
(different individuals or entire cultures have different conceptions may what is considered
`offensive', for example).
%
While RLXF (to coin a catch-all initialism for reinforcement learning from some kind of feedback)
affords a spectacularly simple and general paradigm in terms of alignment, alignment and
distributional-robustness (that we view throughout this thesis in terms of invariance and
worst-group performance) should not be confused as one and the same even though they can align --
for some tasks there is no subjective element to engender the notion of `preference', i.e. the
value system is fixed (inherently or by legislation), and so may be the answer-set (e.g the
label-set for closed-set classification). 
%
Thus, while one may use RLXF to obtain generative models that are more helpful/harmless/honest
(the `HHH triad') -- a sense of `debiasing' -- it is no panacea for problems of the nature discussed in this
thesis, especially so for specialised tasks for which there is little prior knowledge -- acquired
from large-scale pre-training -- to be leveraged; methods like those presented herein might then
still have their place in this brave new world of ML where multi-modal foundation models and RLXF
putatively rule the roost (or at least the headlines).
%
The premise of scaling supervision, via review, as in \cite{bai2022constitutional}, may have
broader application, however: for image-classification, one could conceive of a model that
classifies based on natural-language descriptions of a scene, providing a reasoning mechanism that
is evaluable by `constitutional' critics and human auditors (who, again, should very much remain
part of the equation -- they are to be assisted not superseded).
%

%
\marginpar{\raggedleft\textbf{Theoretical-groundedness}}
%
In Chapter 4 we provided some theoretical guarantees regarding when the proposed support-matching
should succeed, given certain assumptions about the data-generating distribution.
%
Chapter 5, however, features no such proofs -- only intuition -- for the efficacy of its respective
method, that being a notable weakness of the current incarnation of the paper -- having such may
serve to guide us regarding the configuration of the matching algorithm and thereby obviate the
somewhat-lengthy hyperparameter-selection procedure went through to obtain the presented results.
%
In Chapter 3, on the other hand, we leverage an established infomin framework, around which there
is a significant body of existing theoretical work -- both within the context of the various
subfields ML concerned with it (DA, DG, AF) and within the broader context of game theory,
dynamical systems theory, and information theory -- however, we again furnish only intuition and
empirical claims for the stabilisation techniques, for which (proof-driven) theoretical-grounding
would be desirable.

% \marginpar{On the manual and automatic identifiability of bias}
%
% Problem of underspecification: how do we know that intervention is required if the validation set
% is drawn from the same distribution as the training set? The obvious answer is with tools from
% interpretable/explainable machine-learning (xAI); this demands a human-in-the-loop element but it
% seems reasonable to expect any system designed for public-facing, critical-decision-making should
% be subject to rigorous auditing. MaskTune takes advantage of feature-attribute to encourage a
% model to learn multiple views of the data, rather than just the one generated by a spurious
% correlation.

% Using pre-trained models to automatically detect biases (according to a `constitution
% \citep{bai2022constitutional}): for text-based tasks, RLHF has emerged as the leading paradigm
% for aligning the output of generative models with human values (along the competing axes of
% `helpfulness'  \citep{scheurer2022training}, `harmlessness' \citep{bai2022training} and `honest'). While
% RLHF is annotation-intensive -- though this can be ameliorated with crowdsourcing --
% \cite{bai2022constitutional} showed that given an existing RLHF-trained model, one can improve
% performance along the harmlessness axis (i.e. de-bias the model) based on the guidance of the
% RLHF model itself, as given by some constitution (a set of values encoded in the form of
% prompts); this variant of RLHF has been appropriately termed RLAIF (Reinforcement Learning from
% AI feedback).
% While RLHF was initially conceived for, and has been almost-exclusively since applied to,
% text-generation (giving rise to a wave of AI assistants, most famously embodied by ChatGPT), the
% paradigm is a fundamentally modality-agnostic, this fact recently capitalised on by
% \cite{lee2023aligning} in extending RLHF to text-to-image generation.
%
% One might fancy a further broadening of the paradigm to conventional supervised-learning
% tasks, wherein the model is constructed to give natural-language explanations (which goes beyond,
% thinking of vision-language tasks, beyond standard captioning which entail no explanation, only
% description) for its classification, rather than mere logits; RLHF/RLAIF can then be conducted
% using said explanations.
%
% Such an approach, however, if practically realisable, while powerful, would only apply to cases
% where large-scale pre-training is leverageable, that is those tasks that are not highly
% task/domain-specific (e.g. medical-imaging tasks and those of a tabular nature generally, for
% which there is no underlying structure to the input domain); 
% methods such as those presented in
% this thesis still have their place even in this brave new world of ML where multi-modal
% foundation models and RL(HF/AI)F seemingly rule the roost (or at least the headlines).
% 


\section*{Of what has since become and might yet be}
