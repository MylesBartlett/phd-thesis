%%%%%%%%% ICML 2021 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%

%\documentclass{article}

%% Optional math commands from https://github.com/goodfeli/dlbook_notation.
%\input{math_commands.tex}

%% Recommended, but optional, packages for figures and better typesetting:
%\usepackage{microtype}
%\usepackage{graphicx}
%\usepackage{wrapfig}
%\usepackage{caption}
%\usepackage{subcaption}
%\usepackage{booktabs} % for professional tables
%\usepackage{amsfonts}       % blackboard math symbols
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{amsthm}
%\usepackage{nicefrac}       % compact symbols for 1/2, etc.
%\usepackage{xcolor}         %for colouring the text on colourMNIST
%\usepackage{comment}
%\usepackage{enumitem}
%\newtheorem{prop}{Proposition}
%\newcommand{\Xcal}{\mathcal{X}}
%\newcommand{\Bcal}{\mathcal{B}}
%\newcommand{\Ycal}{\mathcal{Y}}
%\newcommand{\Scal}{\mathcal{S}}
%\newcommand{\Lcal}{\mathcal{L}}
%\newcommand{\Dcal}{\mathcal{D}}
%\newcommand{\ie}{i.\,e.}
%\newcommand{\Ie}{I.\,e.}
%\newcommand{\eg}{e.\,g.}
%\newcommand{\Eg}{E.\,g.}
%% \newcommand{\xmark}{\text{\ding{55}}}
%\usepackage{pifont}% http://ctan.org/pkg/pifont
%\newcommand{\cmark}{\ding{51}}%
%\newcommand{\xmark}{\ding{55}}%

%% hyperref makes hyperlinks in the resulting PDF.
%% If your build breaks (sometimes temporarily if a hyperlink spans a page)
%% please comment out the following usepackage line and replace
%% \usepackage{icml2021} with \usepackage[nohyperref]{icml2021} above.
%\usepackage{hyperref}

%% Attempt to make hyperref and algorithmic work together better:
%\newcommand{\theHalgorithm}{\arabic{algorithm}}

%% Use the following line for the initial blind version submitted for review:
%\usepackage{icml2021}

%% If accepted, instead use the following line for the camera-ready submission:
%%\usepackage[accepted]{icml2021}

%% The \icmltitle you define below is probably too long as a header.
%% Therefore, a short form for the running title is supplied here:
%\icmltitlerunning{Appendix}

%\begin{document}

%\twocolumn[
%\icmltitle{APPENDIX:\\
%~\\
%Learning with Perfect Bags:\\
%Addressing Hidden Stratification with Zero Labe led Data}
%\icmlsetsymbol{equal}{*}

%\begin{icmlauthorlist}
%\icmlauthor{Aeiau Zzzz}{to}
%\end{icmlauthorlist}

%\icmlaffiliation{to}{Department of Computation, University of Torontoland, Torontoland, Canada}

%\icmlcorrespondingauthor{Cieua Vvvvv}{c.vvvvv@googol.com}

%\icmlkeywords{semi-supervised learning, dataset bias}

%\vskip 0.3in
%]

%% this must go after the closing bracket ] following \twocolumn[ ...

%\printAffiliationsAndNotice{}  % leave blank if no need to mention equal contribution
%% \printAffiliationsAndNotice{\icmlEqualContribution} % otherwise use the standard text.

%\appendix

% \subsection{Why not use a fair clustering method?}
% Current fair clustering methods \citep{chierichetti2017fair, backurs2019scalable, huang2019coresets} cluster based on the protected attribute and thus are not applicable to our setting in which the deployment set is unlabelled and the training set is incomplete with respect to $s$.

% \begin{figure*}[t]
% \centering
% \includegraphics[width=0.6\textwidth,page=2]{paper3/figures/ideal.pdf}
% \caption{Overview of the zero-shot stratification problem. Training dataset (pairs of input data $x$ and class label $y$) will only contain data points that are labelled ``yes'' by the decision policy. This systematic bias might result in a subgroup(s) to have zero labelled data (in the example above, ``married'' subgroup has zero labelled data). The subgrouping information $s$ is unavailable/unlabelled at deployment time, and is only partially labelled at train time.}
% \label{fig:censoring}
% \end{figure*}

\section{Appendix}\label{sec:zsf-appendix}
\subsection{Results for Adult Income}\label{sec:adult-results}
\begin{figure*}[p]
    \centering
    \includegraphics[width=\columnwidth]{paper3/figures/adult_partial_acc.pdf}
    \includegraphics[width=\columnwidth]{paper3/figures/adult_partial_prr.pdf}
    \includegraphics[width=\columnwidth]{paper3/figures/adult_partial_tprr.pdf}
    \includegraphics[width=\columnwidth]{paper3/figures/adult_partial_tnrr.pdf}
    \caption{%
    Results for the Adult Income dataset with \emph{subgroup bias},
    for the binary classification task of predicting whether an individual earns $>$\$50,000 with a binary subgrouping based on \emph{gender}.
    \texttt{ERM (LD)} refers to a model based on ERM (empirical risk minimisation),
    trained on a \emph{l}abelled \emph{d}eployment set; thus not suffering from bias in the training set.
    \textsc{Top left}: Accuracy.
    \textsc{Top right}: Positive rate ratio.
    \textsc{Bottom left}: True positive rate ratio.
    \textsc{Bottom right}: True negative rate ratio.
    For the \texttt{Ranking} clustering, the clustering accuracy was 69.7\% $\pm$ 0.3\%;
    for \texttt{K-means} it was 43\% $\pm$ 3\%.
    }%
    \label{fig:adult-subgroup-bias}
\end{figure*}
\begin{figure*}[p]
    \centering
    \includegraphics[width=\columnwidth]{paper3/figures/adult_miss_s_acc.pdf}
    \includegraphics[width=\columnwidth]{paper3/figures/adult_miss_s_prr.pdf}
    \includegraphics[width=\columnwidth]{paper3/figures/adult_miss_s_tprr.pdf}
    \includegraphics[width=\columnwidth]{paper3/figures/adult_miss_s_tnrr.pdf}
    \caption{%
    Results for the Adult Income dataset with a \emph{missing subgroup},
    for the binary classification task of predicting whether an individual earns $>$\$50,000 with a binary subgrouping based on \emph{gender}.
    \texttt{ERM (LD)} refers to a model based on ERM (empirical risk minimisation),
    trained on a \emph{l}abelled \emph{d}eployment set; thus not suffering from bias in the training set.
    \textsc{Top left}: Accuracy.
    \textsc{Top right}: Positive rate ratio.
    \textsc{Bottom left}: True positive rate ratio.
    \textsc{Bottom right}: True negative rate ratio.
    For the \texttt{Ranking} clustering, the clustering accuracy was 60.4\% $\pm$ 0.8\%;
    for \texttt{K-means} it was 44\% $\pm$ 3\%.
    }%
    \label{fig:adult-missing-subgroup}
\end{figure*}
Figures~\ref{fig:adult-subgroup-bias} and \ref{fig:adult-missing-subgroup} show results from our method on the Adult Income dataset \cite{Dua:2017}.
This dataset is a common dataset for evaluating fair machine learning models. 
Each instance in the dataset is described by $14$ characteristics including gender, education, marital status, number of work hours per week among others, along with a label denoting income level ($\geq$\$50K or not). 
We transform the representation into $62$ real and binary features along with the subgroup label $s$. %%%%%%%%CHECK THESE VALUES%%%%%%%%%%%% 
The dataset is naturally imbalanced with respect to gender: 30\% of the males are labelled as earning more than \$50K per year (high income), while only 11\% of females are labelled as such.
For further details on the dataset construction, see section~\ref{ssec:dataset-construction-adult}.
%
Following standard practice in algorithmic fairness, e.g. \citet{zemel2013learning}, we consider gender to be the subgroup label $s$.

We study the following two settings.
1) \emph{subgroup bias}: we have labelled training data for males ($s=1$) with both positive and negative outcomes, but for the group of females ($s=0$), we only observe the one-sided negative outcome, so the source $\Omega_{y=1,s=0}$ is missing;
2) \emph{missing subgroup}: we have training data for males with positive and negative outcomes, but do not have labelled data for females, i.e.both \ $\Omega_{y=1,s=0}$ and $\Omega_{y=0,s=0}$ are missing.

As before, \texttt{Ranking}, \texttt{k-means}, \texttt{No bal.}\ and \texttt{Perfect} refer to our method with different procedures for constructing (approximately) perfect bags.
As baseline methods, we have \texttt{ERM} (standard empirical risk minimisation with balanced batches), \texttt{DRO} \citep{HasSriNamLia18}, \texttt{gDRO} \citep{sagawa2019distributionally}
and \texttt{ERM (LD)} which is the same model as \texttt{ERM}, but trained on the labelled deployment set, in addition to the training set.

In both settings, we observe the same order as for the other dataset in terms of accuracy: \texttt{Perfect} (with ground truth labels for balancing) achieves the highest performance, followed by \texttt{Ranking}, then \texttt{No bal.}, and finally \texttt{k-means}.
However, for the \emph{missing subgroup} setting, \texttt{Ranking} and \texttt{Perfect} are almost identical and the former performs better in terms of de-biasing metrics.
This decreased reliance on balancing can be explained by the additional supervision that comes with having two sources missing instead of one - in order for the discriminator to distinguish between bags from the deployment set and bags from the training set, the former need only contain \emph{one} of the two missing sources.

Generally, we observe a high variance in the results. This is not attributable to our method, however, with the baselines exhibiting the same behaviour, but rather to the fact that the Adult Income dataset is a very noisy dataset which, at the best of times, allows only about 85\% accuracy to be attained (see also \cite{agrawal2020debiasing}). The problem is that samples vary widely in how informative they are. This, coupled with our artificially biasing the dataset to be even more biased (as \emph{subgroup bias} and \emph{missing subgroup}), makes the achievable performance very dependent on which samples the classifier gets to see, which varies according to the random seed used for the data set split.

\subsection{Dataset Construction}\label{sec:dataset-construction}

\paragraph{Coloured MNIST biasing parameters.}
To simulate a real-word setting where the data, labelled or otherwise, is not naturally balanced, we bias the Coloured MNIST training and deployment sets by downsampling certain colour/digit combinations. The proportions of each such combination \emph{retained} in the \emph{subgroup bias} (in which we have one source missing from the training set) and \emph{missing subgroup} (in which we have two sources missing from the training set) are enumerated in table~\ref{color_mnist_biasing_po} and \ref{color_mnist_biasing_id}, respectively.
For the 3-digit-3-colour variant of the problem, no biasing is applied to either the deployment set or the training set (the missing combinations are specified in the caption accompanying figure~\ref{fig:cmnist-3dig-4miss-add}); this variant was experimented with only under the subgroup-bias setting.

\begin{table}[ht]
\caption{Biasing parameters for the training (left) and deployment (right) sets of Coloured MNIST in the \emph{subgroup bias} setting.}
\label{color_mnist_biasing_po}
\centering
\begin{tabular}{lcc}
\toprule
Combination   & \multicolumn{2}{c}{Proportion retained} \\ \cmidrule(lr){2-3}
  & training set & deployment set \\ \midrule
(y = 2, s = {\color{purple}purple}) & 1.0  & 0.7 \\
(y = 2, s = {\color{green}green})   & 0.3  & 0.4 \\
(y = 4, s = {\color{purple}purple}) & 0.0  & 0.2 \\
(y = 4, s = {\color{green}green})   & 1.0  & 1.0 \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Biasing parameters for the training (left) and deployment (right) sets of Coloured MNIST in the \emph{missing subgroup} setting.}
\label{color_mnist_biasing_id}
\centering
\begin{tabular}{lcc}
\toprule
Combination   & \multicolumn{2}{c}{Proportion retained} \\ \cmidrule(lr){2-3}
  & training set & deployment set \\ \midrule
(y = 2, s = {\color{purple}purple}) & 0.0  & 0.7 \\
(y = 2, s = {\color{green}green})   & 0.85 & 0.6 \\
(y = 4, s = {\color{purple}purple}) & 0.0  & 0.4 \\
(y = 4, s = {\color{green}green})   & 1.0  & 1.0 \\
\bottomrule
\end{tabular}
\end{table}

\paragraph{Adult Income.}\label{ssec:dataset-construction-adult}
For the Adult Income dataset, we do not need to apply any synthetic biasing as the dataset naturally contains some bias wrt $s$. Thus, we instantiate the deployment set as just a random subset of the original dataset. Explicit balancing of the test set is needed to yield meaningful evaluation, however, namely in the penalising of biased classifiers, but need be taken in doing so. Balancing the test set such that
\begin{align}
    |\{x \in X |s=0, y=0\}| &= |\{x \in X |s=1, y=0\}|    \nonumber\\
    \text{and}~|\{x \in X |s=0, y=1\}| &= |\{x \in X |s=1, y=1\}|
\end{align}
where for both target classes, $y=0$ and $y=1$, the proportions of the groups $s=0$ and $s=1$ are made to be the same, is intuitive, yet at the same time precludes sensible comparison of the accuracy/fairness trade-off of the different classifiers.
Indeed, with the above conditions, a majority classifier (predicting all 1s or 0s) achieves comparable accuracy to the fairness-unaware baselines, while also yielding perfect fairness, by construction.
This observation motivated us to devise an alternative scheme, where we balance the test set according to the following constraints
\begin{align}
    & |\{x \in X |s=0, y=0\}| 
    = |\{x \in X |s=0, y=1\}|  \nonumber \\
    = &|\{x \in X |s=1, y=1\}|
    = |\{x \in X |s=1, y=0\}|~.
 \end{align}
That is, all subsets of $\gS \times \gY$ are made to be equally sized. Under this new scheme the accuracy of the the majority classifier is 50\% for the binary-classification task.


\subsection{Optimisation}
\begin{table*}[p]
 \centering
 \caption{Selected hyperparameters for experiments with Coloured MNIST, Adult and CelebA datasets.}
 \label{tab:hparams}
\scalebox{0.65}{
 \begin{tabular}{llll}
 \toprule
 & \textsc{Coloured MNIST} & \textsc{Adult} & \textsc{CelebA}       
 \\ & 2-dig SB / 2-dig MS / 3-dig SB
 \\ \midrule
 Input size  &   $3 \times 32 \times 32$ & $61$ & $3 \times 64 \times 64$ \\  \midrule
 \multicolumn{4}{c}{Autoencoder}                     \\ \midrule
 Levels                      & $4$         & $1$    & $5$\\
 Level depth                 & $2$         & $1$    & $2$\\
 Hidden units / level        & $[32, 64, 128, 256]$ & $[61]$ & $[32, 64, 128, 256, 512]$\\
 Activation                  & GELU        & GELU   & GELU   \\
 Downsampling op.  & Strided Convs. & -- & Strided Convs.\\
 Reconstruction loss         & MSE         & Mixed$^1$  & MSE \\
 Learning rate               & $1 \times 10^{-3}$   & $1 \times 10^{-3}$  & $1 \times 10^{-3}$ \\ \midrule
 \multicolumn{4}{c}{Clustering}                      \\ \midrule
 Batch size                  & $256$      & $1000$  & --\\
 AE pre-training epochs      & $150$        & $100$ & --   \\
 Clustering epochs           & $100$       & $300$  & -- \\
 Self-supervised loss & Cosine + BCE & Cosine + BCE & --\\
 U (for ranking statistics)             & $5$         & $3$     & --    \\   \midrule
 \multicolumn{4}{c}{Distribution Matching}                   \\ \midrule
 Batch size & $1$/$32$/$14$  & $64$   & $32$ \\
 Bag size   & $256$/$8$/$18$ & $32$ & $8$ \\
 Training iterations    & $8\text{k}/8\text{k}/20\text{k}$ & $5\text{k}$ & $15\text{k}$ \\
 Encoding ($z$) size$^2$  & $128$   & $35$  & $128$ \\
 Binarised $z_s$ & {\boxedsymbols ✗}\, / \cmark\, / \cmark & \xmark & \xmark \\
 $y$-predictor weight ($\lambda_1$) & $1$ & $0$ & $1$  \\ 
 $s$-predictor weight ($\lambda_2$) & $1$ & $0$ & $0$  \\ 
 Adversarial weight ($\lambda_3$)   & $1 \times
 10^{-3}$   & $1$   & $1$\\ 
 Stopgradient $\left(\nabla_\theta h_\psi(f_\theta(X^\mathit{dep}))=0\right)$ & \xmark & \cmark & \xmark \\
 \midrule
 \multicolumn{4}{c}{Predictors}   \\ \midrule
 Learning rate  & $3 \times 10^{-4}$ &   $1 \times 10^{-3}$  $ 1 \times 10^{-3}$\\
 \midrule
 \multicolumn{4}{c}{Discriminator}                   \\ \midrule
 Attention mechanism$^3$    & Gated   & Gated & Gated \\
 Hidden units pre-aggregation  & $[256, 256]$  & $[32]$ & $[256, 256]$\\
 Hidden units post-aggregation & $[256, 256]$ & --  & $[256, 256]$ \\
 Embedding dim (for attention) & $32$ & $128$ & $128$ \\
 Activation & GELU & GELU & GELU \\
 Learning rate  & $3 \times 10^{-4}$    & $1 \times 10^{-3}$ & $1 \times 10^{-3}$\\
 Updates / AE update    & $1$  & $3$    & $1$    \\
 \bottomrule
 \multicolumn{4}{l}{\footnotesize $^1$ Cross-entropy is used for categorical features, MSE for continuous features.} \\
  \multicolumn{4}{p{\textwidth}}{\footnotesize $^2$ $|z|$ denotes the combined size of $z_s$ and $z_y$, with the former occupying $\ceil{\text{log}_2(\mathcal{S})}$ dimensions, the latter the remaining } \\
  \multicolumn{4}{p{\textwidth}}{\footnotesize dimensions.} \\
  \multicolumn{4}{p{\textwidth}}{\footnotesize $^3$ The attention mechanism used for computing the sample-weights within a bag. \emph{Gated} refers to gated attention  proposed by
%   \cite{ilse2018attention}, while \emph{SDP} refers to the scaled dot-product attention proposed by \cite{vaswani2017attention}.
  } \\ 
  \multicolumn{4}{p{\textwidth}}{\footnotesize  \cite{ilse2018attention}.} \\ 
 \end{tabular}
}
\end{table*}

The hyperparameters and architectures for the Autoencoder (\texttt{AE}), Predictor and Discriminator subnetworks used for the experiments with all datasets are detailed in Table \ref{tab:hparams}. All networks are trained using the Adam optimiser \citep{kingma2015adam}.

For the Coloured MNIST and CelebA datasets, the baseline \texttt{CNN}, \texttt{DRO}, and \texttt{LfF} (in the case of the former) models use an architecture identical to that of the encoder with two exceptions: 1) max-pooling being used for spatial downsampling instead of strided convolutions; 2) the final convolutional layer is followed by a global average pooling layer followed by a fully-connected classification layer. For evaluating our method, we simply train a linear classifier on top of $z_y$; this is sufficient due to linear-separability being enforced during training by the $y$-predictor.
For the Adult Income dataset, we use an \ac{MLP} made up of a single hidden layer -- 35 units in size -- followed by a SELU activation \citep{klambauer2017self}, as both both the downstream classifier for our method, and as the network architecture of the baselines. 
All baselines and downstream classifiers alike were trained for $60$ epochs with a learning rate of $1 \times 10^{-3}$ and a batch size of $256$.

Since, by design, we do not have labels for all subgroups the model will be tested on, and bias against these missing subgroups is what we aim to avoid, properly validating, and thus conducting hyperparameter selection for models generally, is not straightforward.
We can use estimates of the mutual information between the learned-representation and $s$ and $y$ (which we wish to minimise w.r.t.\ to the former, maximise w.r.t.\ the latter) to guide the process, though optimising the model w.r.t.\ these metrics obtained from only the training set does not guarantee generalisation to the missing subgroups.
We can, however, additionally measure the entropy of the predictions on the encoded test set and seek to maximise it across all samples, or alternatively train a discriminator of the same kind used for distribution matching as a measure the shift in the latent space between datasets.
We use the latter approach (considering, the learned distance between subspace distributions, accuracy, and reconstruction loss) to inform an extensive grid-search over the hyperparameter space of our model.

For the \texttt{DRO} baseline, we allowed access to the labels of the test set for the purpose of hyperparameters selection, performing a grid-search over multiple splits to avoid overfitting to any particular instantiation.
Specifically, the threshold ($\eta$) parameter for \texttt{DRO} was determined by a grid-search over the space $\{0.01, 0.1, 0.3, 1.0\}$. The same procedure was carried out for selecting the model capacity constant ($C$) of the related \texttt{gDRO} baseline.

In addition to the losses stated in the distribution matching objective, $\mathcal{L}$, in the main text, we also regularise the encoder by the $\ell^2$ norm of its embedding, multiplied by a small pre-factor, finding this to work better than more complex regularisation methods, such as spectral normalisation \citep{miyato2018spectral}, for stabilising training.

\subsection{Visualisations of results}\label{sec:qual-results}
Figures~\ref{fig:attn_maps} and \ref{fig:cmnist-recon} show some additional visualisations for our results.
For details, see the captions.
\begin{figure*}[tb]
  \centering
    % \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{paper3/figures/celeba_attn_map.png}
    % \end{subfigure}
    % \hfill
    % \begin{subfigure}[b]{0.49\textwidth}
    \includegraphics[width=0.9\textwidth]{paper3/figures/cmnist_attn_map.png}
    % \end{subfigure}
  \caption{
    Example sample-wise attention maps for bags of CelebA (left) and Coloured MNIST (right) images sampled from a balanced deployment set. The training set is biased according to the SB setting where for CelebA ``smiling females'' constitute the missing source and for Coloured MNIST {\color{purple}purple} fours constitute the missing source. The attention weights are used during the discriminator's aggregation step to compute a weighted sum over the bag. The attention-weight assigned to each sample is proportional to the lightness of its frame, with black signifying a weight of 0, white a weight of 1. Those samples belonging to the missing subgroup are assigned the highest weight as they signal from which dataset (training vs. deployment) the bag containing them was drawn from.
  }%
  \label{fig:attn_maps}
\end{figure*}

% \subsection{Qualitative results}\label{sec:qual-results}
\begin{figure*}[tb]
  \centering
  \begin{subfigure}[b]{0.39\textwidth}
    \centering
    \includegraphics[width=\textwidth]{paper3/example_images/fresh-dawn-2179_train_reconstructions_9900.png}
    \caption{
    Different reconstructions on the training set.
    Corresponding to: original, full reconstruction, reconstruction of $z_y$, reconstruction of $z_s$.
    }%
    \label{fig:cmnist-recon-training}
  \end{subfigure}
  % \hfill
  \quad
  \begin{subfigure}[b]{0.39\textwidth}
    \centering
    \includegraphics[width=\textwidth]{paper3/example_images/fresh-dawn-2179_context_reconstructions_9900.png}
    \caption{
    Different reconstructions on the deployment set.
    Corresponding to: original, full reconstruction, reconstruction of $z_y$, reconstruction of $z_s$.
    }%
    \label{fig:cmnist-recon-deployment}
  \end{subfigure}
  \caption{
   Visualisation of our method's solutions for the Coloured MNIST dataset, with {\color{purple}purple} as the missing subgroup.
   In each of the subfigures \ref{fig:cmnist-recon-training} and \ref{fig:cmnist-recon-deployment}:
   Column 1 shows the original images from $x$ from the respective set.
   Column 2 shows plain reconstructions generated from $x_\textit{recon}=g(f_y(x), f_s(x))$.
   Column 3 shows reconstruction with zeroed-out $z_s$: $g(f_y(x), 0)$, which effectively visualises $z_y$.
   Column 4 shows the result of an analogous process where $z_y$ was zeroed out instead.
  }%
  \label{fig:cmnist-recon}
\end{figure*}
% Given a learned invariant representation, we can generate a reconstruction to visualise the information contained in it.
% An example of this can be seen in \figref{fig:3dig-examples}.
% This is from our experiment with 3 digits in Coloured MNIST.
% We can clearly see that the reconstructed invariant representation has lost all colour information;
% instead all digits are magenta-coloured, which was the majority colour in the training set.

\subsection{Code}
The code will be published at the following URL: \url{https://github.com/predictive-analytics-lab/fair-dist-matching}.
Instructions on how to run them can be found in the \texttt{README.md}.

\subsection{Additional metrics}
Figures~\ref{fig:cmnist-2v4-partial-add}, \ref{fig:cmnist-2v4-miss-s-add},  and \ref{fig:celeba-gender-smiling-add} show the true positive rate (TPR) ratio and the true negative rate (TNR) ratio as additional metrics for Coloured MNIST (2 digits) and CelebA.
These are computed as the ratio of TPR (or TNR) on subgroup $s=0$ over the TPR (or TNR) on subgroup $s=1$; if this gives a number greater than 1, the inverse is taken.
Similarly to the PR ratio reported in the main paper, these ratios give an indication of how much the prediction of the classifier depends on the subgroup label $s$.

Figure~\ref{fig:cmnist-3dig-4miss-add} shows metrics specific to multi-valued $s$ (\ie, non-binary $s$).
We report the minimum (i.e. farthest away from 1) of the pairwise ratios (PR/TPR/TNR ratio min) as well as the largest difference between the raw values (PR/TPR/TNR diff max) . 
Additionally, we compute the Hirschfeld-Gebelein-R\'enyi (HGR) maximal correlation \citep{renyi1959measures} between $S$ and $Y$, serving as a measure of dependence defined between two variables with arbitrary support.
\begin{figure*}[htp]
  \centering
%   \includegraphics[width=\columnwidth]{paper3/figures/cmnist_2v4_partial_acc.pdf}
%   \includegraphics[width=\columnwidth]{paper3/figures/cmnist_2v4_partial_pr.pdf}
%   \includegraphics[width=\columnwidth]{paper3/figures/cmnist_2v4_partial_tpr.pdf}
%   \includegraphics[width=\columnwidth]{paper3/figures/cmnist_2v4_partial_tnr.pdf}
  \includegraphics[width=0.8\columnwidth]{paper3/figures/cmnist_2v4_partial_overcluster_acc.pdf}
  \includegraphics[width=0.8\columnwidth]{paper3/figures/cmnist_2v4_partial_overcluster_prr.pdf}
  \includegraphics[width=0.8\columnwidth]{paper3/figures/cmnist_2v4_partial_overcluster_tprr.pdf}
  \includegraphics[width=0.8\columnwidth]{paper3/figures/cmnist_2v4_partial_overcluster_tnrr.pdf}
  \caption{
    Results from 30 repeats for the Coloured MNIST dataset with two digits, 2 and 4, with \emph{subgroup bias} for the colour `{\color{purple}purple}': for {\color{purple}purple}, only the digit class `2' is present.
    \textsc{Top left}: Accuracy.
    \textsc{Top right}: Positive rate ratio.
    \textsc{Bottom left}: True positive rate ratio.
    \textsc{Bottom right}: True negative rate ratio.
    For the \texttt{Ranking} clustering, the clustering accuracy was 96\% $\pm$ 6\%;
    for \texttt{K-means} it was 64\% $\pm$ 10\%.
    For an explanation of \texttt{Ranking (8)} and \texttt{K-means (8)} see section~\ref{sec:overclustering}.
  }%
  \label{fig:cmnist-2v4-partial-add}
\end{figure*}
\begin{figure*}[htp]
  \centering
%   \includegraphics[width=\columnwidth]{paper3/figures/cmnist_2v4_miss_s_alt_acc.pdf}
%   \includegraphics[width=\columnwidth]{paper3/figures/cmnist_2v4_miss_s_alt_pr.pdf}
%   \includegraphics[width=\columnwidth]{paper3/figures/cmnist_2v4_miss_s_alt_tpr.pdf}
%   \includegraphics[width=\columnwidth]{paper3/figures/cmnist_2v4_miss_s_alt_tnr.pdf}
  \includegraphics[width=0.8\columnwidth]{paper3/figures/cmnist_2v4_miss_s_overcluster_acc.pdf}
  \includegraphics[width=0.8\columnwidth]{paper3/figures/cmnist_2v4_miss_s_overcluster_prr.pdf}
  \includegraphics[width=0.8\columnwidth]{paper3/figures/cmnist_2v4_miss_s_overcluster_tprr.pdf}
  \includegraphics[width=0.8\columnwidth]{paper3/figures/cmnist_2v4_miss_s_overcluster_tnrr.pdf}
  \caption{
    Results from 30 repeats for the Coloured MNIST dataset with two digits, 2 and 4, with a \emph{missing subgroup}: the training dataset only has {\color{green}green} digits.
    \textsc{Top left}: Accuracy.
    \textsc{Top right}: Positive rate ratio.
    \textsc{Bottom left}: True positive rate ratio.
    \textsc{Bottom right}: True negative rate ratio.
    For the \texttt{Ranking} clustering, the clustering accuracy was 88\% $\pm$ 5\%;
    for \texttt{K-means} it was 72\% $\pm$ 16\%.
    For an explanation of \texttt{Ranking (8)} and \texttt{K-means (8)} see section~\ref{sec:overclustering}.
  }%
  \label{fig:cmnist-2v4-miss-s-add}
\end{figure*}
\begin{figure*}[htp]
  \centering
%   \includegraphics[width=\columnwidth]{paper3/figures/cmnist_3dig_4miss_hgr.pdf}\\
  \includegraphics[width=0.8\columnwidth]{paper3/figures/cmnist_3dig_4miss_prr-min.pdf}
  \includegraphics[width=0.8\columnwidth]{paper3/figures/cmnist_3dig_4miss_prd-max.pdf}
  \includegraphics[width=0.8\columnwidth]{paper3/figures/cmnist_3dig_4miss_tprr-min.pdf}
  \includegraphics[width=0.8\columnwidth]{paper3/figures/cmnist_3dig_4miss_tprd-max.pdf}
  \includegraphics[width=0.8\columnwidth]{paper3/figures/cmnist_3dig_4miss_tnrr-min.pdf}
  \includegraphics[width=0.8\columnwidth]{paper3/figures/cmnist_3dig_4miss_tnrd-max.pdf}
  \caption{
    Results from 30 repeats for the Coloured MNIST dataset with three digits: `2', `4' and `6'.
    Four combinations of digit and colour are missing: {\color{green}green} 2's, {\color{blue}blue} 2's, {\color{blue}blue} 4's and {\color{green}green} 6's.
    % \textsc{First row}: Hirschfeld-Gebelein-R\'enyi maximal correlation between $S$ and $Y$.
    \textsc{First row, left}: minimum of all positive rate ratios.
    \textsc{First row, right}: maximum of all positive rate differences.
    \textsc{Second row, left}: minimum of all true positive rate ratios.
    \textsc{Second row, right}: maximum of all true positive rate differences.
    \textsc{Third row, left}: minimum of all true negative rate ratios.
    \textsc{Third row, right}: maximum of all true negative rate differences.
  }%
  \label{fig:cmnist-3dig-4miss-add}
\end{figure*}
\begin{figure*}[htp]
  \centering
%   \includegraphics[width=\columnwidth]{paper3/figures/celeba_gender_smiling_acc.pdf}
%   \includegraphics[width=\columnwidth]{paper3/figures/celeba_gender_smiling_pr.pdf}
  \includegraphics[width=\columnwidth]{paper3/figures/celeba_gender_smiling_tprr.pdf}
  \includegraphics[width=\columnwidth]{paper3/figures/celeba_gender_smiling_tnrr.pdf}
  \caption{
    Results from 10 repeats for the CelebA dataset with the \emph{subgroup bias} setting.
    The task is to predict ``smiling'' vs ``non-smiling'' and the subgroups are based on gender.
    The subgroup ``female'' is missing samples for the ``smiling'' class.
    % \textsc{Top left}: Accuracy.
    % \textsc{Top right}: Positive rate ratio.
    \textsc{Left}: True positive rate ratio.
    \textsc{Right}: True negative rate ratio.
  }%
  \label{fig:celeba-gender-smiling-add}
\end{figure*}

\subsection{Clustering with an incorrect number of clusters}\label{sec:overclustering}
We also investigate what happens when the number of clusters is set incorrectly.
For 2-digit Coloured MNIST, we expect 4 clusters, corresponding to the 4 possible combinations of the binary class label $y$ and the binary subgroup label $s$.
However, there might be circumstances where the correct number of clusters is not known; how does the batch balancing work in this case?
We run experiments with the number of clusters set to 6 and to 8, while otherwise not changing any part of the method.
It should be noted that this is a very na\"ive way of dealing with an unknown number of clusters.
There are methods specifically designed for identifying the right number of clusters \citep{hamerly2004learning,chazal2013persistence},
and that is what would be used if this situation came up in practice.

The results can be found in figures~\ref{fig:cmnist-2v4-partial-add} and \ref{fig:cmnist-2v4-miss-s-add}.
Bags and batches are constructed by drawing an equal number of samples from each cluster.
Unsurprisingly, the method performs worse than with the correct number of clusters.
When investigating how the clustering methods deal with the larger number of clusters,
we found that it is predominantly those samples that do not appear in the training set
which get spread out among the additional clusters.
This is most likely due to the fact that the clustering is semi-supervised,
with those clusters that occur in the training set having supervision.
The overall effect is that the samples which are not appearing in the training set are overrepresented in the drawn bags,
which means it is easier for the adversary to identify where the bags came from,
and the encoder cannot properly learn to produce an invariant encoding.

% \begin{table*}[tp]
%   \centering
%   \caption{
%     Results on Coloured MNIST dataset for a 3-digits-3-colours task, i.e. classification of the digits \emph{two} versus \emph{four} vs \emph{six} with a protected attribute that can take three values ({\color{purple}purple}, {\color{green}green}, {\color{blue}blue}).
%     We consider the scenarios of learning with subgroup bias with four sources missing (30 repeats). 
%   }
%   \label{tab:colormnist3_sup}
%   \scalebox{0.79}{
%   \begin{tabular}{lccccccc}
%     \hline
%     \multicolumn{6}{c}{}\\
%     \multicolumn{6}{c}{Learning with subgroup bias, the sources
%     $\mathcal{M}_{y=\text{'two'},s=\color{green}green}$,
%     $\mathcal{M}_{y=\text{'two'},s=\color{blue}blue}$,
%     $\mathcal{M}_{y=\text{'four'},s=\color{blue}blue}$ and
%     $\mathcal{M}_{y=\text{'six'},s=\color{green}green}$
%     are invisible.}\\
%     \multicolumn{6}{c}{}\\
%     \hline
%     %
%                                          &         AR min. ratio& TPR min. ratio & TNR min. ratio &   AR max. diff &  TPR max. diff &   TNR max. diff \\
%                              \texttt{ZSF} &    0.604 $\pm$ 0.213 &  0.866 $\pm$ 0.176 &  0.702 $\pm$ 0.292 &  0.236 $\pm$ 0.189 &  0.133 $\pm$ 0.175 &  0.297 $\pm$ 0.291 \\
%  \texttt{DRO \cite{HasSriNamLia18}} &     0.027 $\pm$ 0.05 &  0.077 $\pm$ 0.145 &  0.128 $\pm$ 0.147 &  0.887 $\pm$ 0.101 &  0.923 $\pm$ 0.145 &  0.871 $\pm$ 0.147 \\
%   \texttt{Kamiran \& Calders (2012) CNN} &    0.072 $\pm$ 0.077 &  0.208 $\pm$ 0.217 &  0.039 $\pm$ 0.054 &  0.904 $\pm$ 0.087 &  0.792 $\pm$ 0.217 &  0.961 $\pm$ 0.054 \\
%     \hline
%     \hline
%   \end{tabular}}
% \vspace{-0.5cm}
% \end{table*}
% \bibliography{bibfile}
% \bibliographystyle{icml2021}
% \end{document}
