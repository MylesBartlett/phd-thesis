%
\subsection{Perfect bags}\label{sec:sm-implementation}
%
A visual overview of our pipeline is given in Fig.~\ref{fig:sm-pipeline}. 
%
Borrowing from the \ac{AF} literature \citep{chouldechova17,KleMulRag16}, we refer to a bag in
which all elements of $\gG$ appear in equal proportions as a ``perfect bag'' (even if the balancing
is only approximate). 
%
Our pipeline can be broken down into two steps: 1) sample perfect bags from an unlabelled
deployment set; and 2) produce disentangled representations using the perfect bags via adversarial
support-matching as described in \S\ref{ssec:sm-realisation}.

\textbf{Constructing perfect bags via clustering.}
%
We cluster the data points from the deployment set into \( N^C=|\gG| \) clusters by applying
spherical k-means to CLIP \citep{radford2021learning} embeddings. 
%
Specifically, we use the ResNet-50 version of CLIP, finding this to work better than the ViT-based
variants. 
%
We inject labelled knowledge into the k-means algorithm by initialising the centroids of the known
sources to the mean of their features in the labelled (training) data. 
%
We find this works reasonably well for the considered datasets; since, the aim of this work is to
propose a pipeline for effectively leveraging unlabelled data for invariance-learning, not to set a
new state-of-the-art in clustering, we adopt this simple clustering method for a practical
proof-of-concept compared with the artificial approach of injecting noise into the ground-truth
labels. 
%
The latter procedure is useful, however, for performing a fine-grained sensitivity analysis of our
algorithm \wrt{} clustering accuracy, in that we can simulate the runs of the algorithm at
different levels of noisiness in the bag-sampling. 
%
% We present such an analysis in \S\ref{ssec:sensitivity}.
%
Given, the cluster assignments, we can then stratify the deployment set into perfect bags, to be
used by the subsequent support-matching phase.

As a result of clustering, the data points in the deployment set \( \gD^\mathit{dep} \) are
labelled with cluster assignments generated by clustering algorithm, \( C \), giving \(
\gD^\mathit{dep}_C=\{(x_i, c_i)\} \), \(c_i = C(z_i) \),
%
so that we can form perfect bags from \( \gD^\mathit{dep}_C \) by sampling all clusters at equal
rates; there is no need for application of the $\Pi$ operator since the deployment set is complete
\wrt{} $\gG$.
%
We note that we do \emph{not} have to associate the clusters with specific $s$ or $y$ labels as the
labels are not directly used for supervision.

Balancing bags based on clusters instead of the true labels introduces an error, which we can try
to bound. 
%
For this error-bounding, we assume that the probability distribution distance measure used in
Eq.~\ref{eq:objective} is the \emph{total variation distance} \(TV\). 
%
The proof can be found in Appendix~\ref{sec:sm-theoretical-analysis}.

\begin{theorem}
%
If \(q_f(Z)\) is a data distribution on \(\gZ\) that is a mixture of \(n_y\cdot n_s\) Gaussians,
which correspond to all the unique combinations of \(y\in\gY\) and \(s\in\gS\), and \(p_f(Z)\) is
any data distribution on \(\gZ\), then without knowing \(y\) and \(s\) on \(q_f\), it is possible
to estimate
%
\begin{align}
  \sum\limits_{s^\prime\in\gS}\sum\limits_{y^\prime\in\gY} TV(p_f|_{s\in
  \Pi(s^\prime,y^\prime),Y=y^\prime},
q_f|_{S=s^\prime,Y=y^\prime})
\end{align}
%
with an error that is bounded by \(\tilde{O}(\sqrt{1/N})\) with high probability, where \(N\) is
the number of samples drawn from \(q_f\) for learning.
%
\end{theorem}
%
