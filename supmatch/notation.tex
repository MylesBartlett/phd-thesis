\begin{table}[ht]
\caption{Notation and abbreviations used throughout the paper}
\label{notation}
\centering
\begin{tabular}{p{0.2\linewidth} | p{0.7\linewidth}}
\toprule
%
Notation/Abbreviation   & Definition \\ \midrule
%
$\gX$ & The input domain \\
$k_y$ & The number of unique class labels \\
$k_s$ & The number of unique subgroup labels \\
$[ k_y ]$ & The set of $k_y$ unique class labels; shorthand for $\{ 0, 1, \dots, k_y \}$ \\
$[ k_s ]$ & The set of $k_s$ unique subgroup labels; shorthand for $\{ 0, 1, \dots, k_s \}$ \\
$\gY$ & The domain of class labels (alias for $[ k_y ]$) \\
$\gS$ & The domain of subgroup labels (alias for $[ k_s ]$) \\
$\gG$ & The set of sources, where a ``source'' refers to a unique pair of $s$ and $y$. \\
$y$ & Discrete random variable, defined on $\gY$, encoding the target class. \\
$s$ & Discrete random variable, defined on $\gS$, encoding subgroup membership. \\
$\gD^{tr}$ & Training set of size $N^{tr}$ \\
$\gD^{te}$ & Test set on which we ultimately wish to deploy our model. \\
$\gD^{dep}$ & Deployment set of size $N^{dep}$ that is unlabelled \wrt{} both $s$ and $y$
but has support over the unobserved labels matching that of the test set. \\
$f$ & Encoder function mapping from the input domain $\gX$ to a subgroup-invariant embedding space
$\gZ$. \\
$c$ & Classification head mapping from the embedding space generated by $f$ to the probability
simplex over $\gY$: $c: \gZ \to \bigtriangleup^{|\gY|}$. \\
$\Gamma$ & Classification model composed of encoder $f$ and classification head $c$, $\Gamma
\triangleq c \circ
f$. \\
$h$ & Discriminator network trained to discriminate bags between samples drawn from the training set from
bags of samples drawn from the deployment set. \\
$ \gI(\cdot; \cdot)$ & Mutual Information \\
SC & Spurious Correlations; correlations arising between the target and a secondary attribute
that is acausal \wrt{} said target but that models can exploit to learn shortcut solutions 
that lead to low loss on the training set but poor performance at deployment time when the
correlations no longer hold. \\
MS & \emph{Missing Sources} where we use ``source'' to mean a unique class-subgroup pair and
the missingness is \wrt{} the training set. The problems considered in this paper are MS problems
in that there are sources in the test set that are absent from the training set -- this is a form
of sampling bias that induces classifiers to learn subgroup-specific shortcuts that fail to
generalise to the source-complete test set. \\
$\gM$ & The set of missing sources, $\gG^{te} \setminus \gG^{te}$. \\
SB & \emph{Subgroup Bias} setting characterised by certain certain class-subgroup combinations
missing from the training set but with all classes and subgroups represented. \\
%
\bottomrule
%
\end{tabular}
%
\end{table}

