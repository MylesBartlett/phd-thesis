\subsection{Domain Generalisation}

\subsubsection{Supervised Learning}

\subsubsubsection{I.I.D Learning}

Traditional learning algorithms usually assume (or are only optimal for) that the training and test
samples are \emph{both} variables  identically-and-independently distributed (i.i.d) random
variables, such that one has $P_{tr}}(X, Y) \approx P_{te}(X, Y)$.
Based on this assumption, the method of Empirical Risk Minimisation (ERM; \citet{vapnik1991principles}) seeks the
hypothesis $f: X \rightarrow Y$ that is minimiser, $f^\ast$ of the \emph{expected risk},
\emph{risk}, $\gR$, defined s the expectation of the loss,$\ell: \theta \times \gY \rightarrow
\mathbb{R}_\+$, over the training distribution, $P_{tr}(X, Y)$.
Formally, the risk is defined by: $\allall f \in \gF$,

\equ\begin{equation*} \label{eq:risk}
  \gR(f) := \mathbb{E}_{P_{tr}(X, Y)}\[ \ell (f(X), Y) \]
\end{equation*}
Since in practice one does not have access to the true generative distribution, but only a finite
set of realizations, $\gD_{tr}$ \ref{eq:risk} requires substituting with its empirical counterpart,
the \emph{empirical risk},
$\hat{\gR}$ and uses this a proxy for the aforementioned true risk:

\equ\begin{equation*} \label{eq:risk}
  \hat{\gR(f)) :=  n^{-1} \sum_i=1^n \ell (f(x_i), y_i)
\end{equation*}
% where we recover the stand ard (unweighted) formulation by setting $w := \{w_i}_i=1,^n$

(see \citept{vogel2020weighted} as reference for formulating the traditional ERM setup and
that of its weighted counterpart, also \citep{wang2021importance, semenova2019study,
zhai2022understanding, idrissi2022simple} 

\subsubsubsection{Importance weighting}
\citep{wang2021importance, } 


\subsubsection{In Search of Lost Domain Generalization \citep{gulrajani2020search}}
Machine learning systems often fail to generalise out-of-distribution (OOD), crashing in 
spectacular way when tested outside the domain of training examples.

\itemi\begin{itemize}
  \item Self-driving car systems struggle to perform under conditions different to those of 
    training, including variations in lighting \citep{dai2018dark}, weather \citep{volk2019towards}, 
    and object poses \citep{alcorn2019strike}
  \item Systems trained on medical data collected in one hospital do not generalise to other health
    centres \citep{castro2020causality, albadawy2018deep}
  \item failing to generalise is failing to capture the causal factors of variation in data, 
    clinging instead to easier-to-fit spurious correlations, which are prone to change from 
    training to testing domains (unstable in the face of interventions)
  \item  Examples of spurious correlations (SC) commonly encountered in machine learning include 
    racial biases, texture biases \citep{geirhos2018imagenet}, and object backgrounds
    \citep{beery2018recognition} .
  \item Alas, the capricious behaviour of machine learning systems to distributional shifts is a 
    roadblock to their deployment in critical applications.
\end{itemize}

The goal of DG is OOD generalisation: learning a predictor able to perform well on some unseen test
domain when no data from the test domain is available during training -- we must assume the 
existence of some statistical invariances across training and testing domains.
DG differs from Domain Adaptation (DA) in that the latter assumes that the unlabelled data derived 
from the test domain is available during training.

\subsubsection{Simplicity Bias \citep{shah2020pitfalls}}
\begin{itemize}
  \item In stationary settings where there is no mismatch between the training and test 
    distributions, generalisation is usually maximised by selecting models according to the 
    statistical equivalent of Occam's Razor, of using the simplest model that explains the data 
    well. However, when the training and test distributions are not aligned in some sense, this 
    fails to hold up where the simplest models are those that latch onto spurious statistical 
    correlations in the training data and thus are not stable under non-causal interventions in 
    the marginal or conditional distributions of $p(y|x)$ (resulting in \emph{covariate} shift 
    and \emph{concept} drift respectively).
  \item On real-world datasets there are several distinct ways to discriminate between labels (e.g. 
    based on shape, colour, texture, etc.) that are (a) predictive of the label to varying extents, 
    and (b) define decision boundaries of varying complexity. 
  \item For example, in the image-classification task of swans vs. bears, a linear-like simple 
    classifier that only looks at colour could predict correctly on most instances except white 
    polar bears, while a non-linear complex classifier that infers shape could have almost perfect 
    predictive power.
\end{itemize}

\subsection{Causality}%
\subssubsection{DRO}
\subsection{Underspecification}%
\label{sub:underspecification}

The problem of underspecification can be formalised in terms of 'Rashomon sets' 
\citep{semenova2019study} defined w.r.t. the validation/test set used to guide model-selection and 
indeed assess the feasibility of performing a given task with a machine learning system given the
data available. The Rashomon effect \citep{breiman2001statistical} \footnote{taking its name from 
the Kurosawa film in which four witnesses recount wildly differing versions of the same crime} 
describes the phenomenon in which there exists a non-singular set of equally-performing predictors 
from a given function class $\gF$.
Large Rashomon sets often occur when the machine learning pipeline is underspecified (e.g. due to 
its failure to adequately account for distribution shift occurring at deployment time).
The empirical Rashomon set is a subset of models of the hypothesis space $\gF$ forming an equivalence class
 (according to scoring function $\phi$), with some tolerance, $\epsilon$, w.r.t. the
best model in the class, $f^\ast$. 
We generalise the formulation given in \citet{semenova2019study} to allow 
performance to be measured with respect to an arbitrary dataset,  $\gD_{eval}$, by making this a 
secondary parameter of $\phi$. This generalised form is then given as
\align\begin{align*}
  \mathfrak{R} := \{ f \in \gF: \phi(F, \gD_{eval}) \leq \phi(f^\ast, \gD_{eval}) + \epsilon}
\end{align*}


\subsection{Fairness as an Domain Generalisation problem}
\citet{creager2021environment} cast the problem fair machine learning as one of DG,
where the protected groups takes the role of the different domains/environments. In fairness
literature, the learning objectives represent context-specific fairness notions,
while in OOD literature, the learning objectives should be designed according to invariance 
assumptions.
A number of fair representation learning methods \citep{edwards2015censoring, madras2018learning}
are derived from domain adaptation (DA) methods.
When protected attributes are unknown, DRO and adversarially learning can be applied as in 
\citet{hashimoto2018fairness} and \citet{lahoti2020fairness}, respectively, to obtain a 
distributionally robust predictor and minimizing the worst-subgroup performance; the former can
also be adapted to cases in which such information is known as in \citet{sagawa2019distributionally} 
