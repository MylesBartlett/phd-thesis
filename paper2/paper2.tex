\chapter{Paper 2: Null-sampling for Interpretable and Fair Representations}\label{ch:paper2}
% \documentclass[runningheads]{llncs}
% \pdfoutput=1
% \usepackage[utf8]{inputenc}
% \usepackage{graphicx}
% \usepackage{comment}
% \usepackage{biblatex}
% \usepackage{placeins}
% \usepackage{amsmath,amssymb,amsfonts,bm} % define this before the line numbering.
% \usepackage{color}
% \usepackage{booktabs}

% \usepackage{import}
% \usepackage{appendix}
% \usepackage[caption=false]{subfig}
% \usepackage{mathtools}
% \usepackage{wrapfig}
% \usepackage{hyperref}

% \def\ci{\perp\!\!\!\perp}
% \newcommand*\diff{\mathop{}\!\mathrm{d}}
% \def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% \addbibresource{references.bib}

% \begin{document}

% \pagestyle{headings}
% \mainmatter
% \def\ECCVSubNumber{5488}  % Insert your submission number here

% \title{Null-sampling for Interpretable \\and Fair Representations} % Replace with your title

% \titlerunning{Null-sampling for Interpretable and Fair Representations}
% \author{Thomas Kehrenberg \and
% Myles Bartlett \and
% Oliver Thomas \and \\
% Novi Quadrianto}
% \authorrunning{T. Kehrenberg et al.}
% \institute{Predictive Analytics Lab (PAL), University of Sussex, Brighton, UK
% \email{\{t.kehrenberg,m.bartlett,ot44,n.quadrianto\}@sussex.ac.uk}}
% \maketitle

\textsc{Authors}:\\
Thomas Kehrenberg$^1$, Myles Bartlett$^1$, Oliver Thomas$^1$, and Novi Quadrianto$^1$ \\
\textsc{Affiliations}:\\
$^1$ Predictive Analytics Lab (PAL), University of Sussex, Brighton, UK\\
\textsc{Conference}:\;\;\textit{European Conference on Computer Vision} (ECCV), 2020 \\
% \textsc{DOI}:\;\;10.3389/frai.2020.00033
\textsc{Note}:\;\;The appendix has been included as section~\ref{sec:nifr-appendix}.

% 70-150 words
\section{Abstract}
\noindent
% Training data often contains spurious correlations not reflected in the real world where a trained model will be deployed.
% Consequently, computer vision and machine learning systems can become over-reliant on these correlations for classifications.
% Instead of basing their output on those features which are semantically meaningful, the output can be based on these unintended relationships.
% This is undesirable for many reasons, among them limited generalisation and uncontrolled bias.
We propose to learn invariant representations, in the data domain, to achieve interpretability in algorithmic fairness. 
Invariance implies a selectivity for high level, relevant correlations w.r.t.\ class label annotations, and a robustness to irrelevant correlations with protected characteristics such as race or gender. 
We introduce a non-trivial setup in which the training set exhibits a strong bias such that class label annotations are irrelevant and spurious correlations cannot be distinguished.
To address this problem, we introduce an adversarially trained model with a \emph{null-sampling} procedure to produce invariant representations in the data domain.
To enable disentanglement, a partially-labelled \emph{representative} set is used.
By placing the representations into the data domain, the changes made by the model are easily examinable by human auditors.
We show the effectiveness of our method on both image and tabular datasets: Coloured MNIST, the CelebA and the Adult dataset.%
% At the same time, it is important the people be able to trust the outputs of machine learning systems 
% To address this problem, we propose dividing training into a straightforward and general two-step procedure.
% In this framework, the model is first trained to produce invariant representations from an unlabelled pre-training set, in which there exists minimal spurious correlations.
% In the second step a classifier is trained on the encoding generated for the biased training set.
% We first demonstrate the viability of our  No Shortcuts in Neural Network (NoSiNN) framework with a AutoEncoder model before showing how its drawbacks can be ameliorated through the use of an Invertible architecture.
% Experiments with face images, coloured digits, and census data show promising results in decorrelating the spurious and non-spurious semantic features.

\input{paper2/1-introduction.tex}
\input{paper2/2-related-work.tex}
\input{paper2/3-method.tex}
\input{paper2/4-experiments.tex}
\input{paper2/5-conclusion.tex}

\section*{Acknowledgements}
This work was in part funded by the European Research Council 
under the ERC grant agreement no. 851538.
We are grateful to NVIDIA for donating GPUs.

\input{paper2/appendix.tex}
% \FloatBarrier%
% \clearpage

% \printbibliography%
% \end{refsection}

% \end{document}
