\section{Background}\label{sec:background}
% \noindent We frame our approach in the context of relevant literature on the interrelated
% problems of fair representation learning and learning representations free of spurious
% correlations. The background is far too long - we also need to touch on the interpretability
% literature
%
\subsection{Learning fair representations}
%As we have alluded to, the goal of producing invariant representations is similar to that of
%producing \emph{fair} representations. In fairness problems, there is usually a \emph{sensitive
%attribute}, $s$ (for example, gender or race), that should not be used to make decisions.
Given a sensitive attribute $s$ (for example, gender or race) and inputs $x$, a fair
representation $z$ of $x$ is then one for which $z \perp s$ holds, while ideally
also being predictive of the class label $y$. 
%
\citet{zemel2013learning} was the first to propose the learning of fair representations which allow
for transfer to new classification tasks.
%
More recent methods are often based on
\acfp{VAE}~\citep{kingma2013auto,louizos2016variational,edwards2016censoring,beutel2017data}. The
achieved fairness of the representation can be measured with various fairness metrics. These
measure, however, usually how fair the predictions of a classifier are and not how fair a
representation is.

The appropriate measure of fairness for a given task is domain-specific \citep{liu2018delayed} and
there is often not a universally accepted measure. 
%
However, \emph{\ac{DP}} is the most widely
used~\citep{louizos2016variational,edwards2016censoring,beutel2017data}. 
%
\ac{DP} demands $\hat{y} \perp s$ where $\hat{y}$ refers to the predictions of the classifier. 
%
In the context of fair representations, we measure the Demographic Parity of a downstream
classifier, $f(\cdot )$, which is trained on the representation $z$, i.e.\  $\Gamma: \gZ \to \gY$.

A core principle of all fairness methods is the \emph{accuracy-fairness trade-off}.
%
As previously stated, the fair representation should be invariant to $s$ ($\to$ fairness) but still
be predictive of $y$ ($\to$ accuracy). These desiderata cannot, in general, be simultaneously
satisfied if $s$ and $y$ are correlated.

The majority of existing methods for \ac{FRL} also make use of $y$ labels during training, in order
to ensure that $z$ remains predictive of $y$. 
%
This aspect can, in theory, be removed from the methods, but then there is no guarantee that
information about $y$ is preserved \citep{louizos2016variational}. 
%
% Existing methods designed to create fair representations can, in theory, be extended to the
% regime in which only the $s$, and not the $y$, labels are available. However, it is not without
% its drawbacks as, in removing $s$, there is no guarantee that information about $y$ is preserved
% \cite{louizos2016variational}. For this reason, $y$ is typically supplied during training and the
% representation encouraged to be predictive of it.

\subsection{Learning fair, transferable representations}
% Outside of computer vision, \cite{madras2018learning} have also worked on removing a problematic
% spurious correlation.
In addition to producing fair representations, \citet{madras2018learning} want to ensure the
representations are transferable. Here, an adversary is used to remove sensitive information from
a representation $z$. Auxiliary prediction and reconstruction networks, to predict class label $y$
% to ensure it remains predictive of $y$
and reconstruct the input $x$ respectively, are trained on top of $z$, with $s$ being ancillary
input to the reconstruction.
% alongside a reconstruction loss computed with respect to the output of a decoder that takes $z$
% and the sensitive label $s$ as input is added. This is so that $x$ can still be reconstructed
% from $z$, despite the removal of $s$.
%The decoder is utilised only for the purpose of maximum likelihood learning of the data
%distribution. By conditioning the decoder on the sensitive attribute, information about it can be
%``off-loaded'' from the encoder such that information about $s$ need not be contained in the fair
%representation while still permitting the use of a reconstruction loss needed to capture
%non-sensitive information. The decoder plays a role only in the loss function. In contrast, we
%make explicit use of the decoder not only for characterising the behaviour of the model and also
%for evaluation.

Also related is \citet{creager2019flexibly} who employ a FactorVAE \citep{kim2018disentangling}
regularised for fairness. 
%
The idea is to learn a representation that is both disentangled and invariant to multiple sensitive
attributes. 
%
This factorisation makes the latent space easily manipulable such that the different subspaces can
be freely removed and composed at test time. Zeroing out the dimensions or replacing them with
independent noise imparts invariance to the corresponding sensitive attribute. 
%
This method closely resembles ours when we use an invertible encoder. 
%
However, the emphasis of our approach is on interpretability, information-preservation, and coping
with sampling bias - especially extreme cases where \( |\gS^{tr} \times \gY^{tr}| < | \gS^{te}
\times \gY^{te} | \).
% Namely, the invertibility of the network means we can optimise for invariance singularly without
% the burden of a reconstruction loss. While we do not explicitly consider the case of
% multi-attribute fairness, our method can be easily adapted for this use-case.

Attempts were made by~\citet{QuaShaTho19} prior to this work to learn fair representations in the
data domain in order to make it interpretable and transferable. In their work, the input is assumed
to be additively decomposable in the feature space into a \emph{fair} and \emph{unfair} component,
which together can be used by the decoder to recover the original input. This allows us to examine
representations in a human-interpretable space and confirm that the model is not learning a
relationship reliant on a sensitive attribute. Though a first step in this direction, we believe
such a linear decomposition is not sufficiently expressive to fully capture the relationship
between the sensitive and non-sensitive attributes. Our approach allows for the modelling of more
complex relationships.

\subsection{Learning in the presence of spurious correlations}
% As  previously  discussed,  the  goal  of  producing  fair representations is similar to the goal
% of producing representations invariant to spurious correlations found in the training data.
Strong \aclp{SC} make the task of learning a robust classifier challenging: the classifier may
learn to exploit correlations unrelated to the true causal relationship between the features and
label, and thereby fail to generalise to novel settings. 
%
This problem was recently tackled by \citet{kim2019learning} who apply a penalty based on the
\ac{MI} between the feature embedding and the spurious variable. 
%
While the method is effective under mild biasing, we show experimentally that it is not robust to
the range of settings we consider.

\citet{JacBehZemBet19} explore the vulnerability of traditional neural networks to spurious
variables -- e.g.\ textures, in the case of ImageNet \citep{Geir18} -- and propose an
\ac{INN}-based solution akin to ours. 
%
The \ac{INN}'s encoding is split such that one partition, $z_b$ is encouraged to be predictive of
the spurious variable while the other serves as the logits for classification of the semantic
label. 
%
Information related to the nuisance variable is ``pulled out'' of the logits as a result of
maximising $\log p(s|z_n)$. 
%
This specific approach, however, is incompatible with the settings we consider, due to its
requirement that both $s$ and $y$ be available at training time.

Taking a causal perspective, \citet{arjovsky2019invariant} propose a variant of \ac{ERM} they call
invariant risk minimisation (IRM).
%
The goal of IRM is to train a predictor that generalises across a large set of unseen environments;
because variables with \aclp{SC} correlations do not represent a stable causal mechanism, the
predictor learns to be invariant to them.
%
IRM assumes that the training data is not \emph{\iid{}} but is partitioned into distinct
environments, $e \in E$.
%
The optimal predictor is then defined as the minimiser of the sum of the empirical risk $R_e$ over
this set.
%
In contrast, we assume possession of only a single source of \emph{labelled}, albeit
spuriously-correlated, data, but that we have a second source of data that is free of spurious
correlations, with the benefit being that it only needs to be labelled \emph{\wrt{} $s$}.

% The model thereby enforces their independence. This is achieved with an adversarial approach,
% borrowing the gradient reversal technique from \cite{ganin2016domain}.
%The authors construct the coloured MNIST dataset in two steps. First, ten distinct colours are
%assigned to each digit uniquely; these colours parameterise the means of ten corresponding
%Gaussian distributions from which colour samples are drawn. The standard deviation ($\sigma$) of
%the Gaussian distribution controls the dispersion of the sampled colours around these means. To
%demonstrate the effectiveness of their model, \cite{kim2019learning} construct a coloured version
%of the MNIST dataset as follows. During training, colours are sampled from a Gaussian distribution
%(with standard deviation $\sigma$) where each digit is associated with a single fixed mean colour.
%the colours are sampled abiding by this one-to-one colour mapping; At test time however, a colour
%mean is chosen at random from the 10 mean colours used during training. The actual colour is
%sampled with the same $\sigma$ as in the training set. there is no such designation and colours
%are sampled randomly and unrestrictedly from the complete palette. As such, a classifier that
%lazily minimises its loss by treating the pixel values as a lookup table falls flat at inference
%owing to a shift in the distribution of the spurious variables away from that of the target's. We
%follow this approach to evaluate performance of our NoSiNN framework in a synthetic setting (see
%Fig. \ref{fig:cmnist} for qualitative results).

% For the training strategy of the \cite{kim2019learning} model, a neural network is trained to
% predict the digit class, $y$, while an adversarial network takes one of the intermediate layers
% as input to predict the spurious value, colour. The first network seeks to prevent the adversary
% from making correct predictions, which means discarding or obfuscating information about colour.
% For this approach to work, the adversary needs to be able to distinguish between the digit class
% and the colour. To do this, the adversary is allowed access to the sampled RGB values of the
% colour that it is trying to remove, and not just the mean. As the sampled colour varies according
% to the standard deviation of the Gaussian distribution, the actual colour and the digit class
% vary in correlation. As the colour becomes less descriptive of the digit class, the network
% learns to disentangle the two. This works better, the larger $\sigma$; a major limitation of the
% approach its failure to deal with extremely low $\sigma$ values.


% \begin{itemize} \item Pix2pix and CycleGANs combined standard cGAN discriminators with L1
% reconstruction loss in data domain, the latter doing so in the form of cycle consistency,
% allowing for translation between unpaired samples. Bidirectional GANs extend the GAN
% discriminator to act on the distributions in data and latent space jointly. \item StarGAN
% \cite{choi2018stargan} provides a unified framework for performing image-to-image translation
% across multiple domains. \item Instead of enforcing bijectivity through cycle loses, invertible
% neural networks are bidirectional by design \item Glow achieves impressive attribute
% manipulations \item Rather than trying to translate inputs across domains we seek to do so to a
% subspace which does not abide in either domain. 
    
% \end{itemize}

% \paragraph{Unsupervised approaches.} There is a large literature on the unsupervised
% disentangling of representations; we highlight one of the more recent findings connected with our
% approach. \cite{locatello2019challenging} evidence  that the unsupervised learning of
% disentangled representations requires inductive biases on the part of both the data set and the
% models. Thus, such methods can usually only be used for a single task or kind of data.
